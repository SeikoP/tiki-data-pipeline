"""
Extract products v√† links t·ª´ Tiki categories
"""
import os
import sys
import re
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from typing import List, Dict, Any, Optional

# Fix encoding on Windows
if sys.platform == "win32":
    import io
    try:
        if not hasattr(sys.stdout, 'buffer') or (hasattr(sys.stdout, 'encoding') and sys.stdout.encoding != 'utf-8'):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    except (AttributeError, ValueError):
        pass

from .config import get_config

# C·∫•u h√¨nh
config = get_config()
FIRECRAWL_API_URL = os.getenv("FIRECRAWL_API_URL", "http://localhost:3002")
TIKI_BASE_URL = "https://tiki.vn"


def extract_product_id(url: str) -> Optional[str]:
    """Extract product ID t·ª´ URL (pattern: /p123456 ho·∫∑c ?id=123456 ho·∫∑c -p123456.html)"""
    # Pattern 1: /p123456
    match = re.search(r'/p(\d+)', url)
    if match:
        return match.group(1)
    
    # Pattern 2: -p123456 or -p123456.html (Tiki product URL format)
    match = re.search(r'-p(\d+)', url)
    if match:
        return match.group(1)
    
    # Pattern 3: ?id=123456
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)
    if 'id' in query_params:
        return query_params['id'][0]
    
    return None


def extract_products_from_markdown(markdown_text: str, category_id: str = None, category_name: str = None) -> List[Dict[str, Any]]:
    """
    Extract product links t·ª´ markdown text
    
    Args:
        markdown_text: Markdown content t·ª´ Firecrawl
        category_id: ID c·ªßa category (optional)
        category_name: T√™n category (optional)
    
    Returns:
        List of products v·ªõi 'name', 'url', 'product_id'
    """
    products = []
    
    # Pattern ƒë·ªÉ t√¨m markdown links: [text](url)
    link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    matches = re.findall(link_pattern, markdown_text)
    
    for name, url in matches:
        # Filter ch·ªâ l·∫•y links t·ª´ tiki.vn v√† c√≥ ch·ª©a product
        if 'tiki.vn' in url or url.startswith('/'):
            # Normalize URL
            if url.startswith('/'):
                full_url = urljoin(TIKI_BASE_URL, url)
            else:
                full_url = url
            
            # Check n·∫øu l√† product link - pattern /pXXXXX ho·∫∑c ?id=XXXXX
            product_id = extract_product_id(url)
            is_product = product_id is not None
            
            # Exclude non-product links
            exclude_keywords = [
                'search?', 'checkout', 'cart', 'hotro', 'mailto:', 
                'javascript:', 'data:', 'account', 'login', 'register',
                'help', 'about', 'contact', 'policy', 'terms',
                '/c',  # Category links
                'category', 'danh-muc'
            ]
            is_excluded = any(kw in url.lower() for kw in exclude_keywords)
            
            # Include n·∫øu l√† product
            if is_product and not is_excluded:
                products.append({
                    'name': name.strip(),
                    'url': full_url,
                    'product_id': product_id,
                    'category_id': category_id,
                    'category_name': category_name,
                    'slug': url.split('/')[-1].split('?')[0] if '/' in url else None
                })
    
    return products


def extract_products_from_html(html_text: str, category_id: str = None, category_name: str = None) -> List[Dict[str, Any]]:
    """
    Extract product links t·ª´ HTML
    
    Args:
        html_text: HTML content t·ª´ Firecrawl
        category_id: ID c·ªßa category (optional)
        category_name: T√™n category (optional)
    
    Returns:
        List of products v·ªõi 'name', 'url', 'product_id'
    """
    products = []
    soup = BeautifulSoup(html_text, 'html.parser')
    
    # T√¨m t·∫•t c·∫£ links
    links = soup.find_all('a', href=True)
    
    for link in links:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        
        # Filter product links
        if href and ('tiki.vn' in href or href.startswith('/')):
            # Check product pattern
            product_id = extract_product_id(href)
            is_product = product_id is not None
            
            # Exclude non-product links
            exclude_keywords = [
                'search?', 'checkout', 'cart', 'hotro', 'mailto:', 
                'javascript:', 'data:', 'account', 'login', 'register',
                '/c',  # Category links
                'category', 'danh-muc'
            ]
            is_excluded = any(kw in href.lower() for kw in exclude_keywords)
            
            if is_product and not is_excluded:
                if href.startswith('/'):
                    full_url = urljoin(TIKI_BASE_URL, href)
                else:
                    full_url = href
                
                products.append({
                    'name': text or full_url,
                    'url': full_url,
                    'product_id': product_id,
                    'category_id': category_id,
                    'category_name': category_name,
                    'slug': href.split('/')[-1].split('?')[0] if '/' in href else None
                })
    
    return products


def parse_firecrawl_products(response_data: Dict[str, Any], category_id: str = None, category_name: str = None) -> List[Dict[str, Any]]:
    """
    Parse response t·ª´ Firecrawl v√† extract products
    
    Args:
        response_data: Response t·ª´ Firecrawl API
        category_id: ID c·ªßa category (optional)
        category_name: T√™n category (optional)
    
    Returns:
        List of unique products
    """
    products = []
    
    # Check n·∫øu c√≥ markdown
    if 'data' in response_data and 'markdown' in response_data['data']:
        markdown = response_data['data']['markdown']
        products.extend(extract_products_from_markdown(markdown, category_id, category_name))
    
    # Check n·∫øu c√≥ HTML
    if 'data' in response_data and 'html' in response_data['data']:
        html = response_data['data']['html']
        products.extend(extract_products_from_html(html, category_id, category_name))
    
    # Remove duplicates - ∆∞u ti√™n theo product_id v√† URL
    seen_urls = set()
    seen_ids = set()
    unique_products = []
    
    for product in products:
        url = product['url']
        product_id = product.get('product_id')
        
        # Remove query params ƒë·ªÉ so s√°nh
        clean_url = url.split('?')[0]
        
        # Check duplicate by URL ho·∫∑c product_id
        is_duplicate = (
            clean_url in seen_urls or 
            (product_id and product_id in seen_ids)
        )
        
        if not is_duplicate:
            seen_urls.add(clean_url)
            if product_id:
                seen_ids.add(product_id)
            unique_products.append(product)
    
    # Sort by product_id ƒë·ªÉ d·ªÖ ƒë·ªçc
    unique_products.sort(key=lambda x: int(x.get('product_id', 0)) if x.get('product_id') else 999999)
    
    return unique_products


def crawl_products_from_category(
    category_url: str,
    category_id: str = None,
    category_name: str = None,
    max_products: int = None,
    timeout: int = 60
) -> List[Dict[str, Any]]:
    """
    Crawl products t·ª´ m·ªôt category URL
    
    Args:
        category_url: URL c·ªßa category c·∫ßn crawl products
        category_id: ID c·ªßa category (optional)
        category_name: T√™n c·ªßa category (optional)
        max_products: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products (None = t·∫•t c·∫£)
        timeout: Timeout cho request (seconds)
    
    Returns:
        List of products v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß
    """
    payload = {
        "url": category_url,
        "onlyMainContent": True,
        "maxAge": 172800000,  # 2 days
        "parsers": [],
        "formats": ["html", "markdown"]
    }
    
    try:
        response = requests.post(
            f"{FIRECRAWL_API_URL}/v0/scrape",
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        response_data = response.json()
        
        # Parse products t·ª´ response
        products = parse_firecrawl_products(response_data, category_id, category_name)
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu c·∫ßn
        if max_products and len(products) > max_products:
            products = products[:max_products]
        
        return products
        
    except requests.exceptions.RequestException as e:
        try:
            print(f"‚ö†Ô∏è  L·ªói khi crawl products t·ª´ {category_url}: {e}")
        except (ValueError, OSError):
            try:
                print(f"Error crawling products from {category_url}: {e}", file=sys.stderr)
            except:
                pass
        return []
    except Exception as e:
        try:
            print(f"‚ö†Ô∏è  L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        except (ValueError, OSError):
            try:
                print(f"Unexpected error: {e}", file=sys.stderr)
            except:
                pass
        return []


def crawl_products_from_categories(
    categories: List[Dict[str, Any]],
    max_products_per_category: int = None,
    max_categories: int = None,
    timeout: int = 60
) -> List[Dict[str, Any]]:
    """
    Crawl products t·ª´ nhi·ªÅu categories
    
    Args:
        categories: List c√°c categories (dict v·ªõi 'url', 'category_id', 'name')
        max_products_per_category: Gi·ªõi h·∫°n products m·ªói category
        max_categories: Gi·ªõi h·∫°n s·ªë categories ƒë·ªÉ crawl
        timeout: Timeout cho m·ªói request
    
    Returns:
        List t·∫•t c·∫£ products t·ª´ c√°c categories
    """
    all_products = []
    
    # Gi·ªõi h·∫°n s·ªë categories n·∫øu c·∫ßn
    categories_to_crawl = categories
    if max_categories:
        categories_to_crawl = categories[:max_categories]
    
    total = len(categories_to_crawl)
    
    for i, category in enumerate(categories_to_crawl, 1):
        category_url = category.get('url', '')
        category_id = category.get('category_id', '')
        category_name = category.get('name', 'N/A')
        
        if not category_url:
            continue
        
        try:
            print(f"[{i}/{total}] üì¶ Crawling products t·ª´: {category_name} (ID: {category_id})")
            print(f"   URL: {category_url}")
        except (ValueError, OSError):
            try:
                print(f"[{i}/{total}] Crawling products from: {category_name} (ID: {category_id})", file=sys.stderr)
            except:
                pass
        
        products = crawl_products_from_category(
            category_url=category_url,
            category_id=category_id,
            category_name=category_name,
            max_products=max_products_per_category,
            timeout=timeout
        )
        
        if products:
            try:
                print(f"   ‚úì T√¨m th·∫•y {len(products)} products")
            except (ValueError, OSError):
                try:
                    print(f"   Found {len(products)} products", file=sys.stderr)
                except:
                    pass
            all_products.extend(products)
        else:
            try:
                print(f"   - Kh√¥ng t√¨m th·∫•y products")
            except (ValueError, OSError):
                try:
                    print(f"   - No products found", file=sys.stderr)
                except:
                    pass
    
    return all_products


def save_products_to_json(products: List[Dict[str, Any]], output_file: str):
    """
    L∆∞u products v√†o file JSON
    
    Args:
        products: List c√°c products
        output_file: ƒê∆∞·ªùng d·∫´n file output
    """
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(products, f, indent=2, ensure_ascii=False)
    
    try:
        print(f"üíæ ƒê√£ l∆∞u {len(products)} products v√†o: {output_file}")
    except (ValueError, OSError):
        try:
            print(f"Saved {len(products)} products to: {output_file}", file=sys.stderr)
        except:
            pass


def load_products_from_json(json_file: str) -> List[Dict[str, Any]]:
    """
    Load products t·ª´ file JSON
    
    Args:
        json_file: ƒê∆∞·ªùng d·∫´n file JSON
    
    Returns:
        List c√°c products
    """
    if not os.path.exists(json_file):
        return []
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        try:
            print(f"‚ö†Ô∏è  L·ªói khi parse JSON: {e}")
        except (ValueError, OSError):
            try:
                print(f"Error parsing JSON: {e}", file=sys.stderr)
            except:
                pass
        return []

