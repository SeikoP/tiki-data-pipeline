import json
import sys
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict

# Import tá»« file cÃ¹ng thÆ° má»¥c
import importlib.util

spec = importlib.util.spec_from_file_location(
    "extract_category_link_selenium",
    os.path.join(os.path.dirname(__file__), "extract_category_link_selenium.py"),
)
extract_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(extract_module)

crawl_with_selenium = extract_module.crawl_with_selenium
parse_categories = extract_module.parse_categories

# Set UTF-8 encoding cho stdout trÃªn Windows
if sys.platform == "win32":
    try:
        import io

        if hasattr(sys.stdout, "buffer") and not sys.stdout.closed:
            sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    except:
        try:
            import io

            if hasattr(sys.stdout, "buffer"):
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
        except:
            pass

# Thá»­ import tqdm, náº¿u khÃ´ng cÃ³ thÃ¬ dÃ¹ng fallback
try:
    from tqdm import tqdm

    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("âš ï¸  Khuyáº¿n nghá»‹ cÃ i Ä‘áº·t tqdm Ä‘á»ƒ hiá»ƒn thá»‹ progress bar: pip install tqdm")

    # Fallback progress bar Ä‘Æ¡n giáº£n
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable
            self.total = total or (len(iterable) if iterable else 0)
            self.desc = desc
            self.n = 0
            self.start_time = time.time()

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    self.n += 1
                    self.update(1)
                    yield item
            else:
                return self

        def update(self, n=1):
            self.n += n
            if self.total > 0:
                pct = (self.n / self.total) * 100
                elapsed = time.time() - self.start_time
                if self.n > 0:
                    rate = self.n / elapsed if elapsed > 0 else 0
                    eta = (self.total - self.n) / rate if rate > 0 else 0
                    print(
                        f"\r{self.desc} {self.n}/{self.total} ({pct:.1f}%) | "
                        f"Tá»‘c Ä‘á»™: {rate:.2f}/s | ETA: {eta:.0f}s",
                        end="",
                        flush=True,
                    )

        def set_description(self, desc):
            self.desc = desc


# Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
os.makedirs("data/raw", exist_ok=True)

# Thread-safe locks vÃ  counters
stats_lock = Lock()
stats = {
    "total_crawled": 0,
    "total_success": 0,
    "total_failed": 0,
    "total_categories": 0,
    "by_level": defaultdict(int),
    "start_time": time.time(),
}


def crawl_single_category(
    url, parent_url, level, max_level, visited_urls, cache_dir="data/raw/cache"
):
    """
    Crawl má»™t danh má»¥c Ä‘Æ¡n láº» (thread-safe)

    Returns:
        tuple: (success: bool, categories: list, error: str)
    """
    global stats

    # Kiá»ƒm tra cache
    cache_file = None
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        import hashlib

        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_file = os.path.join(cache_dir, f"{url_hash}.json")

        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cached_data = json.load(f)
                with stats_lock:
                    stats["total_crawled"] += 1
                    stats["total_success"] += 1
                return True, cached_data.get("categories", []), None
            except:
                pass

    try:
        # Crawl vá»›i Selenium (khÃ´ng verbose Ä‘á»ƒ trÃ¡nh spam log)
        html_content = crawl_with_selenium(url, save_html=False, verbose=False)

        # Parse danh má»¥c con
        child_categories = parse_categories(html_content, parent_url=url, level=level + 1)

        # Lá»c chá»‰ láº¥y cÃ¡c danh má»¥c cÃ³ hÃ¬nh áº£nh
        categories_with_images = []
        for cat in child_categories:
            if cat.get("image_url", "").strip():
                categories_with_images.append(cat)

        # LÆ°u cache
        if cache_file:
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {"url": url, "categories": categories_with_images},
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
            except:
                pass

        # Update stats
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_success"] += 1
            stats["total_categories"] += len(categories_with_images)
            stats["by_level"][level + 1] += len(categories_with_images)

        return True, categories_with_images, None

    except Exception as e:
        error_msg = str(e)
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_failed"] += 1
        return False, [], error_msg


def crawl_level_parallel(urls_to_crawl, parent_urls, level, max_level, visited_urls, max_workers=3):
    """
    Crawl song song nhiá»u danh má»¥c cÃ¹ng level

    Args:
        urls_to_crawl: List cÃ¡c URL cáº§n crawl
        parent_urls: List cÃ¡c parent URL tÆ°Æ¡ng á»©ng
        level: Level hiá»‡n táº¡i
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        max_workers: Sá»‘ thread tá»‘i Ä‘a (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i)

    Returns:
        dict: {url: (success, categories, error)}
    """
    results = {}

    # Lá»c cÃ¡c URL chÆ°a crawl
    tasks = []
    for url, parent_url in zip(urls_to_crawl, parent_urls):
        if url not in visited_urls:
            tasks.append((url, parent_url))

    if not tasks:
        return results

    # Táº¡o progress bar
    desc = f"Level {level}"
    with tqdm(total=len(tasks), desc=desc, unit="danh má»¥c") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit táº¥t cáº£ tasks
            future_to_url = {}
            for url, parent_url in tasks:
                future = executor.submit(
                    crawl_single_category, url, parent_url, level, max_level, visited_urls
                )
                future_to_url[future] = (url, parent_url)

            # Xá»­ lÃ½ káº¿t quáº£ khi hoÃ n thÃ nh
            for future in as_completed(future_to_url):
                url, parent_url = future_to_url[future]
                try:
                    success, categories, error = future.result(timeout=300)  # Timeout 5 phÃºt
                    results[url] = (success, categories, error)

                    # ÄÃ¡nh dáº¥u Ä‘Ã£ crawl (dÃ¹ thÃ nh cÃ´ng hay tháº¥t báº¡i)
                    visited_urls.add(url)

                    if success:
                        with stats_lock:
                            pbar.set_postfix({"âœ…": len(categories), "âŒ": stats["total_failed"]})
                    else:
                        with stats_lock:
                            pbar.set_postfix(
                                {"âœ…": stats["total_success"], "âŒ": stats["total_failed"]}
                            )
                        print(f"\n  âŒ Lá»—i crawl {url}: {error}")
                except Exception as e:
                    error_msg = str(e)
                    results[url] = (False, [], error_msg)
                    visited_urls.add(url)  # ÄÃ¡nh dáº¥u Ä‘Ã£ thá»­ crawl
                    with stats_lock:
                        stats["total_failed"] += 1
                    print(f"\n  âŒ Exception khi crawl {url}: {error_msg}")

                pbar.update(1)

    return results


def crawl_category_recursive_optimized(
    root_url, max_level=3, max_workers=3, visited_urls=None, all_categories=None
):
    """
    Crawl Ä‘á»‡ quy cÃ¡c danh má»¥c vá»›i tá»‘i Æ°u song song

    Args:
        root_url: URL danh má»¥c gá»‘c
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        max_workers: Sá»‘ thread tá»‘i Ä‘a cho má»—i level
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        all_categories: List táº¥t cáº£ cÃ¡c danh má»¥c Ä‘Ã£ crawl
    """
    if visited_urls is None:
        visited_urls = set()
    if all_categories is None:
        all_categories = []

    # Queue cÃ¡c URL cáº§n crawl theo level
    # Format: {level: [(url, parent_url), ...]}
    queue = defaultdict(list)
    queue[0] = [(root_url, None)]

    # Crawl tá»«ng level má»™t
    for current_level in range(max_level + 1):
        if current_level not in queue or not queue[current_level]:
            continue

        urls_to_crawl = [url for url, _ in queue[current_level]]
        parent_urls = [parent for _, parent in queue[current_level]]

        # Lá»c cÃ¡c URL chÆ°a crawl
        new_urls = []
        new_parents = []
        for url, parent in zip(urls_to_crawl, parent_urls):
            if url not in visited_urls:
                new_urls.append(url)
                new_parents.append(parent)

        if not new_urls:
            continue

        print(f"\n{'='*70}")
        print(f"ğŸ“Š Level {current_level}: Äang crawl {len(new_urls)} danh má»¥c...")
        print(f"{'='*70}")

        # Crawl song song
        results = crawl_level_parallel(
            new_urls, new_parents, current_level, max_level, visited_urls, max_workers=max_workers
        )

        # Xá»­ lÃ½ káº¿t quáº£ vÃ  chuáº©n bá»‹ level tiáº¿p theo
        for url, (success, categories, error) in results.items():
            if success:
                # ThÃªm vÃ o danh sÃ¡ch tá»•ng
                all_categories.extend(categories)

                # ThÃªm cÃ¡c danh má»¥c con vÃ o queue level tiáº¿p theo
                if current_level < max_level:
                    for cat in categories:
                        child_url = cat["url"]
                        if child_url not in visited_urls:
                            queue[current_level + 1].append((child_url, url))
            else:
                print(f"  âŒ Lá»—i crawl {url}: {error}")

    return all_categories


def print_stats():
    """In thá»‘ng kÃª real-time"""
    global stats
    with stats_lock:
        elapsed = time.time() - stats["start_time"]
        rate = stats["total_crawled"] / elapsed if elapsed > 0 else 0

        print(f"\n{'='*70}")
        print("ğŸ“ˆ THá»NG KÃŠ")
        print(f"{'='*70}")
        print(f"â±  Thá»i gian: {elapsed:.1f}s")
        print(f"ğŸ“¥ ÄÃ£ crawl: {stats['total_crawled']} danh má»¥c")
        print(f"âœ… ThÃ nh cÃ´ng: {stats['total_success']}")
        print(f"âŒ Tháº¥t báº¡i: {stats['total_failed']}")
        print(f"ğŸ“Š Tá»•ng danh má»¥c tÃ¬m Ä‘Æ°á»£c: {stats['total_categories']}")
        print(f"âš¡ Tá»‘c Ä‘á»™: {rate:.2f} danh má»¥c/s")

        if stats["by_level"]:
            print(f"\nğŸ“‹ Theo level:")
            for level in sorted(stats["by_level"].keys()):
                print(f"  Level {level}: {stats['by_level'][level]} danh má»¥c")


def main():
    """HÃ m main Ä‘á»ƒ crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u"""

    # URL danh má»¥c gá»‘c
    root_url = "https://tiki.vn/nha-cua-doi-song/c1883"

    # Äá»™ sÃ¢u tá»‘i Ä‘a
    max_level = 3

    # Sá»‘ thread song song (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i server)
    max_workers = 3

    print("=" * 70)
    print("ğŸš€ CRAWL Äá»† QUY CÃC DANH Má»¤C TIKI (Tá»I Æ¯U)")
    print("=" * 70)
    print(f"URL gá»‘c: {root_url}")
    print(f"Äá»™ sÃ¢u tá»‘i Ä‘a: {max_level}")
    print(f"Sá»‘ thread song song: {max_workers}")
    print(f"Cache: data/raw/cache/")
    print("=" * 70)

    # Reset stats
    global stats
    stats = {
        "total_crawled": 0,
        "total_success": 0,
        "total_failed": 0,
        "total_categories": 0,
        "by_level": defaultdict(int),
        "start_time": time.time(),
    }

    # Crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u
    all_categories = crawl_category_recursive_optimized(
        root_url, max_level=max_level, max_workers=max_workers
    )

    # Loáº¡i bá» trÃ¹ng láº·p theo URL (giá»¯ láº¡i báº£n Ä‘áº§u tiÃªn)
    unique_categories = []
    seen_urls = set()
    for cat in all_categories:
        if cat["url"] not in seen_urls:
            unique_categories.append(cat)
            seen_urls.add(cat["url"])

    # Sáº¯p xáº¿p theo level vÃ  tÃªn
    unique_categories.sort(key=lambda x: (x.get("level", 0), x["name"]))

    # LÆ°u káº¿t quáº£
    output_file = "data/raw/categories_recursive_optimized.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_categories, f, ensure_ascii=False, indent=2)

    # In thá»‘ng kÃª
    print_stats()

    print(f"\nğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_file}")
    print(f"ğŸ“¦ Tá»•ng sá»‘ danh má»¥c unique: {len(unique_categories)}")

    # Thá»‘ng kÃª theo level
    level_counts = defaultdict(int)
    for cat in unique_categories:
        level = cat.get("level", 0)
        level_counts[level] += 1

    if level_counts:
        print(f"\nğŸ“‹ Thá»‘ng kÃª theo level:")
        for level in sorted(level_counts.keys()):
            print(f"  Level {level}: {level_counts[level]} danh má»¥c")


if __name__ == "__main__":
    main()
