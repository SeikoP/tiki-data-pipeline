# Import t·ª´ file c√πng th∆∞ m·ª•c
import importlib.util
import json
import os
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

spec = importlib.util.spec_from_file_location(
    "extract_category_link_selenium",
    os.path.join(os.path.dirname(__file__), "extract_category_link_selenium.py"),
)
extract_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(extract_module)

crawl_with_selenium = extract_module.crawl_with_selenium
parse_categories = extract_module.parse_categories

# Set UTF-8 encoding cho stdout tr√™n Windows
if sys.platform == "win32":
    try:
        import io

        if hasattr(sys.stdout, "buffer") and not sys.stdout.closed:
            sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    except Exception:
        try:
            import io

            if hasattr(sys.stdout, "buffer"):
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
        except Exception:
            pass

# Th·ª≠ import tqdm, n·∫øu kh√¥ng c√≥ th√¨ d√πng fallback
try:
    from tqdm import tqdm

    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("‚ö†Ô∏è  Khuy·∫øn ngh·ªã c√†i ƒë·∫∑t tqdm ƒë·ªÉ hi·ªÉn th·ªã progress bar: pip install tqdm")

    # Fallback progress bar ƒë∆°n gi·∫£n
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable
            self.total = total or (len(iterable) if iterable else 0)
            self.desc = desc
            self.n = 0
            self.start_time = time.time()

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    self.n += 1
                    self.update(1)
                    yield item
            else:
                return self

        def update(self, n=1):
            self.n += n
            if self.total > 0:
                pct = (self.n / self.total) * 100
                elapsed = time.time() - self.start_time
                if self.n > 0:
                    rate = self.n / elapsed if elapsed > 0 else 0
                    eta = (self.total - self.n) / rate if rate > 0 else 0
                    print(
                        f"\r{self.desc} {self.n}/{self.total} ({pct:.1f}%) | "
                        f"T·ªëc ƒë·ªô: {rate:.2f}/s | ETA: {eta:.0f}s",
                        end="",
                        flush=True,
                    )

        def set_description(self, desc):
            self.desc = desc


# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
os.makedirs("data/raw", exist_ok=True)

# Thread-safe locks v√† counters
stats_lock = Lock()
stats = {
    "total_crawled": 0,
    "total_success": 0,
    "total_failed": 0,
    "total_categories": 0,
    "by_level": defaultdict(int),
    "start_time": time.time(),
}


def crawl_single_category(
    url, parent_url, level, max_level, visited_urls, cache_dir="data/raw/cache", driver_pool=None
):
    """
    Crawl m·ªôt danh m·ª•c ƒë∆°n l·∫ª (thread-safe)

    Args:
        driver_pool: Optional SeleniumDriverPool for driver reuse

    Returns:
        tuple: (success: bool, categories: list, error: str)
    """
    global stats

    # Ki·ªÉm tra cache
    cache_file = None
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        import hashlib

        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_file = os.path.join(cache_dir, f"{url_hash}.json")

        if os.path.exists(cache_file):
            try:
                with open(cache_file, encoding="utf-8") as f:
                    cached_data = json.load(f)
                with stats_lock:
                    stats["total_crawled"] += 1
                    stats["total_success"] += 1
                return True, cached_data.get("categories", []), None
            except Exception:
                pass

    try:
        # Crawl v·ªõi Selenium (∆∞u ti√™n driver pool n·∫øu c√≥)
        html_content = None
        if driver_pool is not None:
            driver = driver_pool.get_driver()
            if driver is not None:
                try:
                    # Import crawl_with_driver n·∫øu c√≥
                    try:
                        from extract_category_link_selenium import crawl_with_driver

                        html_content = crawl_with_driver(
                            driver, url, save_html=False, verbose=False
                        )
                    except (ImportError, AttributeError):
                        # Fallback: crawl_with_driver ch∆∞a c√≥
                        pass
                finally:
                    driver_pool.return_driver(driver)

        # Fallback: t·∫°o driver ri√™ng n·∫øu pool kh√¥ng c√≥ ho·∫∑c fail
        if html_content is None:
            html_content = crawl_with_selenium(url, save_html=False, verbose=False)

        # Parse danh m·ª•c con
        child_categories = parse_categories(html_content, parent_url=url, level=level + 1)

        # L·ªçc ch·ªâ l·∫•y c√°c danh m·ª•c c√≥ h√¨nh ·∫£nh
        categories_with_images = [
            cat for cat in child_categories if cat.get("image_url", "").strip()
        ]

        # L∆∞u cache
        if cache_file:
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {"url": url, "categories": categories_with_images},
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
            except Exception:
                pass

        # Update stats
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_success"] += 1
            stats["total_categories"] += len(categories_with_images)
            stats["by_level"][level + 1] += len(categories_with_images)

        return True, categories_with_images, None

    except Exception as e:
        error_msg = str(e)
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_failed"] += 1
        return False, [], error_msg


def crawl_level_parallel(
    urls_to_crawl, parent_urls, level, max_level, visited_urls, max_workers=3, driver_pool=None
):
    """
    Crawl song song nhi·ªÅu danh m·ª•c c√πng level

    Args:
        urls_to_crawl: List c√°c URL c·∫ßn crawl
        parent_urls: List c√°c parent URL t∆∞∆°ng ·ª©ng
        level: Level hi·ªán t·∫°i
        max_level: ƒê·ªô s√¢u t·ªëi ƒëa
        visited_urls: Set c√°c URL ƒë√£ crawl
        max_workers: S·ªë thread t·ªëi ƒëa (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i)
        driver_pool: Optional SeleniumDriverPool for driver reuse

    Returns:
        dict: {url: (success, categories, error)}
    """
    results = {}

    # L·ªçc c√°c URL ch∆∞a crawl
    tasks = []
    for url, parent_url in zip(urls_to_crawl, parent_urls, strict=False):
        if url not in visited_urls:
            tasks.append((url, parent_url))

    if not tasks:
        return results

    # T·∫°o progress bar
    desc = f"Level {level}"
    with tqdm(total=len(tasks), desc=desc, unit="danh m·ª•c") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit t·∫•t c·∫£ tasks
            future_to_url = {}
            for url, parent_url in tasks:
                future = executor.submit(
                    crawl_single_category,
                    url,
                    parent_url,
                    level,
                    max_level,
                    visited_urls,
                    "data/raw/cache",
                    driver_pool,
                )
                future_to_url[future] = (url, parent_url)

            # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
            for future in as_completed(future_to_url):
                url, parent_url = future_to_url[future]
                try:
                    success, categories, error = future.result(timeout=300)  # Timeout 5 ph√∫t
                    results[url] = (success, categories, error)

                    # ƒê√°nh d·∫•u ƒë√£ crawl (d√π th√†nh c√¥ng hay th·∫•t b·∫°i)
                    visited_urls.add(url)

                    if success:
                        with stats_lock:
                            pbar.set_postfix({"‚úÖ": len(categories), "‚ùå": stats["total_failed"]})
                    else:
                        with stats_lock:
                            pbar.set_postfix(
                                {"‚úÖ": stats["total_success"], "‚ùå": stats["total_failed"]}
                            )
                        print(f"\n  ‚ùå L·ªói crawl {url}: {error}")
                except Exception as e:
                    error_msg = str(e)
                    results[url] = (False, [], error_msg)
                    visited_urls.add(url)  # ƒê√°nh d·∫•u ƒë√£ th·ª≠ crawl
                    with stats_lock:
                        stats["total_failed"] += 1
                    print(f"\n  ‚ùå Exception khi crawl {url}: {error_msg}")

                pbar.update(1)

    return results


def crawl_category_recursive_optimized(
    root_url, max_level=3, max_workers=3, visited_urls=None, all_categories=None
):
    """
    Crawl ƒë·ªá quy c√°c danh m·ª•c v·ªõi t·ªëi ∆∞u song song

    Args:
        root_url: URL danh m·ª•c g·ªëc
        max_level: ƒê·ªô s√¢u t·ªëi ƒëa
        max_workers: S·ªë thread t·ªëi ƒëa cho m·ªói level
        visited_urls: Set c√°c URL ƒë√£ crawl
        all_categories: List t·∫•t c·∫£ c√°c danh m·ª•c ƒë√£ crawl
    """
    if visited_urls is None:
        visited_urls = set()
    if all_categories is None:
        all_categories = []

    # Initialize driver pool for reuse
    driver_pool = None
    try:
        # Try to import SeleniumDriverPool
        try:
            spec = importlib.util.spec_from_file_location(
                "crawl_utils",
                os.path.join(os.path.dirname(__file__), "utils.py"),
            )
            if spec and spec.loader:
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                SeleniumDriverPool = getattr(utils_module, "SeleniumDriverPool", None)
                if SeleniumDriverPool:
                    driver_pool = SeleniumDriverPool(
                        pool_size=max_workers, headless=True, timeout=90
                    )
                    print(f"‚úÖ ƒê√£ kh·ªüi t·∫°o driver pool v·ªõi {max_workers} drivers")
        except Exception:
            pass  # Fallback: kh√¥ng d√πng pool

        # Queue c√°c URL c·∫ßn crawl theo level
        # Format: {level: [(url, parent_url), ...]}
        queue = defaultdict(list)
        queue[0] = [(root_url, None)]

        # Crawl t·ª´ng level m·ªôt
        for current_level in range(max_level + 1):
            if current_level not in queue or not queue[current_level]:
                continue

            urls_to_crawl = [url for url, _ in queue[current_level]]
            parent_urls = [parent for _, parent in queue[current_level]]

            # L·ªçc c√°c URL ch∆∞a crawl
            new_urls = []
            new_parents = []
            for url, parent in zip(urls_to_crawl, parent_urls, strict=False):
                if url not in visited_urls:
                    new_urls.append(url)
                    new_parents.append(parent)

            if not new_urls:
                continue

            print(f"\n{'='*70}")
            print(f"üìä Level {current_level}: ƒêang crawl {len(new_urls)} danh m·ª•c...")
            print(f"{'='*70}")

            # Crawl song song
            results = crawl_level_parallel(
                new_urls,
                new_parents,
                current_level,
                max_level,
                visited_urls,
                max_workers=max_workers,
                driver_pool=driver_pool,
            )

            # X·ª≠ l√Ω k·∫øt qu·∫£ v√† chu·∫©n b·ªã level ti·∫øp theo
            for url, (success, categories, error) in results.items():
                if success:
                    # Th√™m v√†o danh s√°ch t·ªïng
                    all_categories.extend(categories)

                    # Th√™m c√°c danh m·ª•c con v√†o queue level ti·∫øp theo
                    if current_level < max_level:
                        for cat in categories:
                            child_url = cat["url"]
                            if child_url not in visited_urls:
                                queue[current_level + 1].append((child_url, url))
                else:
                    print(f"  ‚ùå L·ªói crawl {url}: {error}")

        return all_categories
    finally:
        # Cleanup driver pool
        if driver_pool is not None:
            try:
                driver_pool.cleanup()
                print("‚úÖ ƒê√£ cleanup driver pool")
            except Exception:
                pass


def print_stats():
    """In th·ªëng k√™ real-time"""
    global stats
    with stats_lock:
        elapsed = time.time() - stats["start_time"]
        rate = stats["total_crawled"] / elapsed if elapsed > 0 else 0

        print(f"\n{'='*70}")
        print("üìà TH·ªêNG K√ä")
        print(f"{'='*70}")
        print(f"‚è±  Th·ªùi gian: {elapsed:.1f}s")
        print(f"üì• ƒê√£ crawl: {stats['total_crawled']} danh m·ª•c")
        print(f"‚úÖ Th√†nh c√¥ng: {stats['total_success']}")
        print(f"‚ùå Th·∫•t b·∫°i: {stats['total_failed']}")
        print(f"üìä T·ªïng danh m·ª•c t√¨m ƒë∆∞·ª£c: {stats['total_categories']}")
        print(f"‚ö° T·ªëc ƒë·ªô: {rate:.2f} danh m·ª•c/s")

        if stats["by_level"]:
            print("\nüìã Theo level:")
            for level in sorted(stats["by_level"].keys()):
                print(f"  Level {level}: {stats['by_level'][level]} danh m·ª•c")


def main():
    """H√†m main ƒë·ªÉ crawl ƒë·ªá quy v·ªõi t·ªëi ∆∞u"""

    # URL danh m·ª•c g·ªëc
    root_url = "https://tiki.vn/nha-cua-doi-song/c1883"

    # ƒê·ªô s√¢u t·ªëi ƒëa
    max_level = 3

    # S·ªë thread song song (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i server)
    max_workers = 3

    print("=" * 70)
    print("üöÄ CRAWL ƒê·ªÜ QUY C√ÅC DANH M·ª§C TIKI (T·ªêI ∆ØU)")
    print("=" * 70)
    print(f"URL g·ªëc: {root_url}")
    print(f"ƒê·ªô s√¢u t·ªëi ƒëa: {max_level}")
    print(f"S·ªë thread song song: {max_workers}")
    print("Cache: data/raw/cache/")
    print("=" * 70)

    # Reset stats
    global stats
    stats = {
        "total_crawled": 0,
        "total_success": 0,
        "total_failed": 0,
        "total_categories": 0,
        "by_level": defaultdict(int),
        "start_time": time.time(),
    }

    # Crawl ƒë·ªá quy v·ªõi t·ªëi ∆∞u
    all_categories = crawl_category_recursive_optimized(
        root_url, max_level=max_level, max_workers=max_workers
    )

    # Lo·∫°i b·ªè tr√πng l·∫∑p theo URL (gi·ªØ l·∫°i b·∫£n ƒë·∫ßu ti√™n)
    unique_categories = []
    seen_urls = set()
    for cat in all_categories:
        if cat["url"] not in seen_urls:
            unique_categories.append(cat)
            seen_urls.add(cat["url"])

    # S·∫Øp x·∫øp theo level v√† t√™n
    unique_categories.sort(key=lambda x: (x.get("level", 0), x["name"]))

    # L∆∞u k·∫øt qu·∫£
    output_file = "data/raw/categories_recursive_optimized.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_categories, f, ensure_ascii=False, indent=2)

    # In th·ªëng k√™
    print_stats()

    print(f"\nüíæ ƒê√£ l∆∞u v√†o: {output_file}")
    print(f"üì¶ T·ªïng s·ªë danh m·ª•c unique: {len(unique_categories)}")

    # Th·ªëng k√™ theo level
    level_counts = defaultdict(int)
    for cat in unique_categories:
        level = cat.get("level", 0)
        level_counts[level] += 1

    if level_counts:
        print("\nüìã Th·ªëng k√™ theo level:")
        for level in sorted(level_counts.keys()):
            print(f"  Level {level}: {level_counts[level]} danh m·ª•c")


if __name__ == "__main__":
    main()
