# Import tá»« file cÃ¹ng thÆ° má»¥c
import importlib.util
import json
import os
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

spec = importlib.util.spec_from_file_location(
    "extract_category_link_selenium",
    os.path.join(os.path.dirname(__file__), "extract_category_link_selenium.py"),
)
extract_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(extract_module)

crawl_with_selenium = extract_module.crawl_with_selenium
parse_categories = extract_module.parse_categories

# Set UTF-8 encoding cho stdout trÃªn Windows
if sys.platform == "win32":
    try:
        import io

        if hasattr(sys.stdout, "buffer") and not sys.stdout.closed:
            sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    except Exception:
        try:
            import io

            if hasattr(sys.stdout, "buffer"):
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
        except Exception:
            pass

# Thá»­ import tqdm, náº¿u khÃ´ng cÃ³ thÃ¬ dÃ¹ng fallback
try:
    from tqdm import tqdm

    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("âš ï¸  Khuyáº¿n nghá»‹ cÃ i Ä‘áº·t tqdm Ä‘á»ƒ hiá»ƒn thá»‹ progress bar: pip install tqdm")

    # Fallback progress bar Ä‘Æ¡n giáº£n
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", **kwargs):
            self.iterable = iterable
            self.total = total or (len(iterable) if iterable else 0)
            self.desc = desc
            self.n = 0
            self.start_time = time.time()

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    self.n += 1
                    self.update(1)
                    yield item
            else:
                return self

        def update(self, n=1):
            self.n += n
            if self.total > 0:
                pct = (self.n / self.total) * 100
                elapsed = time.time() - self.start_time
                if self.n > 0:
                    rate = self.n / elapsed if elapsed > 0 else 0
                    eta = (self.total - self.n) / rate if rate > 0 else 0
                    print(
                        f"\r{self.desc} {self.n}/{self.total} ({pct:.1f}%) | "
                        f"Tá»‘c Ä‘á»™: {rate:.2f}/s | ETA: {eta:.0f}s",
                        end="",
                        flush=True,
                    )

        def set_description(self, desc):
            self.desc = desc


# Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
os.makedirs("data/raw", exist_ok=True)

# Thread-safe locks vÃ  counters
stats_lock = Lock()
stats = {
    "total_crawled": 0,
    "total_success": 0,
    "total_failed": 0,
    "total_categories": 0,
    "by_level": defaultdict(int),
    "start_time": time.time(),
}


def crawl_single_category(
    url, parent_url, level, max_level, visited_urls, cache_dir="data/raw/cache", driver_pool=None
):
    """
    Crawl má»™t danh má»¥c Ä‘Æ¡n láº» (thread-safe)

    Args:
        driver_pool: Optional SeleniumDriverPool for driver reuse

    Returns:
        tuple: (success: bool, categories: list, error: str)
    """
    global stats

    # Kiá»ƒm tra cache
    cache_file = None
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        import hashlib

        url_hash = hashlib.md5(url.encode()).hexdigest()
        cache_file = os.path.join(cache_dir, f"{url_hash}.json")

        if os.path.exists(cache_file):
            try:
                with open(cache_file, encoding="utf-8") as f:
                    cached_data = json.load(f)
                cached_categories = cached_data.get("categories", [])
                with stats_lock:
                    stats["total_crawled"] += 1
                    stats["total_success"] += 1
                    stats["total_categories"] += len(cached_categories)
                return True, cached_categories, None
            except Exception as e:
                # Náº¿u cache file bá»‹ lá»—i, tiáº¿p tá»¥c crawl láº¡i
                print(f"  âš ï¸  Cache file lá»—i, sáº½ crawl láº¡i: {cache_file} - {str(e)}")
                pass

    try:
        # Crawl vá»›i Selenium (Æ°u tiÃªn driver pool náº¿u cÃ³)
        html_content = None
        if driver_pool is not None:
            driver = driver_pool.get_driver()
            if driver is not None:
                try:
                    # Import crawl_with_driver náº¿u cÃ³
                    try:
                        from extract_category_link_selenium import crawl_with_driver

                        html_content = crawl_with_driver(
                            driver, url, save_html=False, verbose=False
                        )
                    except (ImportError, AttributeError):
                        # Fallback: crawl_with_driver chÆ°a cÃ³
                        pass
                finally:
                    driver_pool.return_driver(driver)

        # Fallback: táº¡o driver riÃªng náº¿u pool khÃ´ng cÃ³ hoáº·c fail
        if html_content is None:
            html_content = crawl_with_selenium(url, save_html=False, verbose=False)

        # Parse danh má»¥c con
        child_categories = parse_categories(html_content, parent_url=url, level=level + 1)

        # Lá»c chá»‰ láº¥y cÃ¡c danh má»¥c cÃ³ hÃ¬nh áº£nh
        categories_with_images = [
            cat for cat in child_categories if cat.get("image_url", "").strip()
        ]

        # LÆ°u cache
        if cache_file:
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {"url": url, "categories": categories_with_images},
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
            except Exception:
                pass

        # Update stats
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_success"] += 1
            stats["total_categories"] += len(categories_with_images)
            stats["by_level"][level + 1] += len(categories_with_images)

        return True, categories_with_images, None

    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_failed"] += 1
        # Log chi tiáº¿t hÆ¡n cho debugging
        print(f"  âš ï¸  Lá»—i crawl {url}: [{error_type}] {error_msg}")
        return False, [], error_msg


def crawl_level_parallel(
    urls_to_crawl, parent_urls, level, max_level, visited_urls, max_workers=3, driver_pool=None
):
    """
    Crawl song song nhiá»u danh má»¥c cÃ¹ng level

    Args:
        urls_to_crawl: List cÃ¡c URL cáº§n crawl
        parent_urls: List cÃ¡c parent URL tÆ°Æ¡ng á»©ng
        level: Level hiá»‡n táº¡i
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        max_workers: Sá»‘ thread tá»‘i Ä‘a (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i)
        driver_pool: Optional SeleniumDriverPool for driver reuse

    Returns:
        dict: {url: (success, categories, error)}
    """
    results = {}

    # Lá»c cÃ¡c URL chÆ°a crawl
    tasks = []
    for url, parent_url in zip(urls_to_crawl, parent_urls, strict=False):
        if url not in visited_urls:
            tasks.append((url, parent_url))

    if not tasks:
        return results

    # Táº¡o progress bar
    desc = f"Level {level}"
    with tqdm(total=len(tasks), desc=desc, unit="danh má»¥c") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit táº¥t cáº£ tasks
            future_to_url = {}
            for url, parent_url in tasks:
                future = executor.submit(
                    crawl_single_category,
                    url,
                    parent_url,
                    level,
                    max_level,
                    visited_urls,
                    "data/raw/cache",
                    driver_pool,
                )
                future_to_url[future] = (url, parent_url)

            # Xá»­ lÃ½ káº¿t quáº£ khi hoÃ n thÃ nh
            for future in as_completed(future_to_url):
                url, parent_url = future_to_url[future]
                try:
                    success, categories, error = future.result(timeout=300)  # Timeout 5 phÃºt
                    results[url] = (success, categories, error)

                    # ÄÃ¡nh dáº¥u Ä‘Ã£ crawl (dÃ¹ thÃ nh cÃ´ng hay tháº¥t báº¡i)
                    visited_urls.add(url)

                    if success:
                        with stats_lock:
                            pbar.set_postfix({"âœ…": len(categories), "âŒ": stats["total_failed"]})
                    else:
                        with stats_lock:
                            pbar.set_postfix(
                                {"âœ…": stats["total_success"], "âŒ": stats["total_failed"]}
                            )
                        print(f"\n  âŒ Lá»—i crawl {url}: {error}")
                except Exception as e:
                    error_msg = str(e)
                    error_type = type(e).__name__
                    results[url] = (False, [], error_msg)
                    visited_urls.add(url)  # ÄÃ¡nh dáº¥u Ä‘Ã£ thá»­ crawl
                    with stats_lock:
                        stats["total_failed"] += 1
                    print(f"\n  âŒ Exception khi crawl {url}: [{error_type}] {error_msg}")

                pbar.update(1)

    return results


def crawl_category_recursive_optimized(
    root_urls, max_level=3, max_workers=3, visited_urls=None, all_categories=None
):
    """
    Crawl Ä‘á»‡ quy cÃ¡c danh má»¥c vá»›i tá»‘i Æ°u song song

    Args:
        root_urls: URL danh má»¥c gá»‘c (str) hoáº·c danh sÃ¡ch cÃ¡c URL gá»‘c (list[str])
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        max_workers: Sá»‘ thread tá»‘i Ä‘a cho má»—i level
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        all_categories: List táº¥t cáº£ cÃ¡c danh má»¥c Ä‘Ã£ crawl
    """
    if visited_urls is None:
        visited_urls = set()
    if all_categories is None:
        all_categories = []

    # Há»— trá»£ cáº£ single URL vÃ  list URLs
    if isinstance(root_urls, str):
        root_urls = [root_urls]

    # Initialize driver pool for reuse
    driver_pool = None
    try:
        # Try to import SeleniumDriverPool
        try:
            spec = importlib.util.spec_from_file_location(
                "crawl_utils",
                os.path.join(os.path.dirname(__file__), "utils.py"),
            )
            if spec and spec.loader:
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                SeleniumDriverPool = getattr(utils_module, "SeleniumDriverPool", None)
                if SeleniumDriverPool:
                    driver_pool = SeleniumDriverPool(
                        pool_size=max_workers, headless=True, timeout=90
                    )
                    print(f"âœ… ÄÃ£ khá»Ÿi táº¡o driver pool vá»›i {max_workers} drivers")
        except Exception:
            pass  # Fallback: khÃ´ng dÃ¹ng pool

        # Queue cÃ¡c URL cáº§n crawl theo level
        # Format: {level: [(url, parent_url), ...]}
        queue = defaultdict(list)
        queue[0] = [(url, None) for url in root_urls]

        # QUAN TRá»ŒNG: ThÃªm root categories vÃ o Ä‘áº§u káº¿t quáº£
        # Äá»ƒ Ä‘áº£m báº£o category hierarchy Ä‘áº§y Ä‘á»§ (root -> children -> grandchildren)
        import re

        # Äáº£m báº£o khÃ´ng thÃªm trÃ¹ng root category theo URL
        existing_root_urls = {c["url"] for c in all_categories if c.get("url")}
        for root_url in root_urls:
            if root_url in existing_root_urls:
                # Bá» qua URL trÃ¹ng, trÃ¡nh thÃªm duplicate root category
                continue

            match = re.search(r"/([^/]+)/(c\d+)", root_url)
            if match:
                root_slug = match.group(1)
                root_cat_id = match.group(2)
                root_name = root_slug.replace("-", " ").title()

                root_category = {
                    "name": root_name,
                    "slug": root_slug,
                    "url": root_url,
                    "image_url": "",
                    "parent_url": "",  # Root khÃ´ng cÃ³ parent
                    "level": 0,
                }
                all_categories.append(root_category)
                existing_root_urls.add(root_url)
                print(f"âœ… ÄÃ£ thÃªm root category: {root_name} ({root_cat_id})")

        # Crawl tá»«ng level má»™t
        for current_level in range(max_level + 1):
            if current_level not in queue or not queue[current_level]:
                continue

            urls_to_crawl = [url for url, _ in queue[current_level]]
            parent_urls = [parent for _, parent in queue[current_level]]

            # Lá»c cÃ¡c URL chÆ°a crawl
            new_urls = []
            new_parents = []
            for url, parent in zip(urls_to_crawl, parent_urls, strict=False):
                if url not in visited_urls:
                    new_urls.append(url)
                    new_parents.append(parent)

            if not new_urls:
                continue

            print(f"\n{'='*70}")
            print(f"Level {current_level}: Dang crawl {len(new_urls)} danh muc...")
            print(f"{'='*70}")

            # Crawl song song
            results = crawl_level_parallel(
                new_urls,
                new_parents,
                current_level,
                max_level,
                visited_urls,
                max_workers=max_workers,
                driver_pool=driver_pool,
            )

            # Xá»­ lÃ½ káº¿t quáº£ vÃ  chuáº©n bá»‹ level tiáº¿p theo
            for url, (success, categories, error) in results.items():
                if success:
                    # ThÃªm vÃ o danh sÃ¡ch tá»•ng
                    all_categories.extend(categories)

                    # ThÃªm cÃ¡c danh má»¥c con vÃ o queue level tiáº¿p theo
                    if current_level < max_level:
                        for cat in categories:
                            child_url = cat["url"]
                            if child_url not in visited_urls:
                                queue[current_level + 1].append((child_url, url))
                else:
                    print(f"  âŒ Lá»—i crawl {url}: {error}")

        return all_categories
    finally:
        # Cleanup driver pool
        if driver_pool is not None:
            try:
                driver_pool.cleanup()
                print("âœ… ÄÃ£ cleanup driver pool")
            except Exception:
                pass


def print_stats():
    """In thá»‘ng kÃª real-time"""
    global stats
    with stats_lock:
        elapsed = time.time() - stats["start_time"]
        rate = stats["total_crawled"] / elapsed if elapsed > 0 else 0

        print(f"\n{'='*70}")
        print("ğŸ“ˆ THá»NG KÃŠ")
        print(f"{'='*70}")
        print(f"â±  Thá»i gian: {elapsed:.1f}s")
        print(f"ğŸ“¥ ÄÃ£ crawl: {stats['total_crawled']} danh má»¥c")
        print(f"âœ… ThÃ nh cÃ´ng: {stats['total_success']}")
        print(f"âŒ Tháº¥t báº¡i: {stats['total_failed']}")
        print(f"ğŸ“Š Tá»•ng danh má»¥c tÃ¬m Ä‘Æ°á»£c: {stats['total_categories']}")
        print(f"âš¡ Tá»‘c Ä‘á»™: {rate:.2f} danh má»¥c/s")

        if stats["by_level"]:
            print("\nğŸ“‹ Theo level:")
            for level in sorted(stats["by_level"].keys()):
                print(f"  Level {level}: {stats['by_level'][level]} danh má»¥c")


def main():
    """HÃ m main Ä‘á»ƒ crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u"""

    # Há»— trá»£ nhiá»u root categories tá»« config file hoáº·c tham sá»‘
    # CÃ³ thá»ƒ cáº¥u hÃ¬nh qua:
    # 1. File JSON: data/raw/root_categories.json
    # 2. Biáº¿n mÃ´i trÆ°á»ng: TIKI_ROOT_CATEGORIES (comma-separated URLs)
    # 3. Default: danh sÃ¡ch máº·c Ä‘á»‹nh

    root_urls = []

    # Thá»­ Ä‘á»c tá»« file config
    config_file = "data/raw/root_categories.json"
    if os.path.exists(config_file):
        try:
            with open(config_file, encoding="utf-8") as f:
                config_data = json.load(f)
                if isinstance(config_data, list):
                    root_urls = config_data
                elif isinstance(config_data, dict) and "root_urls" in config_data:
                    root_urls = config_data["root_urls"]
                else:
                    print(f"âš ï¸  Config file cÃ³ cáº¥u trÃºc khÃ´ng há»£p lá»‡: {config_file}")
                    root_urls = []
                
                # Chá»‰ print success message náº¿u thá»±c sá»± load Ä‘Æ°á»£c URLs
                if root_urls:
                    print(f"âœ… ÄÃ£ load {len(root_urls)} root categories tá»« {config_file}")
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ Ä‘á»c config file: {e}")

    # Thá»­ Ä‘á»c tá»« biáº¿n mÃ´i trÆ°á»ng
    if not root_urls:
        env_urls = os.getenv("TIKI_ROOT_CATEGORIES", "")
        if env_urls:
            root_urls = [url.strip() for url in env_urls.split(",") if url.strip()]
            print(f"âœ… ÄÃ£ load {len(root_urls)} root categories tá»« biáº¿n mÃ´i trÆ°á»ng")

    # Default: danh sÃ¡ch máº·c Ä‘á»‹nh náº¿u khÃ´ng cÃ³ config
    if not root_urls:
        root_urls = [
            "https://tiki.vn/thoi-trang-nam/c915",
            "https://tiki.vn/thoi-trang-nu/c931",
            # CÃ³ thá»ƒ thÃªm cÃ¡c root categories khÃ¡c á»Ÿ Ä‘Ã¢y
            # "https://tiki.vn/nha-cua-doi-song/c1883",
            # "https://tiki.vn/dien-tu-dien-lanh/c4221",
        ]
        print("â„¹ï¸  Sá»­ dá»¥ng root categories máº·c Ä‘á»‹nh")

    # Äá»™ sÃ¢u tá»‘i Ä‘a
    max_level = int(os.getenv("TIKI_MAX_CATEGORY_LEVEL", "4"))

    # Sá»‘ thread song song (giá»›i háº¡n Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i server)
    max_workers = int(os.getenv("TIKI_CRAWL_MAX_WORKERS", "3"))

    print("=" * 70)
    print("ğŸš€ CRAWL Äá»† QUY CÃC DANH Má»¤C TIKI (Tá»I Æ¯U)")
    print("=" * 70)
    print(f"Sá»‘ root categories: {len(root_urls)}")
    for i, url in enumerate(root_urls, 1):
        print(f"  {i}. {url}")
    print(f"Äá»™ sÃ¢u tá»‘i Ä‘a: {max_level}")
    print(f"Sá»‘ thread song song: {max_workers}")
    print("Cache: data/raw/cache/")
    print("=" * 70)

    # Reset stats
    global stats
    stats = {
        "total_crawled": 0,
        "total_success": 0,
        "total_failed": 0,
        "total_categories": 0,
        "by_level": defaultdict(int),
        "start_time": time.time(),
    }

    # Crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u
    all_categories = crawl_category_recursive_optimized(
        root_urls, max_level=max_level, max_workers=max_workers
    )

    # Loáº¡i bá» trÃ¹ng láº·p theo URL (giá»¯ láº¡i báº£n Ä‘áº§u tiÃªn)
    unique_categories = []
    seen_urls = set()
    for cat in all_categories:
        if cat["url"] not in seen_urls:
            unique_categories.append(cat)
            seen_urls.add(cat["url"])

    # Sáº¯p xáº¿p theo level vÃ  tÃªn
    unique_categories.sort(key=lambda x: (x.get("level", 0), x["name"]))

    # LÆ°u káº¿t quáº£
    output_file = "data/raw/categories_recursive_optimized.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_categories, f, ensure_ascii=False, indent=2)

    # In thá»‘ng kÃª
    print_stats()

    print(f"\nğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_file}")
    print(f"ğŸ“¦ Tá»•ng sá»‘ danh má»¥c unique: {len(unique_categories)}")

    # Thá»‘ng kÃª theo level
    level_counts = defaultdict(int)
    for cat in unique_categories:
        level = cat.get("level", 0)
        level_counts[level] += 1

    if level_counts:
        print("\nğŸ“‹ Thá»‘ng kÃª theo level:")
        for level in sorted(level_counts.keys()):
            print(f"  Level {level}: {level_counts[level]} danh má»¥c")


if __name__ == "__main__":
    main()
