# Import tá»« file cÃ¹ng thÆ° má»¥c
import importlib.util
import json
import os
import sys
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

spec = importlib.util.spec_from_file_location(
    "extract_category_link_selenium",
    os.path.join(os.path.dirname(__file__), "extract_category_link_selenium.py"),
)
extract_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(extract_module)

crawl_with_selenium = extract_module.crawl_with_selenium
parse_categories = extract_module.parse_categories

# Set UTF-8 encoding cho stdout trÃªn Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

# Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
os.makedirs("data/raw", exist_ok=True)

# Thread-safe locks vÃ  counters
stats_lock = Lock()
stats = {
    "total_crawled": 0,
    "total_success": 0,
    "total_failed": 0,
    "total_categories": 0,
    "by_level": defaultdict(int),
    "start_time": time.time(),
}


def crawl_single_category(url, parent_url, level, max_level, visited_urls):
    """
    Crawl má»™t danh má»¥c Ä‘Æ¡n láº» (thread-safe)
    
    Returns:
        tuple: (success: bool, categories: list, error: str)
    """
    global stats
    
    try:
        # Crawl vá»›i Selenium
        html_content = crawl_with_selenium(url, save_html=False, verbose=False)
        
        # Parse danh má»¥c con
        child_categories = parse_categories(html_content, parent_url=url, level=level + 1)
        
        # Lá»c chá»‰ láº¥y cÃ¡c danh má»¥c cÃ³ hÃ¬nh áº£nh
        categories_with_images = [
            cat for cat in child_categories if cat.get("image_url", "").strip()
        ]
        
        # Update stats
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_success"] += 1
            stats["total_categories"] += len(categories_with_images)
            stats["by_level"][level + 1] += len(categories_with_images)
        
        return True, categories_with_images, None
    
    except Exception as e:
        error_msg = str(e)
        with stats_lock:
            stats["total_crawled"] += 1
            stats["total_failed"] += 1
        return False, [], error_msg


def crawl_level_parallel(urls_to_crawl, parent_urls, level, max_level, visited_urls, max_workers=3):
    """
    Crawl song song nhiá»u danh má»¥c cÃ¹ng level
    
    Args:
        urls_to_crawl: List cÃ¡c URL cáº§n crawl
        parent_urls: List cÃ¡c parent URL tÆ°Æ¡ng á»©ng
        level: Level hiá»‡n táº¡i
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        max_workers: Sá»‘ thread tá»‘i Ä‘a
    
    Returns:
        dict: {url: (success, categories, error)}
    """
    results = {}
    
    # Lá»c cÃ¡c URL chÆ°a crawl
    tasks = []
    for url, parent_url in zip(urls_to_crawl, parent_urls):
        if url not in visited_urls:
            tasks.append((url, parent_url))
    
    if not tasks:
        return results
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Level {level}: Äang crawl {len(tasks)} danh má»¥c...")
    print(f"{'='*70}")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit táº¥t cáº£ tasks
        future_to_url = {}
        for url, parent_url in tasks:
            future = executor.submit(
                crawl_single_category,
                url,
                parent_url,
                level,
                max_level,
                visited_urls,
            )
            future_to_url[future] = (url, parent_url)
        
        # Xá»­ lÃ½ káº¿t quáº£ khi hoÃ n thÃ nh
        completed = 0
        for future in as_completed(future_to_url):
            url, parent_url = future_to_url[future]
            try:
                success, categories, error = future.result(timeout=300)  # Timeout 5 phÃºt
                results[url] = (success, categories, error)
                
                # ÄÃ¡nh dáº¥u Ä‘Ã£ crawl
                visited_urls.add(url)
                
                completed += 1
                if success:
                    print(f"  âœ… [{completed}/{len(tasks)}] {url}: {len(categories)} danh má»¥c con")
                else:
                    print(f"  âŒ [{completed}/{len(tasks)}] {url}: Lá»—i - {error}")
            except Exception as e:
                error_msg = str(e)
                results[url] = (False, [], error_msg)
                visited_urls.add(url)
                with stats_lock:
                    stats["total_failed"] += 1
                completed += 1
                print(f"  âŒ [{completed}/{len(tasks)}] {url}: Exception - {error_msg}")
    
    return results


def crawl_category_recursive_optimized(
    root_url, max_level=4, max_workers=3, visited_urls=None, all_categories=None
):
    """
    Crawl Ä‘á»‡ quy cÃ¡c danh má»¥c vá»›i tá»‘i Æ°u song song
    
    Args:
        root_url: URL danh má»¥c gá»‘c
        max_level: Äá»™ sÃ¢u tá»‘i Ä‘a
        max_workers: Sá»‘ thread tá»‘i Ä‘a cho má»—i level
        visited_urls: Set cÃ¡c URL Ä‘Ã£ crawl
        all_categories: List táº¥t cáº£ cÃ¡c danh má»¥c Ä‘Ã£ crawl
    """
    if visited_urls is None:
        visited_urls = set()
    if all_categories is None:
        all_categories = []
    
    # Queue cÃ¡c URL cáº§n crawl theo level
    # Format: {level: [(url, parent_url), ...]}
    queue = defaultdict(list)
    queue[0] = [(root_url, None)]
    
    # Crawl tá»«ng level má»™t
    for current_level in range(max_level + 1):
        if current_level not in queue or not queue[current_level]:
            continue
        
        urls_to_crawl = [url for url, _ in queue[current_level]]
        parent_urls = [parent for _, parent in queue[current_level]]
        
        # Lá»c cÃ¡c URL chÆ°a crawl
        new_urls = []
        new_parents = []
        for url, parent in zip(urls_to_crawl, parent_urls):
            if url not in visited_urls:
                new_urls.append(url)
                new_parents.append(parent)
        
        if not new_urls:
            continue
        
        # Crawl song song
        results = crawl_level_parallel(
            new_urls,
            new_parents,
            current_level,
            max_level,
            visited_urls,
            max_workers=max_workers,
        )
        
        # Xá»­ lÃ½ káº¿t quáº£ vÃ  chuáº©n bá»‹ level tiáº¿p theo
        for url, (success, categories, error) in results.items():
            if success:
                # ThÃªm vÃ o danh sÃ¡ch tá»•ng
                all_categories.extend(categories)
                
                # ThÃªm cÃ¡c danh má»¥c con vÃ o queue level tiáº¿p theo
                if current_level < max_level:
                    for cat in categories:
                        child_url = cat["url"]
                        if child_url not in visited_urls:
                            queue[current_level + 1].append((child_url, url))
    
    return all_categories


def print_stats():
    """In thá»‘ng kÃª real-time"""
    global stats
    with stats_lock:
        elapsed = time.time() - stats["start_time"]
        rate = stats["total_crawled"] / elapsed if elapsed > 0 else 0
        
        print(f"\n{'='*70}")
        print("ğŸ“ˆ THá»NG KÃŠ")
        print(f"{'='*70}")
        print(f"â±  Thá»i gian: {elapsed:.1f}s")
        print(f"ğŸ“¥ ÄÃ£ crawl: {stats['total_crawled']} danh má»¥c")
        print(f"âœ… ThÃ nh cÃ´ng: {stats['total_success']}")
        print(f"âŒ Tháº¥t báº¡i: {stats['total_failed']}")
        print(f"ğŸ“Š Tá»•ng danh má»¥c tÃ¬m Ä‘Æ°á»£c: {stats['total_categories']}")
        print(f"âš¡ Tá»‘c Ä‘á»™: {rate:.2f} danh má»¥c/s")
        
        if stats["by_level"]:
            print("\nğŸ“‹ Theo level:")
            for level in sorted(stats["by_level"].keys()):
                print(f"  Level {level}: {stats['by_level'][level]} danh má»¥c")


def load_env_file():
    """Load biáº¿n mÃ´i trÆ°á»ng tá»« file .env á»Ÿ root project"""
    try:
        # TÃ¬m file .env: Tá»« file nÃ y (src/pipelines/crawl/...) ra root
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
        env_path = os.path.join(project_root, '.env')
        
        if os.path.exists(env_path):
            print(f"ğŸ“„ Loading config from {env_path}")
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, val = line.split('=', 1)
                        if key not in os.environ: # KhÃ´ng override náº¿u Ä‘Ã£ set
                            os.environ[key] = val
    except Exception as e:
        print(f"âš ï¸  Could not load .env file: {e}")

def main():
    """HÃ m main Ä‘á»ƒ crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u"""
    
    # Load env file first
    load_env_file()
    
    # URL danh má»¥c gá»‘c tá»« Env Var
    default_url = "https://tiki.vn/nha-cua-doi-song/c1883"
    root_url = os.getenv("CRAWL_ROOT_CATEGORY_URL", default_url)
    
    # Äá»™ sÃ¢u tá»‘i Ä‘a (tÄƒng lÃªn 4 Ä‘á»ƒ bao quÃ¡t háº¿t)
    max_level = 4
    
    # Sá»‘ thread song song (tÄƒng lÃªn 5 Ä‘á»ƒ crawl nhanh hÆ¡n)
    max_workers = 5
    
    print("=" * 70)
    print("ğŸš€ CRAWL Äá»† QUY CÃC DANH Má»¤C TIKI (Tá»I Æ¯U)")
    print("=" * 70)
    print(f"URL gá»‘c: {root_url}")
    print(f"Äá»™ sÃ¢u tá»‘i Ä‘a: {max_level}")
    print(f"Sá»‘ thread song song: {max_workers}")
    print("=" * 70)
    
    # Reset stats
    global stats
    stats = {
        "total_crawled": 0,
        "total_success": 0,
        "total_failed": 0,
        "total_categories": 0,
        "by_level": defaultdict(int),
        "start_time": time.time(),
    }
    
    # Crawl Ä‘á»‡ quy vá»›i tá»‘i Æ°u
    all_categories = crawl_category_recursive_optimized(
        root_url, max_level=max_level, max_workers=max_workers
    )
    
    # Loáº¡i bá» trÃ¹ng láº·p theo URL (giá»¯ láº¡i báº£n Ä‘áº§u tiÃªn)
    unique_categories = []
    seen_urls = set()
    for cat in all_categories:
        if cat["url"] not in seen_urls:
            unique_categories.append(cat)
            seen_urls.add(cat["url"])
    
    # Sáº¯p xáº¿p theo level vÃ  tÃªn
    unique_categories.sort(key=lambda x: (x.get("level", 0), x["name"]))
    
    # LÆ°u káº¿t quáº£ vÃ o file mÃ  DAG sá»­ dá»¥ng
    output_file = "data/raw/categories_recursive_optimized.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_categories, f, ensure_ascii=False, indent=2)
    
    # In thá»‘ng kÃª
    print_stats()
    
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_file}")
    print(f"ğŸ“¦ Tá»•ng sá»‘ danh má»¥c unique: {len(unique_categories)}")
    
    # Thá»‘ng kÃª theo level
    level_counts = defaultdict(int)
    for cat in unique_categories:
        level = cat.get("level", 0)
        level_counts[level] += 1
    
    if level_counts:
        print("\nğŸ“‹ Thá»‘ng kÃª theo level:")
        for level in sorted(level_counts.keys()):
            print(f"  Level {level}: {level_counts[level]} danh má»¥c")


if __name__ == "__main__":
    main()
