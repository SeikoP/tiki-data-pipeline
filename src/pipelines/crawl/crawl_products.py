import json
import os
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from urllib.parse import parse_qs, urljoin

import requests

# Lazy import BeautifulSoup Ä‘á»ƒ trÃ¡nh timeout khi load DAG
# from bs4 import BeautifulSoup  # Moved to function level

# Import shared utilities - há»— trá»£ cáº£ relative vÃ  absolute import
try:
    # Thá»­ relative import trÆ°á»›c (khi cháº¡y nhÆ° package)
    from .utils import (
        DEFAULT_CACHE_DIR,
        DEFAULT_DATA_DIR,
        DEFAULT_PRODUCT_LIST_CACHE_DIR,
        DEFAULT_PRODUCTS_DIR,
        RateLimiter,
        atomic_write_json,
        ensure_dir,
        extract_product_id_from_url,
        normalize_url,
        parse_price,
        parse_sales_count,
        safe_read_json,
        setup_utf8_encoding,
    )
except ImportError:
    # Fallback: absolute import (khi Ä‘Æ°á»£c load qua importlib)
    import os

    # TÃ¬m utils.py trong cÃ¹ng thÆ° má»¥c
    current_dir = os.path.dirname(os.path.abspath(__file__))
    utils_path = os.path.join(current_dir, "utils.py")
    if os.path.exists(utils_path):
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
        if spec and spec.loader:
            utils_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(utils_module)
            setup_utf8_encoding = utils_module.setup_utf8_encoding
            parse_sales_count = utils_module.parse_sales_count
            parse_price = utils_module.parse_price
            ensure_dir = utils_module.ensure_dir
            atomic_write_json = utils_module.atomic_write_json
            safe_read_json = utils_module.safe_read_json
            extract_product_id_from_url = utils_module.extract_product_id_from_url
            normalize_url = utils_module.normalize_url
            RateLimiter = utils_module.RateLimiter
            DEFAULT_DATA_DIR = utils_module.DEFAULT_DATA_DIR
            DEFAULT_CACHE_DIR = utils_module.DEFAULT_CACHE_DIR
            DEFAULT_PRODUCT_LIST_CACHE_DIR = utils_module.DEFAULT_PRODUCT_LIST_CACHE_DIR
            DEFAULT_PRODUCTS_DIR = utils_module.DEFAULT_PRODUCTS_DIR
        else:
            raise ImportError(f"KhÃ´ng thá»ƒ load utils tá»« {utils_path}") from None
    else:
        raise ImportError(f"KhÃ´ng tÃ¬m tháº¥y utils.py táº¡i {utils_path}") from None

# Setup UTF-8 encoding
setup_utf8_encoding()


# Lazy import tqdm Ä‘á»ƒ trÃ¡nh timeout khi load DAG
# Sáº½ import trong functions khi cáº§n
def _get_tqdm():
    """Lazy import tqdm - chá»‰ import khi cáº§n"""
    try:
        from tqdm import tqdm

        return tqdm
    except ImportError:
        # Fallback: táº¡o fake tqdm class náº¿u khÃ´ng cÃ³
        class FakeTqdm:
            def __init__(self, iterable=None, total=None, desc="", **kwargs):
                self.iterable = iterable
                self.total = total or (len(iterable) if iterable else 0)
                self.desc = desc
                self.n = 0
                self.start_time = time.time()

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

            def __iter__(self):
                if self.iterable:
                    for item in self.iterable:
                        self.n += 1
                        self.update(1)
                        yield item
                else:
                    return self

            def update(self, n=1):
                self.n += n
                if self.total > 0:
                    pct = (self.n / self.total) * 100
                    elapsed = time.time() - self.start_time
                    if self.n > 0:
                        rate = self.n / elapsed if elapsed > 0 else 0
                        eta = (self.total - self.n) / rate if rate > 0 else 0
                        print(
                            f"\r{self.desc} {self.n}/{self.total} ({pct:.1f}%) | "
                            f"Tá»‘c Ä‘á»™: {rate:.2f}/s | ETA: {eta:.0f}s",
                            end="",
                            flush=True,
                        )

            def set_postfix(self, _postfix=None):
                pass

        return FakeTqdm


# Lazy import Selenium vÃ  webdriver_manager Ä‘á»ƒ trÃ¡nh timeout khi load DAG
# KhÃ´ng import á»Ÿ top level - sáº½ import trong functions khi cáº§n
HAS_SELENIUM = None  # Sáº½ Ä‘Æ°á»£c check khi cáº§n
HAS_WEBDRIVER_MANAGER = None  # Sáº½ Ä‘Æ°á»£c check khi cáº§n


def _check_selenium_available():
    """
    Check xem Selenium cÃ³ sáºµn khÃ´ng (lazy check)
    """
    global HAS_SELENIUM, HAS_WEBDRIVER_MANAGER
    if HAS_SELENIUM is not None:
        return HAS_SELENIUM

    try:
        from selenium import webdriver  # noqa: F401

        HAS_SELENIUM = True

        # Thá»­ import webdriver-manager
        try:
            from webdriver_manager.chrome import ChromeDriverManager  # noqa: F401

            HAS_WEBDRIVER_MANAGER = True
        except ImportError:
            HAS_WEBDRIVER_MANAGER = False
    except ImportError:
        HAS_SELENIUM = False
        HAS_WEBDRIVER_MANAGER = False

    return HAS_SELENIUM


# Táº¡o thÆ° má»¥c output
# Táº¡o thÆ° má»¥c output
os.makedirs(DEFAULT_PRODUCTS_DIR, exist_ok=True)
os.makedirs(DEFAULT_PRODUCT_LIST_CACHE_DIR, exist_ok=True)

# Thread-safe locks vÃ  stats
stats_lock = Lock()
stats = {
    "total_categories": 0,
    "total_products": 0,
    "total_pages": 0,
    "total_success": 0,
    "total_failed": 0,
    "start_time": time.time(),
}


def get_page_with_selenium(url, timeout=30, use_redis_cache=True, use_rate_limiting=True):
    """Láº¥y HTML cá»§a trang vá»›i Selenium (cho dynamic content)

    Args:
        url: URL cáº§n crawl
        timeout: Timeout cho page load
        use_redis_cache: CÃ³ dÃ¹ng Redis cache khÃ´ng
        use_rate_limiting: CÃ³ dÃ¹ng rate limiting khÃ´ng
    """
    # Thá»­ Redis cache trÆ°á»›c
    if use_redis_cache:
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_html = redis_cache.get_cached_html(url)
                if cached_html:
                    return cached_html
        except Exception:
            pass  # Fallback vá» crawl

    # Adaptive Rate Limiting
    adaptive_limiter = None
    if use_rate_limiting:
        try:
            from urllib.parse import urlparse

            from pipelines.crawl.storage.adaptive_rate_limiter import get_adaptive_rate_limiter

            adaptive_limiter = get_adaptive_rate_limiter("redis://redis:6379/2")
            if adaptive_limiter:
                domain = urlparse(url).netloc or "tiki.vn"
                adaptive_limiter.wait(domain)
        except Exception:
            time.sleep(0.7)  # Fixed delay fallback

    # Lazy import Ä‘á»ƒ trÃ¡nh timeout khi load DAG
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service

    # Check Selenium availability
    if not _check_selenium_available():
        raise ImportError("Selenium chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
    }
    chrome_options.add_experimental_option("prefs", prefs)
    # Faster page load
    try:
        chrome_options.page_load_strategy = "eager"
    except Exception:
        pass

    # Æ¯u tiÃªn dÃ¹ng ChromeDriver cÃ³ sáºµn trong PATH (nhanh nháº¥t)
    try:
        driver = webdriver.Chrome(options=chrome_options)
    except Exception as e:
        # Náº¿u khÃ´ng cÃ³ ChromeDriver trong PATH, thá»­ webdriver-manager
        error_msg = str(e).lower()
        if "chromedriver" in error_msg or "driver" in error_msg:
            if HAS_WEBDRIVER_MANAGER:
                try:
                    # Táº¯t log cá»§a webdriver_manager Ä‘á»ƒ giáº£m noise
                    import logging
                    import os
                    import stat

                    from webdriver_manager.chrome import ChromeDriverManager

                    wdm_logger = logging.getLogger("WDM")
                    wdm_logger.setLevel(logging.WARNING)

                    # Install ChromeDriver
                    driver_path = ChromeDriverManager().install()

                    # QUAN TRá»ŒNG: Set quyá»n thá»±c thi cho ChromeDriver (fix lá»—i status code 127)
                    # Äáº·c biá»‡t cáº§n thiáº¿t trong WSL2/Linux
                    try:
                        os.chmod(
                            driver_path,
                            os.stat(driver_path).st_mode
                            | stat.S_IEXEC
                            | stat.S_IXGRP
                            | stat.S_IXOTH,
                        )
                    except Exception:
                        pass  # Náº¿u khÃ´ng set Ä‘Æ°á»£c quyá»n, váº«n thá»­ tiáº¿p

                    service = Service(driver_path)
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                except Exception:
                    # Náº¿u webdriver-manager cÅ©ng fail, raise lá»—i gá»‘c
                    raise e from None
            else:
                raise e from None
        else:
            # Lá»—i khÃ¡c, raise ngay
            raise
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        time.sleep(0.5)  # Chá» JavaScript load (optimized)

        # Scroll Ä‘á»ƒ load lazy images (optimized)
        try:
            driver.execute_script("window.scrollTo(0, 500);")
            time.sleep(0.3)
            driver.execute_script("window.scrollTo(0, 1500);")
            time.sleep(0.5)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
        except Exception:
            pass

        html = driver.page_source

        # Cache HTML vÃ o Redis sau khi crawl thÃ nh cÃ´ng (vá»›i canonical URL)
        if use_redis_cache and html:
            try:
                from pipelines.crawl.config import REDIS_CACHE_TTL_HTML
                from pipelines.crawl.storage.redis_cache import get_redis_cache

                redis_cache = get_redis_cache("redis://redis:6379/1")
                if redis_cache:
                    # CRITICAL: Chuáº©n hÃ³a URL trÆ°á»›c khi cache Ä‘á»ƒ maximize hit rate
                    canonical_url = redis_cache._canonicalize_url(url)
                    redis_cache.cache_html(canonical_url, html, ttl=REDIS_CACHE_TTL_HTML)  # 7 days
            except Exception:
                pass  # Ignore cache errors

        return html
    finally:
        driver.quit()


def get_page_with_requests(url, max_retries=3, use_redis_cache=True, use_rate_limiting=True):
    """Láº¥y HTML cá»§a trang vá»›i requests (nhanh hÆ¡n nhÆ°ng khÃ´ng há»— trá»£ JS)

    Args:
        url: URL cáº§n crawl
        max_retries: Sá»‘ láº§n retry tá»‘i Ä‘a
        use_redis_cache: CÃ³ dÃ¹ng Redis cache khÃ´ng
        use_rate_limiting: CÃ³ dÃ¹ng rate limiting khÃ´ng
    """
    # Thá»­ Redis cache trÆ°á»›c
    if use_redis_cache:
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_html = redis_cache.get_cached_html(url)
                if cached_html:
                    return cached_html
        except Exception:
            pass  # Fallback vá» crawl

    # Adaptive Rate Limiting
    adaptive_limiter = None
    if use_rate_limiting:
        try:
            from urllib.parse import urlparse

            from pipelines.crawl.storage.adaptive_rate_limiter import get_adaptive_rate_limiter

            adaptive_limiter = get_adaptive_rate_limiter("redis://redis:6379/2")
            if adaptive_limiter:
                domain = urlparse(url).netloc or "tiki.vn"
                adaptive_limiter.wait(domain)
        except Exception:
            time.sleep(0.7)  # Fixed delay fallback

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            response.encoding = "utf-8"
            html = response.text

            # Cache HTML vÃ o Redis sau khi crawl thÃ nh cÃ´ng (vá»›i canonical URL)
            if use_redis_cache and html:
                try:
                    from pipelines.crawl.config import REDIS_CACHE_TTL_HTML
                    from pipelines.crawl.storage.redis_cache import get_redis_cache

                    redis_cache = get_redis_cache("redis://redis:6379/1")
                    if redis_cache:
                        # CRITICAL: Chuáº©n hÃ³a URL trÆ°á»›c khi cache Ä‘á»ƒ maximize hit rate
                        canonical_url = redis_cache._canonicalize_url(url)
                        redis_cache.cache_html(
                            canonical_url, html, ttl=REDIS_CACHE_TTL_HTML
                        )  # 7 days
                except Exception:
                    pass  # Ignore cache errors

            # Record success cho adaptive rate limiter
            if adaptive_limiter:
                try:
                    from urllib.parse import urlparse

                    domain = urlparse(url).netloc or "tiki.vn"
                    adaptive_limiter.record_success(domain)
                except Exception:
                    pass

            return html
        except requests.exceptions.RequestException as e:
            # Record error cho adaptive rate limiter
            if adaptive_limiter:
                try:
                    from urllib.parse import urlparse

                    domain = urlparse(url).netloc or "tiki.vn"
                    error_type_str = None
                    if "429" in str(e) or "Too Many Requests" in str(e):
                        error_type_str = "429"
                    elif "timeout" in str(e).lower():
                        error_type_str = "timeout"
                    adaptive_limiter.record_error(domain, error_type=error_type_str)
                except Exception:
                    pass

            if attempt == max_retries - 1:
                raise
            time.sleep(2**attempt)  # Exponential backoff

    return None


def parse_products_from_next_data(html_content):
    """
    Parse sáº£n pháº©m tá»« __NEXT_DATA__ (Next.js)
    """
    # Lazy import Ä‘á»ƒ trÃ¡nh timeout khi load DAG
    from bs4 import BeautifulSoup

    products = []

    try:
        # TÃ¬m script tag chá»©a __NEXT_DATA__
        soup = BeautifulSoup(html_content, "html.parser")
        next_data_script = soup.find("script", id="__NEXT_DATA__")

        if not next_data_script:
            return []

        # Parse JSON
        next_data = json.loads(next_data_script.string)

        # Äi sÃ¢u vÃ o cáº¥u trÃºc Next.js Ä‘á»ƒ tÃ¬m product data
        # Tiki cÃ³ thá»ƒ lÆ°u products á»Ÿ nhiá»u nÆ¡i trong cáº¥u trÃºc
        def find_products_in_dict(obj, path=""):
            """
            Äá»‡ quy tÃ¬m products trong nested dict.
            """
            if isinstance(obj, dict):
                # Kiá»ƒm tra cÃ¡c key cÃ³ thá»ƒ chá»©a products
                if "products" in obj and isinstance(obj["products"], list):
                    return obj["products"]
                if "items" in obj and isinstance(obj["items"], list):
                    # Kiá»ƒm tra xem cÃ³ pháº£i product items khÃ´ng
                    if obj["items"] and isinstance(obj["items"][0], dict):
                        if any(
                            key in obj["items"][0] for key in ["id", "product_id", "name", "price"]
                        ):
                            return obj["items"]
                if "data" in obj:
                    result = find_products_in_dict(obj["data"], path + ".data")
                    if result:
                        return result
                if "props" in obj:
                    result = find_products_in_dict(obj["props"], path + ".props")
                    if result:
                        return result
                if "pageProps" in obj:
                    result = find_products_in_dict(obj["pageProps"], path + ".pageProps")
                    if result:
                        return result
                if "initialState" in obj:
                    result = find_products_in_dict(obj["initialState"], path + ".initialState")
                    if result:
                        return result

                # Äá»‡ quy táº¥t cáº£ values
                for key, value in obj.items():
                    result = find_products_in_dict(value, f"{path}.{key}")
                    if result:
                        return result

            elif isinstance(obj, list):
                # Náº¿u lÃ  list, kiá»ƒm tra pháº§n tá»­ Ä‘áº§u tiÃªn
                if obj and isinstance(obj[0], dict):
                    # Kiá»ƒm tra xem cÃ³ pháº£i product objects khÃ´ng
                    first_item = obj[0]
                    if any(key in first_item for key in ["id", "product_id", "name", "price"]):
                        return obj

                # Äá»‡ quy cÃ¡c pháº§n tá»­
                for i, item in enumerate(obj):
                    result = find_products_in_dict(item, f"{path}[{i}]")
                    if result:
                        return result

            return None

        product_data = find_products_in_dict(next_data)

        if product_data:
            for item in product_data:
                try:
                    # Extract thÃ´ng tin sáº£n pháº©m
                    product_id = str(
                        item.get("id") or item.get("product_id") or item.get("sku") or ""
                    )
                    if not product_id:
                        continue

                    name = item.get("name") or item.get("title") or ""

                    # Láº¥y URL
                    url = item.get("url") or item.get("link") or ""
                    if not url or not url.startswith("http"):
                        if product_id:
                            url = f"https://tiki.vn/p/{product_id}"

                    # Láº¥y image (Ä‘á»ƒ cÃ³ thá»ƒ dÃ¹ng preview)
                    image_url = (
                        item.get("image_url")
                        or item.get("thumbnail_url")
                        or item.get("images", [{}])[0].get("url", "")
                        if isinstance(item.get("images"), list)
                        else ""
                    )

                    # Extract sá»‘ lÆ°á»£ng bÃ¡n - dÃ¹ng shared utility
                    sales_count_raw = (
                        item.get("sales_count")
                        or item.get("quantity_sold")
                        or item.get("sold_count")
                        or item.get("total_sold")
                        or item.get("order_count")
                        or item.get("sales_quantity")
                        or item.get("quantity")
                        or item.get("sold")
                        or item.get("total_quantity_sold")
                    )
                    sales_count = parse_sales_count(sales_count_raw)

                    product = {
                        "product_id": product_id,
                        "name": name,
                        "url": url,
                        "image_url": image_url,
                        "sales_count": sales_count,
                    }

                    if product_id and name:
                        products.append(product)

                except Exception:
                    continue

    except Exception:
        pass

    return products


def parse_products_from_html(html_content, category_url):
    """
    Parse danh sÃ¡ch sáº£n pháº©m tá»« HTML.
    """
    # Lazy import Ä‘á»ƒ trÃ¡nh timeout khi load DAG
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html_content, "html.parser")
    products = []

    # CÃ¡ch 1: Parse tá»« __NEXT_DATA__ (Æ°u tiÃªn)
    next_data_products = parse_products_from_next_data(html_content)
    if next_data_products:
        # ThÃªm category_url vÃ  Ä‘áº£m báº£o sales_count cÃ³ trong má»—i product
        for product in next_data_products:
            product["category_url"] = category_url
            product["crawled_at"] = time.strftime("%Y-%m-%d %H:%M:%S")
            # Äáº£m báº£o sales_count luÃ´n cÃ³ (ká»ƒ cáº£ None)
            if "sales_count" not in product:
                product["sales_count"] = None
        products.extend(next_data_products)
        return products

    # CÃ¡ch 2: Parse tá»« HTML elements (fallback)
    # TÃ¬m táº¥t cáº£ link cÃ³ pattern /p/
    all_links = soup.find_all("a", href=re.compile(r"/p/\d+"))

    seen_product_ids = set()

    for link in all_links:
        try:
            product_url = link.get("href", "")
            if not product_url:
                continue

            # Chuáº©n hÃ³a URL
            if product_url.startswith("/"):
                product_url = urljoin("https://tiki.vn", product_url)
            elif not product_url.startswith("http"):
                continue

            # Extract product ID tá»« URL
            product_id_match = re.search(r"/p/(\d+)", product_url)
            if not product_id_match:
                continue

            product_id = product_id_match.group(1)
            if product_id in seen_product_ids:
                continue

            seen_product_ids.add(product_id)

            # TÃ¬m parent container
            parent = link.find_parent()
            if not parent:
                parent = link

            # Láº¥y tÃªn sáº£n pháº©m tá»« parent hoáº·c link
            name = ""
            # Thá»­ tá»« title
            name = link.get("title", "") or link.get("aria-label", "")

            # Thá»­ tÃ¬m trong parent
            if not name:
                title_elem = parent.find(
                    ["h3", "h2", "div"], class_=re.compile(r"title|name", re.I)
                )
                if title_elem:
                    name = title_elem.get_text(strip=True)

            if not name:
                # Láº¥y text tá»« link
                name = link.get_text(strip=True)

            # Láº¥y hÃ¬nh áº£nh (Ä‘á»ƒ cÃ³ thá»ƒ dÃ¹ng preview)
            image_url = ""
            img_elem = parent.find("img") or link.find("img")
            if img_elem:
                image_url = (
                    img_elem.get("src", "")
                    or img_elem.get("data-src", "")
                    or img_elem.get("data-lazy-src", "")
                )
                if image_url:
                    if image_url.startswith("//"):
                        image_url = "https:" + image_url
                    elif image_url.startswith("/"):
                        image_url = urljoin("https://tiki.vn", image_url)

            # Extract sá»‘ lÆ°á»£ng bÃ¡n tá»« HTML
            sales_count = None
            # TÃ¬m text chá»©a "Ä‘Ã£ bÃ¡n", "bÃ¡n", "sold"
            sales_text = ""
            sales_elem = parent.find(string=re.compile(r"Ä‘Ã£\s*bÃ¡n|bÃ¡n|sold", re.I))
            if sales_elem:
                sales_text = sales_elem.strip()
            else:
                # TÃ¬m trong cÃ¡c tháº» con
                for elem in parent.find_all(
                    ["span", "div", "p"], string=re.compile(r"Ä‘Ã£\s*bÃ¡n|bÃ¡n|sold", re.I)
                ):
                    sales_text = elem.get_text(strip=True)
                    break

            if sales_text:
                # Parse sá»‘ tá»« text - dÃ¹ng shared utility
                sales_count = parse_sales_count(sales_text)

            # Táº¡o object sáº£n pháº©m
            product = {
                "product_id": product_id,
                "name": name,
                "url": product_url,
                "category_url": category_url,
                "image_url": image_url,
                "sales_count": sales_count,
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            }

            # Chá»‰ thÃªm náº¿u cÃ³ Ä‘á»§ thÃ´ng tin cÆ¡ báº£n
            if product_id and name:
                products.append(product)

        except Exception:
            continue

    return products


def get_total_pages(html_content):
    """
    Láº¥y tá»•ng sá»‘ trang tá»« HTML.
    """
    # Lazy import Ä‘á»ƒ trÃ¡nh timeout khi load DAG
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html_content, "html.parser")

    # TÃ¬m pháº§n phÃ¢n trang
    pagination_selectors = [
        ".pagination",
        '[class*="pagination"]',
        '[data-view-id="product_list_pagination"]',
    ]

    max_page = 1
    for selector in pagination_selectors:
        pagination = soup.select_one(selector)
        if pagination:
            # TÃ¬m cÃ¡c link phÃ¢n trang
            page_links = pagination.find_all("a")
            for link in page_links:
                page_text = link.get_text(strip=True)
                if page_text.isdigit():
                    try:
                        page_num = int(page_text)
                        max_page = max(max_page, page_num)
                    except Exception:
                        pass

    # Hoáº·c thá»­ tÃ¬m tá»« text "Trang X/Y"
    page_info = soup.find(string=re.compile(r"trang\s*\d+", re.I))
    if page_info:
        page_match = re.search(r"trang\s*\d+.*?(\d+)", page_info, re.I)
        if page_match:
            try:
                max_page = int(page_match.group(1))
            except Exception:
                pass

    return max_page


def get_category_page_url(category_url, page=1):
    """
    Táº¡o URL trang phÃ¢n trang cá»§a danh má»¥c.
    """
    if "?" in category_url:
        # Náº¿u Ä‘Ã£ cÃ³ query params
        base_url, query_string = category_url.split("?", 1)
        params = parse_qs(query_string)
        params["page"] = [str(page)]
        new_query = "&".join([f"{k}={v[0]}" for k, v in params.items()])
        return f"{base_url}?{new_query}"
    else:
        return f"{category_url}?page={page}"


def crawl_category_products(
    category_url,
    max_pages=None,
    use_selenium=False,
    cache_dir=DEFAULT_PRODUCT_LIST_CACHE_DIR,
    use_redis_cache=True,
    use_rate_limiting=True,
):
    """Crawl táº¥t cáº£ sáº£n pháº©m tá»« má»™t danh má»¥c.

    Args:
        category_url: URL cá»§a category
        max_pages: Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl
        use_selenium: CÃ³ dÃ¹ng Selenium khÃ´ng
        cache_dir: ThÆ° má»¥c cache (fallback náº¿u Redis khÃ´ng available)
        use_redis_cache: CÃ³ dÃ¹ng Redis cache khÃ´ng (máº·c Ä‘á»‹nh True)
    """

    all_products = []

    # Thá»­ Redis cache trÆ°á»›c (nhanh hÆ¡n, distributed)
    redis_cache = None
    if use_redis_cache:
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_products = redis_cache.get_cached_products(category_url)
                if cached_products:
                    print(f"[Redis Cache] âœ… Hit cache cho {category_url[:60]}...")
                    return cached_products
        except Exception:
            # Redis khÃ´ng available, fallback vá» file cache
            pass  # Silent fallback

    # Fallback: Kiá»ƒm tra file cache
    cache_file = None
    if cache_dir:
        import hashlib

        url_hash = hashlib.md5(category_url.encode()).hexdigest()
        cache_file = os.path.join(cache_dir, f"{url_hash}.json")

        cached_data = safe_read_json(cache_file)
        if cached_data:
            cached_products = cached_data.get("products", [])
            if cached_products:  # Chá»‰ return cache náº¿u cÃ³ sáº£n pháº©m
                print(f"[File Cache] âœ… Hit cache cho {category_url[:60]}...")
                return cached_products

    try:
        # Láº¥y trang Ä‘áº§u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh sá»‘ trang
        html = None
        if use_selenium:
            if _check_selenium_available():
                html = get_page_with_selenium(
                    category_url,
                    use_redis_cache=use_redis_cache,
                    use_rate_limiting=use_rate_limiting,
                )
            else:
                print("âš ï¸  Selenium chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t, dÃ¹ng requests thay tháº¿")
                html = get_page_with_requests(
                    category_url,
                    use_redis_cache=use_redis_cache,
                    use_rate_limiting=use_rate_limiting,
                )
        else:
            html = get_page_with_requests(
                category_url, use_redis_cache=use_redis_cache, use_rate_limiting=use_rate_limiting
            )
            # Náº¿u khÃ´ng tÃ¬m tháº¥y sáº£n pháº©m vá»›i requests, thá»­ Selenium
            if html:
                products_test = parse_products_from_html(html, category_url)
                if not products_test and _check_selenium_available():
                    print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y sáº£n pháº©m vá»›i requests, thá»­ Selenium...")
                    html = get_page_with_selenium(category_url)
                    use_selenium = True  # ÄÃ¡nh dáº¥u Ä‘Ã£ dÃ¹ng Selenium

        if not html:
            return []

        # Parse sáº£n pháº©m tá»« trang Ä‘áº§u
        products = parse_products_from_html(html, category_url)
        all_products.extend(products)

        # Náº¿u khÃ´ng tÃ¬m tháº¥y sáº£n pháº©m, return sá»›m
        if not products:
            # LÆ°u cache rá»—ng Ä‘á»ƒ Ä‘Ã¡nh dáº¥u Ä‘Ã£ thá»­
            if cache_file:
                try:
                    with open(cache_file, "w", encoding="utf-8") as f:
                        json.dump(
                            {
                                "category_url": category_url,
                                "products": [],
                                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                            },
                            f,
                            ensure_ascii=False,
                            indent=2,
                        )
                except Exception:
                    pass
            return []

        # Láº¥y tá»•ng sá»‘ trang
        total_pages = get_total_pages(html)
        if max_pages:
            total_pages = min(total_pages, max_pages)

        # Crawl cÃ¡c trang tiáº¿p theo
        for page in range(2, total_pages + 1):
            try:
                page_url = get_category_page_url(category_url, page)
                if use_selenium and _check_selenium_available():
                    html = get_page_with_selenium(
                        page_url,
                        use_redis_cache=use_redis_cache,
                        use_rate_limiting=use_rate_limiting,
                    )
                else:
                    html = get_page_with_requests(
                        page_url,
                        use_redis_cache=use_redis_cache,
                        use_rate_limiting=use_rate_limiting,
                    )

                if html:
                    products = parse_products_from_html(html, category_url)
                    if products:
                        all_products.extend(products)
                    else:
                        # Náº¿u khÃ´ng tÃ¬m tháº¥y sáº£n pháº©m á»Ÿ trang nÃ y, cÃ³ thá»ƒ Ä‘Ã£ háº¿t
                        break

                # Rate limiting Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ trong get_page_with_requests/selenium
                # Chá»‰ sleep náº¿u khÃ´ng dÃ¹ng rate limiting
                if not use_rate_limiting:
                    time.sleep(1)  # Delay giá»¯a cÃ¡c trang

            except Exception:
                continue

        # Loáº¡i bá» trÃ¹ng láº·p theo product_id
        seen_ids = set()
        unique_products = []
        for product in all_products:
            if product["product_id"] not in seen_ids:
                seen_ids.add(product["product_id"])
                unique_products.append(product)

        # LÆ°u cache - Æ°u tiÃªn Redis, fallback vá» file
        # Redis cache (nhanh, distributed)
        if redis_cache:
            try:
                redis_cache.cache_products(category_url, unique_products, ttl=43200)  # 12 giá»
                print(
                    f"[Redis Cache] âœ… ÄÃ£ cache {len(unique_products)} products cho {category_url[:60]}..."
                )
            except Exception as e:
                print(f"[Redis Cache] âš ï¸  Lá»—i khi cache vÃ o Redis: {e}")

        # File cache (fallback)
        if cache_file:
            try:
                with open(cache_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "category_url": category_url,
                            "products": unique_products,
                            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
            except Exception:
                pass

        return unique_products

    except Exception:
        return []


def crawl_single_category(category, max_pages=None, use_selenium=False):
    """
    Crawl sáº£n pháº©m tá»« má»™t danh má»¥c (wrapper cho threading)
    """
    global stats

    category_url = category.get("url", "")
    if not category_url:
        with stats_lock:
            stats["total_failed"] += 1
        return None, []

    try:
        products = crawl_category_products(
            category_url, max_pages=max_pages, use_selenium=use_selenium
        )

        with stats_lock:
            stats["total_categories"] += 1
            stats["total_products"] += len(products)
            stats["total_success"] += 1

        return category, products

    except Exception:
        with stats_lock:
            stats["total_categories"] += 1
            stats["total_failed"] += 1
        return category, []


def crawl_products_from_categories(
    categories_file,
    output_file=None,
    max_categories=None,
    max_pages_per_category=None,
    max_workers=5,
    use_selenium=False,
    categories_filter=None,
):
    """Crawl sáº£n pháº©m tá»« file danh má»¥c.

    Args:
        categories_file: ÄÆ°á»ng dáº«n file JSON chá»©a danh má»¥c
        output_file: File output (máº·c Ä‘á»‹nh: data/demo/products/products.json)
        max_categories: Sá»‘ danh má»¥c tá»‘i Ä‘a Ä‘á»ƒ crawl (None = táº¥t cáº£)
        max_pages_per_category: Sá»‘ trang tá»‘i Ä‘a má»—i danh má»¥c (None = táº¥t cáº£)
        max_workers: Sá»‘ thread song song
        use_selenium: CÃ³ dÃ¹ng Selenium khÃ´ng (cháº­m hÆ¡n nhÆ°ng chÃ­nh xÃ¡c hÆ¡n)
        categories_filter: Function filter danh má»¥c (cat) -> bool
    """
    global stats

    # Reset stats
    stats = {
        "total_categories": 0,
        "total_products": 0,
        "total_pages": 0,
        "total_success": 0,
        "total_failed": 0,
        "start_time": time.time(),
    }

    # Äá»c danh má»¥c
    print(f"ğŸ“– Äang Ä‘á»c danh má»¥c tá»«: {categories_file}")
    try:
        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)
        print(f"âœ“ ÄÃ£ Ä‘á»c {len(categories)} danh má»¥c")
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file: {e}")
        return []

    # Lá»c danh má»¥c náº¿u cÃ³ filter
    if categories_filter:
        categories = [cat for cat in categories if categories_filter(cat)]
        print(f"âœ“ Sau khi lá»c: {len(categories)} danh má»¥c")

    # Giá»›i háº¡n sá»‘ danh má»¥c
    if max_categories:
        categories = categories[:max_categories]
        print(f"âœ“ Giá»›i háº¡n: {len(categories)} danh má»¥c")

    # Crawl song song
    print("\nğŸš€ Báº¯t Ä‘áº§u crawl sáº£n pháº©m...")
    print(f"ğŸ“Š Sá»‘ thread: {max_workers}")
    print(f"ğŸ”§ Sá»­ dá»¥ng Selenium: {use_selenium}")
    print(f"ğŸ“„ Trang tá»‘i Ä‘a má»—i danh má»¥c: {max_pages_per_category or 'Táº¥t cáº£'}")
    print("=" * 70)

    all_products = []
    category_results = {}

    # Lazy import tqdm
    tqdm = _get_tqdm()
    with tqdm(total=len(categories), desc="Crawl danh má»¥c", unit="danh má»¥c") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit tasks
            future_to_category = {}
            for category in categories:
                future = executor.submit(
                    crawl_single_category,
                    category,
                    max_pages=max_pages_per_category,
                    use_selenium=use_selenium,
                )
                future_to_category[future] = category

            # Xá»­ lÃ½ káº¿t quáº£
            for future in as_completed(future_to_category):
                category = future_to_category[future]
                try:
                    cat, products = future.result(timeout=300)
                    if products:
                        all_products.extend(products)
                        category_results[category.get("url", "")] = len(products)

                    pbar.set_postfix(
                        {
                            "âœ…": stats["total_success"],
                            "âŒ": stats["total_failed"],
                            "ğŸ“¦": stats["total_products"],
                        }
                    )
                except Exception:
                    with stats_lock:
                        stats["total_failed"] += 1
                    pbar.set_postfix(
                        {
                            "âœ…": stats["total_success"],
                            "âŒ": stats["total_failed"],
                            "ğŸ“¦": stats["total_products"],
                        }
                    )

                pbar.update(1)

    # Loáº¡i bá» trÃ¹ng láº·p theo product_id
    seen_ids = set()
    unique_products = []
    for product in all_products:
        if product["product_id"] not in seen_ids:
            seen_ids.add(product["product_id"])
            unique_products.append(product)

    # LÆ°u káº¿t quáº£
    if not output_file:
        output_file = DEFAULT_PRODUCTS_DIR / "products.json"

    print(f"\nğŸ’¾ Äang lÆ°u káº¿t quáº£ vÃ o: {output_file}")
    print("ğŸ“ LÆ°u Ã½: Crawl thÃ´ng tin cÆ¡ báº£n (ID, tÃªn, URL, hÃ¬nh, sá»‘ lÆ°á»£ng bÃ¡n)")
    print("          GiÃ¡, Ä‘Ã¡nh giÃ¡ chi tiáº¿t sáº½ Ä‘Æ°á»£c crawl detail sau")

    # Äáº£m báº£o táº¥t cáº£ products cÃ³ sales_count (ká»ƒ cáº£ None)
    for product in unique_products:
        if "sales_count" not in product:
            product["sales_count"] = None

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {
                "total_products": len(unique_products),
                "total_categories": stats["total_categories"],
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "note": "Crawl thÃ´ng tin cÆ¡ báº£n bao gá»“m sá»‘ lÆ°á»£ng bÃ¡n (sales_count) - giÃ¡ vÃ  Ä‘Ã¡nh giÃ¡ chi tiáº¿t sáº½ crawl sau",
                "products": unique_products,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    # In thá»‘ng kÃª
    elapsed = time.time() - stats["start_time"]
    print("\n" + "=" * 70)
    print("ğŸ“ˆ THá»NG KÃŠ")
    print("=" * 70)
    print(f"â±  Thá»i gian: {elapsed:.1f}s")
    print(f"ğŸ“ Danh má»¥c Ä‘Ã£ crawl: {stats['total_categories']}")
    print(f"âœ… ThÃ nh cÃ´ng: {stats['total_success']}")
    print(f"âŒ Tháº¥t báº¡i: {stats['total_failed']}")
    print(f"ğŸ“¦ Tá»•ng sáº£n pháº©m: {len(unique_products)}")
    print(f"âš¡ Tá»‘c Ä‘á»™: {stats['total_products'] / elapsed:.2f} sáº£n pháº©m/s" if elapsed > 0 else "")
    print("=" * 70)

    return unique_products


def main():
    """
    HÃ m main.
    """
    categories_file = "data/raw/categories_recursive_optimized.json"
    output_file = DEFAULT_PRODUCTS_DIR / "products.json"

    # TÃ¹y chá»n
    max_categories = 10  # None Ä‘á»ƒ crawl táº¥t cáº£
    max_pages_per_category = 3  # None Ä‘á»ƒ crawl táº¥t cáº£ trang
    max_workers = 5  # Sá»‘ thread song song
    use_selenium = False  # True náº¿u cáº§n JS rendering

    print("=" * 70)
    print("ğŸ›ï¸  CRAWL Sáº¢N PHáº¨M Tá»ª DANH Má»¤C TIKI")
    print("=" * 70)
    print(f"ğŸ“ File danh má»¥c: {categories_file}")
    print(f"ğŸ“ File output: {output_file}")
    print(f"ğŸ“Š Sá»‘ danh má»¥c tá»‘i Ä‘a: {max_categories or 'Táº¥t cáº£'}")
    print(f"ğŸ“„ Trang tá»‘i Ä‘a má»—i danh má»¥c: {max_pages_per_category or 'Táº¥t cáº£'}")
    print(f"âš™ï¸  Sá»‘ thread: {max_workers}")
    print(f"ğŸ”§ Sá»­ dá»¥ng Selenium: {use_selenium}")
    print("=" * 70)

    crawl_products_from_categories(
        categories_file=categories_file,
        output_file=output_file,
        max_categories=max_categories,
        max_pages_per_category=max_pages_per_category,
        max_workers=max_workers,
        use_selenium=use_selenium,
    )


if __name__ == "__main__":
    main()
