import json
import os
import sys
from pathlib import Path


def load_categories(json_file_path: str):
    """
    Load data from categories JSON file to DB using Bulk Copy.
    Re-creates table if needed.
    """
    from pipelines.crawl.storage.postgres_storage import PostgresStorage

    print(f"üìÇ Reading categories from: {json_file_path}")

    if not os.path.exists(json_file_path):
        print(f"‚ùå File not found: {json_file_path}")
        return

    try:
        # 1. Read JSON
        with open(json_file_path, encoding="utf-8") as f:
            categories = json.load(f)

        if not categories:
            print("‚ö†Ô∏è  File is empty.")
            return

        print(f"üìä Found {len(categories)} categories.")

        # 2. CRITICAL: ƒê·∫£m b·∫£o t·∫•t c·∫£ parent categories ƒë∆∞·ª£c include
        # Build URL -> category map t·ª´ file JSON ƒë·∫ßy ƒë·ªß
        url_to_cat_full = {cat.get("url"): cat for cat in categories}

        # T√¨m t·∫•t c·∫£ parent URLs c·∫ßn thi·∫øt cho c√°c leaf categories c√≥ products
        storage = PostgresStorage()
        used_category_ids = set()
        try:
            used_category_ids = storage.get_used_category_ids()
            print(f"üîç Found {len(used_category_ids)} active categories in products table")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not get used category IDs: {e}")

        # CRITICAL: N·∫øu kh√¥ng c√≥ products (DB m·ªõi), load t·∫•t c·∫£ leaf categories v√† parents
        # N·∫øu c√≥ products, ch·ªâ load leaf categories c√≥ products v√† parents c·ªßa ch√∫ng
        parent_urls_needed = set()
        leaf_categories_to_load = []

        # X√°c ƒë·ªãnh leaf categories
        parent_urls_in_list = {c.get("parent_url") for c in categories if c.get("parent_url")}

        for cat in categories:
            cat_id = cat.get("category_id")
            if not cat_id and cat.get("url"):
                import re

                match = re.search(r"c?(\d+)", cat.get("url", ""))
                if match:
                    cat_id = f"c{match.group(1)}"

            # Check if leaf category (kh√¥ng c√≥ children trong danh s√°ch)
            is_leaf = cat.get("url") not in parent_urls_in_list

            # N·∫øu kh√¥ng c√≥ products, load t·∫•t c·∫£ leaf categories
            # N·∫øu c√≥ products, ch·ªâ load leaf categories c√≥ products
            should_load = False
            if not used_category_ids:
                # DB m·ªõi, load t·∫•t c·∫£ leaf categories
                should_load = is_leaf
            else:
                # C√≥ products, ch·ªâ load leaf categories c√≥ products
                should_load = is_leaf and cat_id and cat_id in used_category_ids

            if should_load:
                leaf_categories_to_load.append(cat)
                # Traverse up ƒë·ªÉ collect T·∫§T C·∫¢ parent URLs l√™n ƒë·∫øn root
                current = cat
                visited = set()
                depth = 0
                while current and depth < 10:
                    parent_url = current.get("parent_url")
                    if not parent_url:
                        # ƒê√£ ƒë·∫øn root, d·ª´ng l·∫°i
                        break
                    if parent_url in visited:
                        # Circular reference, d·ª´ng l·∫°i
                        break
                    visited.add(parent_url)
                    parent_urls_needed.add(parent_url)

                    # T√¨m parent trong url_to_cat_full
                    if parent_url in url_to_cat_full:
                        current = url_to_cat_full[parent_url]
                    else:
                        # Parent kh√¥ng c√≥ trong file JSON
                        print(f"‚ö†Ô∏è  Parent {parent_url} kh√¥ng c√≥ trong file JSON")
                        break
                    depth += 1

        # Include t·∫•t c·∫£ parent categories v√†o danh s√°ch categories ƒë·ªÉ load
        # CRITICAL: ƒê·∫£m b·∫£o traverse ƒë·∫ßy ƒë·ªß ƒë·ªÉ include c·∫£ parent c·ªßa parent
        categories_to_load = list(leaf_categories_to_load)
        missing_parents = []

        # Traverse ƒë·ªá quy ƒë·ªÉ include T·∫§T C·∫¢ parent categories
        parent_urls_to_check = set(parent_urls_needed)
        while parent_urls_to_check:
            current_batch = set(parent_urls_to_check)
            parent_urls_to_check = set()

            for parent_url in current_batch:
                if parent_url in url_to_cat_full:
                    parent_cat = url_to_cat_full[parent_url]
                    # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ trong danh s√°ch
                    if not any(c.get("url") == parent_url for c in categories_to_load):
                        categories_to_load.append(parent_cat)
                        # Ki·ªÉm tra parent c·ªßa parent n√†y
                        grandparent_url = parent_cat.get("parent_url")
                        if grandparent_url and grandparent_url not in parent_urls_needed:
                            parent_urls_needed.add(grandparent_url)
                            parent_urls_to_check.add(grandparent_url)
                else:
                    missing_parents.append(parent_url)

        if missing_parents:
            print(f"‚ö†Ô∏è  C·∫£nh b√°o: {len(missing_parents)} parent URLs kh√¥ng c√≥ trong file JSON:")
            for url in missing_parents[:5]:
                print(f"   - {url}")

        print(
            f"üìÇ S·∫Ω load {len(categories_to_load)} categories ({len(leaf_categories_to_load)} leaves + {len(parent_urls_needed)} parents)"
        )

        # Debug: In ra danh s√°ch categories s·∫Ω load (ch·ªâ hi·ªÉn th·ªã m·ªôt s·ªë)
        if len(categories_to_load) <= 20:
            print("\nüìã Danh s√°ch categories s·∫Ω load:")
            for cat in categories_to_load:
                print(f"   - [{cat.get('level', '?')}] {cat.get('name')} ({cat.get('url')})")
        else:
            print(f"\nüìã S·∫Ω load {len(categories_to_load)} categories (qu√° nhi·ªÅu ƒë·ªÉ hi·ªÉn th·ªã)")

        # 3. Connect & Save
        print("üöÄ Importing to Database...")

        # Load categories v·ªõi ƒë·∫ßy ƒë·ªß parent hierarchy
        saved_count = storage.save_categories(
            categories_to_load,
            only_leaf=False,  # Load c·∫£ parents ƒë·ªÉ ƒë·∫£m b·∫£o path ƒë·∫ßy ƒë·ªß
            sync_with_products=False,  # ƒê√£ filter ·ªü tr√™n r·ªìi
        )

        print(
            f"‚úÖ DONE! Successfully loaded {saved_count} categories (including parent categories)."
        )
        print("‚ÑπÔ∏è  Table 'categories' relies on 'url' as Primary Key.")

        # Update product_count from actual products in database
        print("üìä Updating product_count from products table...")
        updated_count = storage.update_category_product_counts()
        print(f"‚úÖ Updated product_count for {updated_count} categories.")

    except Exception as e:
        print(f"‚ùå Error during load: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # Side-effects only when run as a script
    current_dir = Path(__file__).resolve().parent
    src_dir = current_dir.parent.parent
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))

    # Config DB host for local run
    if "POSTGRES_HOST" not in os.environ:
        print("‚ÑπÔ∏è  Setting POSTGRES_HOST=localhost for local execution")
        os.environ["POSTGRES_HOST"] = "localhost"

    # Default path based on user's structure
    default_path = os.path.join(
        src_dir.parent, "data", "raw", "categories_recursive_optimized.json"
    )

    # Allow command line arg override
    target_file = sys.argv[1] if len(sys.argv) > 1 else default_path

    load_categories(target_file)
