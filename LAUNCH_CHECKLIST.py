#!/usr/bin/env python3
"""
Summary checklist cho viá»‡c khá»Ÿi cháº¡y ETL pipeline Ä‘áº§y Ä‘á»§ vá»›i category_path fix
"""

import os
import sys
from pathlib import Path

# Color codes
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
CYAN = "\033[96m"
END = "\033[0m"


def main():
    print(f"\n{BLUE}{'=' * 90}{END}")
    print(f"{BLUE}{'ğŸš€ ETL PIPELINE LAUNCH CHECKLIST':^90}{END}")
    print(f"{BLUE}{'=' * 90}{END}\n")
    
    print(f"""
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}PHASE 1: PREPARATION (CÃ¡c bÆ°á»›c chuáº©n bá»‹){END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

{GREEN}âœ… COMPLETED:{END}

  1. Enrich categories file vá»›i category_id vÃ  category_path
     $ python scripts/enrich_categories_with_paths.py
     âœ“ 465 categories, táº¥t cáº£ cÃ³ category_id
     âœ“ 465 categories, táº¥t cáº£ cÃ³ category_path
     
  2. Update DAG Ä‘á»ƒ enrich products vá»›i category_path
     âœ“ Added category_path_lookup logic trong transform_products
     âœ“ Task 'enrich_products_category_path' sáº½ bá»• sung category_path
     
  3. Update database schema
     âœ“ Added category_id column
     âœ“ Added category_path column (JSONB)
     âœ“ Created indexes
     
  4. Update Loader Ä‘á»ƒ save category_path
     âœ“ loader_optimized.py updated
     âœ“ postgres_storage.py supports category_path
     
  5. Update Transformer Ä‘á»ƒ preserve category_path
     âœ“ DataTransformer handles category_path


{YELLOW}ğŸ”„ TODO:{END}

  â˜ Cháº¡y DAG end-to-end Ä‘á»ƒ enrich products

{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}PHASE 2: EXECUTION (Cháº¡y DAG){END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

{YELLOW}âš ï¸  BÆ¯á»šC 1: Verify Docker containers Ä‘ang cháº¡y{END}

  $ docker-compose ps
  
  Expected:
    postgres       ... Up (port 5432)
    redis          ... Up (port 6379)
    airflow-webserver ... Up (port 8080)
    airflow-scheduler  ... Up
    airflow-worker ... Up

{YELLOW}âš ï¸  BÆ¯á»šC 2: Backup database (optional nhÆ°ng recommended){END}

  $ docker-compose exec postgres pg_dump -U postgres crawl_data > backup_before_enrichment.sql
  
  Hoáº·c dÃ¹ng script:
  $ python scripts/backup-postgres.ps1  # Windows
  $ bash scripts/backup-postgres.sh     # Linux/Mac


{YELLOW}âš ï¸  BÆ¯á»šC 3: Trigger DAG trÃªn Airflow UI{END}

  1. Má»Ÿ browser: http://localhost:8080
  2. Login: username='airflow', password='airflow' (máº·c Ä‘á»‹nh)
  3. TÃ¬m DAG 'tiki_crawl_products'
  4. Click nÃºt Play (â–¶ï¸) hoáº·c "Trigger DAG"
  
  DAG flow:
    â€¢ load_and_prepare â†’ load categories
    â€¢ crawl_categories â†’ dynamic map over categories
    â€¢ process_and_save â†’ merge products
    â€¢ crawl_product_details â†’ crawl detail per product
    â€¢ enrich_category_path â† {YELLOW}NEW{END}: Enrich category_path
    â€¢ transform_and_load â†’ Transform & Load to DB
    â€¢ validate â†’ Validate data quality
    â€¢ aggregate_and_notify â†’ Report results


{YELLOW}âš ï¸  BÆ¯á»šC 4: Monitor DAG execution{END}

  1. Watch task status trÃªn Airflow UI
  2. Check logs cho tá»«ng task:
     - Click task â†’ Logs tab
     - TÃ¬m "category_path enriched" message
  
  CÃ¡c metrics cáº§n check:
    âœ“ crawl_product_details: Sá»‘ products crawled
    âœ“ enrich_category_path: Sá»‘ products enriched vá»›i category_path
    âœ“ transform_products: Sá»‘ products transformed
    âœ“ load_products: Sá»‘ products loaded
    âœ“ validate_data: Validation passed/failed

{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}PHASE 3: VERIFICATION (Kiá»ƒm chá»©ng káº¿t quáº£){END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

{GREEN}After DAG completes, run verification:{END}

  1. Visualize final data:
     $ python scripts/visualize_final_data.py
     
     Expected output:
       â€¢ Products with category_path: >90%
       â€¢ Sample product cÃ³ Ä‘áº§y Ä‘á»§ category_path
  
  2. Query database directly:
  
     # Connect to database
     $ docker-compose exec postgres psql -U postgres -d crawl_data
     
     # Check category_path
     crawl_data=# SELECT COUNT(*) FROM products WHERE category_path IS NOT NULL;
     
     # Sample product
     crawl_data=# SELECT 
         product_id, 
         name, 
         category_id, 
         category_path
       FROM products 
       WHERE category_path IS NOT NULL 
       LIMIT 1;
       
     Example output:
       product_id â”‚ name â”‚ category_id â”‚ category_path
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       271624999  â”‚ ... â”‚ c8314 â”‚ ["PhÃ²ng ngá»§", "Phá»¥ kiá»‡n phÃ²ng ngá»§"]
  
  3. Check data quality:
     crawl_data=# SELECT 
         COUNT(*) as total_products,
         COUNT(CASE WHEN category_path IS NOT NULL THEN 1 END) as with_path,
         COUNT(CASE WHEN price IS NOT NULL THEN 1 END) as with_price,
         COUNT(CASE WHEN sales_count IS NOT NULL THEN 1 END) as with_sales
       FROM products;

{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}OPTIMIZATION STATUS{END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

{CYAN}Current Score: 81.2%{END}

Breakdown:
  ğŸ“ Files & Structures: 50% (âš ï¸ will improve to 100% after DAG)
  ğŸ”„ DAG Structure: 100% (âœ…)
  ğŸ”§ Data Pipeline: 100% (âœ…)
  ğŸ—„ï¸ Database: 100% (âœ…)
  ğŸ’» Code Quality: 80% (âœ…)

{GREEN}Main improvements in this phase:{END}
  âœ“ Category path enrichment implemented
  âœ“ Database schema updated
  âœ“ DAG logic enhanced to use categories for product enrichment
  âœ“ Loader updated to persist category_path

{YELLOW}Remaining optimizations (future):{END}
  â€¢ Rate limiting optimization
  â€¢ HTML response caching
  â€¢ Batch size tuning
  â€¢ Pydantic validation models
  â€¢ Prometheus metrics
  â€¢ Checkpoint/resume support

{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}TROUBLESHOOTING{END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

{YELLOW}Q: DAG fails khi enrich_category_path?{END}
  A: Kiá»ƒm tra:
     1. Categories file cÃ³ category_path: 
        $ python -c "import json; cats = json.load(open('data/raw/categories_recursive_optimized.json')); print(all(c.get('category_path') for c in cats))"
     2. DAG logs: http://localhost:8080 â†’ Logs tab
     3. Restart scheduler: docker-compose restart airflow-scheduler

{YELLOW}Q: Products khÃ´ng cÃ³ category_path trong database?{END}
  A: CÃ³ thá»ƒ do:
     1. DAG chÆ°a cháº¡y: Trigger láº¡i DAG
     2. Category lookup file path sai: Kiá»ƒm tra CATEGORIES_FILE trong DAG
     3. Database schema chÆ°a updated: Cháº¡y apply_schema_changes.py

{YELLOW}Q: Database tables khÃ´ng cÃ³ data?{END}
  A: Kháº£ nÄƒng:
     1. DAG chÆ°a hoÃ n thÃ nh: Chá» DAG finish
     2. Load task failed: Check logs
     3. Database connection issue: docker-compose logs postgres

{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{CYAN}QUICK COMMANDS{END}
{CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}

  # View Airflow logs
  $ docker-compose logs -f airflow-scheduler
  
  # Restart services
  $ docker-compose restart airflow-scheduler airflow-worker
  
  # Check database
  $ docker-compose exec postgres psql -U postgres -d crawl_data
  
  # View category_path sample
  $ docker-compose exec postgres psql -U postgres -d crawl_data \\
    -c "SELECT product_id, name, category_path FROM products WHERE category_path IS NOT NULL LIMIT 5;"
  
  # Count products with category_path
  $ docker-compose exec postgres psql -U postgres -d crawl_data \\
    -c "SELECT COUNT(*) FROM products WHERE category_path IS NOT NULL;"

{BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}
{GREEN}{'Ready to launch ETL pipeline with category_path enrichment!':^90}{END}
{BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{END}\n
    """)


if __name__ == "__main__":
    main()
