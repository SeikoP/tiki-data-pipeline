"""
DAG Airflow Ä‘á»ƒ crawl sáº£n pháº©m Tiki vá»›i tá»‘i Æ°u hÃ³a cho dá»¯ liá»‡u lá»›n

TÃ­nh nÄƒng:
- Dynamic Task Mapping: crawl song song nhiá»u danh má»¥c
- Chia nhá» tasks: má»—i task má»™t chá»©c nÄƒng riÃªng
- XCom: chia sáº» dá»¯ liá»‡u giá»¯a cÃ¡c tasks
- Retry: tá»± Ä‘á»™ng retry khi lá»—i
- Timeout: giá»›i háº¡n thá»i gian thá»±c thi
- Logging: ghi log rÃµ rÃ ng cho tá»«ng task
- Error handling: xá»­ lÃ½ lá»—i vÃ  tiáº¿p tá»¥c vá»›i danh má»¥c khÃ¡c
- Atomic writes: ghi file an toÃ n, trÃ¡nh corrupt
- TaskGroup: nhÃ³m cÃ¡c tasks liÃªn quan
- Tá»‘i Æ°u: batch processing, rate limiting, caching

Dependencies Ä‘Æ°á»£c quáº£n lÃ½ báº±ng >> operator giá»¯a cÃ¡c tasks.
"""

import json
import os
import re
import shutil
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator

# Asset/Dataset Ä‘Ã£ Ä‘Æ°á»£c xÃ³a vÃ¬ khÃ´ng cáº§n thiáº¿t cho single DAG vÃ  gÃ¢y lá»—i vá»›i PythonOperator

# Import Variable vÃ  TaskGroup vá»›i suppress warning
try:
    # Thá»­ import tá»« airflow.sdk (Airflow 3.x)
    from airflow.sdk import TaskGroup, Variable

    _Variable = Variable  # Alias Ä‘á»ƒ dÃ¹ng wrapper
except ImportError:
    # Fallback: dÃ¹ng airflow.models vÃ  airflow.utils.task_group (Airflow 2.x)
    try:
        from airflow.utils.task_group import TaskGroup
    except ImportError:
        # Náº¿u khÃ´ng cÃ³ TaskGroup, táº¡o dummy class
        class TaskGroup:
            def __init__(self, *args, **kwargs):
                pass

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

    from airflow.models import Variable as _Variable


# Wrapper function Ä‘á»ƒ suppress deprecation warning khi gá»i Variable.get()
def get_variable(key, default_var=None):
    """Wrapper cho Variable.get() Ä‘á»ƒ suppress deprecation warning"""
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore", category=DeprecationWarning, module="airflow.models.variable"
        )
        return _Variable.get(key, default=default_var)


# Alias Variable Ä‘á»ƒ code cÅ© váº«n hoáº¡t Ä‘á»™ng, nhÆ°ng dÃ¹ng wrapper
class VariableWrapper:
    """Wrapper cho Variable Ä‘á»ƒ suppress warnings"""

    @staticmethod
    def get(key, default_var=None):
        return get_variable(key, default_var)

    @staticmethod
    def set(key, value):
        return _Variable.set(key, value)


Variable = VariableWrapper

# ThÃªm Ä‘Æ°á»ng dáº«n src vÃ o sys.path
# Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i cá»§a DAG file
dag_file_dir = os.path.dirname(os.path.abspath(__file__))

# Thá»­ nhiá»u Ä‘Æ°á»ng dáº«n cÃ³ thá»ƒ
# Trong Docker, src Ä‘Æ°á»£c mount vÃ o /opt/airflow/src
possible_paths = [
    # Tá»« /opt/airflow (Docker default - Æ°u tiÃªn)
    "/opt/airflow/src/pipelines/crawl",
    # Tá»« airflow/dags/ lÃªn 2 cáº¥p Ä‘áº¿n root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")),
    # Tá»« airflow/dags/ lÃªn 1 cáº¥p (náº¿u airflow/ lÃ  root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "pipelines", "crawl")),
    # Tá»« workspace root (náº¿u mount vÃ o /workspace)
    "/workspace/src/pipelines/crawl",
    # Tá»« current working directory
    os.path.join(os.getcwd(), "src", "pipelines", "crawl"),
]

# TÃ¬m Ä‘Æ°á»ng dáº«n há»£p lá»‡
crawl_module_path = None
crawl_products_path = None

for path in possible_paths:
    test_path = os.path.join(path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = path
        crawl_products_path = test_path
        break

if not crawl_module_path:
    # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»« DAG file
    relative_path = os.path.abspath(
        os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")
    )
    test_path = os.path.join(relative_path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = relative_path
        crawl_products_path = test_path

# Import module utils TRÆ¯á»šC (cáº§n thiáº¿t cho crawl_products vÃ  crawl_products_detail)
# Khá»Ÿi táº¡o SeleniumDriverPool = None Ä‘á»ƒ trÃ¡nh NameError
SeleniumDriverPool = None

utils_path = None
if crawl_module_path:
    utils_path = os.path.join(crawl_module_path, "utils.py")
    if not os.path.exists(utils_path):
        utils_path = None

if not utils_path:
    # Thá»­ tÃ¬m trong cÃ¡c possible paths
    for path in possible_paths:
        test_path = os.path.join(path, "utils.py")
        if os.path.exists(test_path):
            utils_path = test_path
            break

if utils_path and os.path.exists(utils_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
        if spec and spec.loader:
            utils_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(utils_module)
            # LÆ°u vÃ o sys.modules Ä‘á»ƒ cÃ¡c module khÃ¡c cÃ³ thá»ƒ import
            sys.modules["crawl_utils"] = utils_module
            # Táº¡o fake package structure Ä‘á»ƒ relative import hoáº¡t Ä‘á»™ng
            if "pipelines.crawl.utils" not in sys.modules:
                sys.modules["pipelines"] = type(sys)("pipelines")
                sys.modules["pipelines.crawl"] = type(sys)("pipelines.crawl")
                sys.modules["pipelines.crawl.utils"] = utils_module
            # Extract SeleniumDriverPool Ä‘á»ƒ sá»­ dá»¥ng trá»±c tiáº¿p
            SeleniumDriverPool = getattr(utils_module, "SeleniumDriverPool", None)
    except Exception as e:
        # Náº¿u import lá»—i, log vÃ  tiáº¿p tá»¥c (sáº½ fail khi cháº¡y task)
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import utils module: {e}", stacklevel=2)
        SeleniumDriverPool = None

# Import module crawl_products
if crawl_products_path and os.path.exists(crawl_products_path):
    try:
        # Sá»­ dá»¥ng importlib Ä‘á»ƒ import trá»±c tiáº¿p tá»« file (cÃ¡ch Ä‘Ã¡ng tin cáº­y nháº¥t)
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"KhÃ´ng thá»ƒ load spec tá»« {crawl_products_path}")
        crawl_products_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_module)

        # Extract cÃ¡c functions cáº§n thiáº¿t
        crawl_category_products = crawl_products_module.crawl_category_products
        get_page_with_requests = crawl_products_module.get_page_with_requests
        parse_products_from_html = crawl_products_module.parse_products_from_html
        get_total_pages = crawl_products_module.get_total_pages
    except Exception as e:
        # Náº¿u import lá»—i, log vÃ  tiáº¿p tá»¥c (sáº½ fail khi cháº¡y task)
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import crawl_products module: {e}", stacklevel=2)

        # Táº¡o dummy functions Ä‘á»ƒ trÃ¡nh NameError
        error_msg = str(e)

        def crawl_category_products(*args, **kwargs):
            raise ImportError(f"Module crawl_products chÆ°a Ä‘Æ°á»£c import: {error_msg}")

        get_page_with_requests = crawl_category_products
        parse_products_from_html = crawl_category_products
        get_total_pages = crawl_category_products
else:
    # Fallback: thá»­ import thÃ´ng thÆ°á»ng náº¿u Ä‘Ã£ thÃªm vÃ o sys.path
    if crawl_module_path and crawl_module_path not in sys.path:
        sys.path.insert(0, crawl_module_path)

    try:
        from crawl_products import crawl_category_products
    except ImportError as e:
        # Debug: kiá»ƒm tra xem thÆ° má»¥c cÃ³ tá»“n táº¡i khÃ´ng
        debug_info = {
            "dag_file_dir": dag_file_dir,
            "cwd": os.getcwd(),
            "possible_paths": possible_paths,
            "crawl_module_path": crawl_module_path,
            "crawl_products_path": crawl_products_path,
            "sys_path": sys.path[:5],  # Chá»‰ láº¥y 5 Ä‘áº§u tiÃªn
        }

        # Kiá»ƒm tra xem /opt/airflow/src cÃ³ tá»“n táº¡i khÃ´ng
        if os.path.exists("/opt/airflow/src"):
            try:
                debug_info["opt_airflow_src_contents"] = os.listdir("/opt/airflow/src")
            except Exception:
                pass

        raise ImportError(
            f"KhÃ´ng tÃ¬m tháº¥y module crawl_products.\n" f"Debug info: {debug_info}\n" f"Lá»—i gá»‘c: {e}"
        ) from e

# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products_detail.py")
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_products_detail", crawl_products_detail_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"KhÃ´ng thá»ƒ load spec tá»« {crawl_products_detail_path}")
        crawl_products_detail_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_detail_module)

        # Extract cÃ¡c functions cáº§n thiáº¿t
        crawl_product_detail_with_selenium = (
            crawl_products_detail_module.crawl_product_detail_with_selenium
        )
        extract_product_detail = crawl_products_detail_module.extract_product_detail
        crawl_product_detail_async = crawl_products_detail_module.crawl_product_detail_async
    except Exception as e:
        # Náº¿u import lá»—i, log vÃ  tiáº¿p tá»¥c (sáº½ fail khi cháº¡y task)
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import crawl_products_detail module: {e}", stacklevel=2)

        # Táº¡o dummy functions Ä‘á»ƒ trÃ¡nh NameError
        error_msg = str(e)

        def crawl_product_detail_with_selenium(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail chÆ°a Ä‘Æ°á»£c import: {error_msg}")

        extract_product_detail = crawl_product_detail_with_selenium
        
        async def crawl_product_detail_async(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail chÆ°a Ä‘Æ°á»£c import: {error_msg}")
else:
    # Fallback: thá»­ import thÃ´ng thÆ°á»ng
    try:
        from crawl_products_detail import (
            crawl_product_detail_with_selenium,
            extract_product_detail,
            crawl_product_detail_async,
        )
    except ImportError as e:
        raise ImportError(
            f"KhÃ´ng tÃ¬m tháº¥y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"Lá»—i gá»‘c: {e}"
        ) from e

# Import resilience patterns
# Import trá»±c tiáº¿p tá»«ng module con Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» relative imports
resilience_module_path = None
for path in possible_paths:
    test_path = os.path.join(path, "resilience", "__init__.py")
    if os.path.exists(test_path):
        resilience_module_path = os.path.join(path, "resilience")
        break

if resilience_module_path and os.path.exists(resilience_module_path):
    try:
        import importlib.util
        import sys

        # ThÃªm parent path (pipelines/crawl) vÃ o sys.path
        parent_path = os.path.dirname(resilience_module_path)  # .../crawl
        if parent_path not in sys.path:
            sys.path.insert(0, parent_path)

        # ThÃªm grandparent path (pipelines) vÃ o sys.path
        grandparent_path = os.path.dirname(parent_path)  # .../pipelines
        if grandparent_path not in sys.path:
            sys.path.insert(0, grandparent_path)

        # Import trá»±c tiáº¿p tá»«ng module con vá»›i tÃªn module Ä‘áº§y Ä‘á»§
        # Äiá»u nÃ y Ä‘áº£m báº£o cÃ¡c module cÃ³ thá»ƒ import láº«n nhau náº¿u cáº§n

        # Táº¡o package structure trong sys.modules
        import types

        if "pipelines" not in sys.modules:
            sys.modules["pipelines"] = types.ModuleType("pipelines")
        if "pipelines.crawl" not in sys.modules:
            sys.modules["pipelines.crawl"] = types.ModuleType("pipelines.crawl")
        if "pipelines.crawl.resilience" not in sys.modules:
            sys.modules["pipelines.crawl.resilience"] = types.ModuleType(
                "pipelines.crawl.resilience"
            )

        # Äáº£m báº£o utils module Ä‘Ã£ Ä‘Æ°á»£c import (cáº§n thiáº¿t cho dead_letter_queue)
        # utils module Ä‘Ã£ Ä‘Æ°á»£c import á»Ÿ trÃªn (dÃ²ng 137-156)
        # Náº¿u chÆ°a cÃ³, táº¡o fake module
        if "pipelines.crawl.utils" not in sys.modules and "crawl_utils" in sys.modules:
            sys.modules["pipelines.crawl.utils"] = sys.modules["crawl_utils"]

        # 1. Import exceptions trÆ°á»›c (khÃ´ng cÃ³ dependency)
        exceptions_path = os.path.join(resilience_module_path, "exceptions.py")
        if os.path.exists(exceptions_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.exceptions", exceptions_path
            )
            if spec and spec.loader:
                exceptions_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.exceptions"] = exceptions_module
                spec.loader.exec_module(exceptions_module)
                CrawlError = exceptions_module.CrawlError
                classify_error = exceptions_module.classify_error
            else:
                raise ImportError(f"KhÃ´ng thá»ƒ load exceptions module tá»« {exceptions_path}")
        else:
            raise ImportError(f"KhÃ´ng tÃ¬m tháº¥y exceptions.py táº¡i {exceptions_path}")

        # 2. Import circuit_breaker (khÃ´ng cÃ³ dependency)
        circuit_breaker_path = os.path.join(resilience_module_path, "circuit_breaker.py")
        if os.path.exists(circuit_breaker_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.circuit_breaker", circuit_breaker_path
            )
            if spec and spec.loader:
                circuit_breaker_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.circuit_breaker"] = circuit_breaker_module
                spec.loader.exec_module(circuit_breaker_module)
                CircuitBreaker = circuit_breaker_module.CircuitBreaker
                CircuitBreakerOpenError = circuit_breaker_module.CircuitBreakerOpenError
            else:
                raise ImportError("KhÃ´ng thá»ƒ load circuit_breaker module")
        else:
            raise ImportError("KhÃ´ng tÃ¬m tháº¥y circuit_breaker.py")

        # 3. Import dead_letter_queue (cÃ³ thá»ƒ import tá»« utils)
        dlq_path = os.path.join(resilience_module_path, "dead_letter_queue.py")
        if os.path.exists(dlq_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.dead_letter_queue", dlq_path
            )
            if spec and spec.loader:
                dlq_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.dead_letter_queue"] = dlq_module
                spec.loader.exec_module(dlq_module)
                DeadLetterQueue = dlq_module.DeadLetterQueue
                get_dlq = dlq_module.get_dlq
            else:
                raise ImportError("KhÃ´ng thá»ƒ load dead_letter_queue module")
        else:
            raise ImportError("KhÃ´ng tÃ¬m tháº¥y dead_letter_queue.py")

        # 4. Import graceful_degradation (khÃ´ng cÃ³ dependency)
        degradation_path = os.path.join(resilience_module_path, "graceful_degradation.py")
        if os.path.exists(degradation_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.graceful_degradation", degradation_path
            )
            if spec and spec.loader:
                degradation_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.graceful_degradation"] = degradation_module
                spec.loader.exec_module(degradation_module)
                GracefulDegradation = degradation_module.GracefulDegradation
                DegradationLevel = degradation_module.DegradationLevel
                get_service_health = degradation_module.get_service_health
            else:
                raise ImportError("KhÃ´ng thá»ƒ load graceful_degradation module")
        else:
            raise ImportError("KhÃ´ng tÃ¬m tháº¥y graceful_degradation.py")

    except Exception as e:
        # Náº¿u import lá»—i, táº¡o dummy classes Ä‘á»ƒ trÃ¡nh NameError
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import resilience module: {e}", stacklevel=2)

        # Táº¡o dummy classes
        class CircuitBreaker:
            def __init__(self, *args, **kwargs):
                pass

            def call(self, func, *args, **kwargs):
                return func(*args, **kwargs)

        class CircuitBreakerOpenError(Exception):
            pass

        class DeadLetterQueue:
            def add(self, *args, **kwargs):
                pass

        def get_dlq(*args, **kwargs):
            return DeadLetterQueue()

        class GracefulDegradation:
            def should_skip(self):
                return False

            def record_success(self):
                pass

            def record_failure(self):
                pass

        class DegradationLevel:
            FULL = "full"
            REDUCED = "reduced"
            MINIMAL = "minimal"
            FAILED = "failed"

        def get_service_health():
            return type(
                "ServiceHealth",
                (),
                {"register_service": lambda *args, **kwargs: GracefulDegradation()},
            )()

        def classify_error(error, **kwargs):
            return error

        class CrawlError(Exception):
            pass

else:
    # Fallback: táº¡o dummy classes
    class CircuitBreaker:
        def __init__(self, *args, **kwargs):
            pass

        def call(self, func, *args, **kwargs):
            return func(*args, **kwargs)

    class CircuitBreakerOpenError(Exception):
        pass

    class DeadLetterQueue:
        def add(self, *args, **kwargs):
            pass

    def get_dlq(*args, **kwargs):
        return DeadLetterQueue()

    class GracefulDegradation:
        def should_skip(self):
            return False

        def record_success(self):
            pass

        def record_failure(self):
            pass

    class DegradationLevel:
        FULL = "full"
        REDUCED = "reduced"
        MINIMAL = "minimal"
        FAILED = "failed"

    def get_service_health():
        return type(
            "ServiceHealth", (), {"register_service": lambda *args, **kwargs: GracefulDegradation()}
        )()

    def classify_error(error, **kwargs):
        return error

    class CrawlError(Exception):
        pass


# Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
DEFAULT_ARGS = {
    "owner": "data-team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,  # Retry 3 láº§n
    "retry_delay": timedelta(minutes=2),  # Delay 2 phÃºt giá»¯a cÃ¡c retry
    "retry_exponential_backoff": True,  # Exponential backoff
    "max_retry_delay": timedelta(minutes=10),
}

# Cáº¥u hÃ¬nh DAG - CÃ³ thá»ƒ chuyá»ƒn Ä‘á»•i giá»¯a tá»± Ä‘á»™ng vÃ  thá»§ cÃ´ng qua Variable
# Äá»c schedule mode tá»« Airflow Variable (máº·c Ä‘á»‹nh: 'manual' Ä‘á»ƒ test)
# CÃ³ thá»ƒ set Variable 'TIKI_DAG_SCHEDULE_MODE' = 'scheduled' Ä‘á»ƒ cháº¡y tá»± Ä‘á»™ng
try:
    schedule_mode = Variable.get("TIKI_DAG_SCHEDULE_MODE", default_var="manual")
except Exception:
    schedule_mode = "manual"  # Máº·c Ä‘á»‹nh lÃ  manual Ä‘á»ƒ test

# XÃ¡c Ä‘á»‹nh schedule dá»±a trÃªn mode
if schedule_mode == "scheduled":
    dag_schedule = timedelta(days=1)  # Cháº¡y tá»± Ä‘á»™ng hÃ ng ngÃ y
    dag_description = (
        "TEST - Crawl sáº£n pháº©m Tiki vá»›i cáº¥u hÃ¬nh tá»‘i giáº£n Ä‘á»ƒ test E2E (Tá»± Ä‘á»™ng cháº¡y hÃ ng ngÃ y)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "scheduled"]
else:
    dag_schedule = None  # Chá»‰ cháº¡y khi trigger thá»§ cÃ´ng
    dag_description = (
        "TEST - Crawl sáº£n pháº©m Tiki vá»›i cáº¥u hÃ¬nh tá»‘i giáº£n Ä‘á»ƒ test E2E (Cháº¡y thá»§ cÃ´ng - Test mode)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "manual"]

# Cáº¥u hÃ¬nh DAG schedule
dag_schedule_config = dag_schedule

# Documentation Ä‘Æ¡n giáº£n cho DAG
dag_doc_md = "Crawl sáº£n pháº©m tá»« Tiki.vn vá»›i Dynamic Task Mapping vÃ  Selenium"

DAG_CONFIG = {
    "dag_id": "tiki_crawl_products_test",
    "description": dag_description,
    "doc_md": dag_doc_md,
    "default_args": DEFAULT_ARGS,
    "schedule": dag_schedule_config,
    "start_date": datetime(2025, 11, 1),  # NgÃ y cá»‘ Ä‘á»‹nh trong quÃ¡ khá»©
    "catchup": False,  # KhÃ´ng cháº¡y láº¡i cÃ¡c task Ä‘Ã£ bá» lá»¡
    "tags": dag_tags,
    "max_active_runs": 1,  # Chá»‰ cháº¡y 1 DAG instance táº¡i má»™t thá»i Ä‘iá»ƒm
    "max_active_tasks": 3,  # TEST MODE: Giáº£m xuá»‘ng 3 tasks song song Ä‘á»ƒ test nhanh,  # Giáº£m xuá»‘ng 10 tasks song song Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i khi táº¡o Selenium driver
}

# ThÆ° má»¥c dá»¯ liá»‡u
# Trong Docker, data Ä‘Æ°á»£c mount vÃ o /opt/airflow/data
# Thá»­ nhiá»u Ä‘Æ°á»ng dáº«n
possible_data_dirs = [
    Path("/opt/airflow/data"),  # Docker mount
    Path(__file__).parent.parent.parent / "data",  # Local development
    Path(os.getcwd()) / "data",  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: dÃ¹ng Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i
    DATA_DIR = Path(__file__).parent.parent.parent / "data"

CATEGORIES_FILE = DATA_DIR / "raw" / "categories_recursive_optimized.json"
CATEGORIES_TREE_FILE = DATA_DIR / "raw" / "categories_tree.json"
OUTPUT_DIR = DATA_DIR / "raw" / "products"
CACHE_DIR = OUTPUT_DIR / "cache"
DETAIL_CACHE_DIR = OUTPUT_DIR / "detail" / "cache"
OUTPUT_FILE = OUTPUT_DIR / "products.json"
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / "products_with_detail.json"
# Progress tracking cho multi-day crawling
PROGRESS_FILE = OUTPUT_DIR / "crawl_progress.json"

# Asset/Dataset Ä‘Ã£ Ä‘Æ°á»£c xÃ³a - dependencies Ä‘Æ°á»£c quáº£n lÃ½ báº±ng >> operator

# Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()

# Khá»Ÿi táº¡o resilience patterns
# Circuit breaker cho Tiki API
tiki_circuit_breaker = CircuitBreaker(
    failure_threshold=int(Variable.get("TIKI_CIRCUIT_BREAKER_FAILURE_THRESHOLD", default_var="5")),
    recovery_timeout=int(Variable.get("TIKI_CIRCUIT_BREAKER_RECOVERY_TIMEOUT", default_var="60")),
    expected_exception=Exception,
    name="tiki_api",
)

# Dead Letter Queue
try:
    # Thá»­ dÃ¹ng Redis náº¿u cÃ³
    redis_url = Variable.get("REDIS_URL", default_var="redis://redis:6379/3")
    tiki_dlq = get_dlq(storage_type="redis", redis_url=redis_url)
except Exception:
    # Fallback vá» file-based
    try:
        dlq_path = DATA_DIR / "dlq"
        tiki_dlq = get_dlq(storage_type="file", storage_path=str(dlq_path))
    except Exception:
        # Náº¿u khÃ´ng táº¡o Ä‘Æ°á»£c, dÃ¹ng default
        tiki_dlq = get_dlq()

# Graceful Degradation cho Tiki service
service_health = get_service_health()
tiki_degradation = service_health.register_service(
    name="tiki",
    failure_threshold=int(Variable.get("TIKI_DEGRADATION_FAILURE_THRESHOLD", default_var="3")),
    recovery_threshold=int(Variable.get("TIKI_DEGRADATION_RECOVERY_THRESHOLD", default_var="5")),
)

# Import modules cho AI summarization vÃ  Discord notification
# TÃ¬m cÃ¡c thÆ° má»¥c: analytics/, ai/, notifications/ á»Ÿ common/ (sau src/)
analytics_path = None
ai_path = None
notifications_path = None
config_path = None

# Thá»­ nhiá»u Ä‘Æ°á»ng dáº«n cÃ³ thá»ƒ cho cÃ¡c modules á»Ÿ common/
common_base_paths = [
    # Tá»« /opt/airflow (Docker default - Æ°u tiÃªn)
    "/opt/airflow/src/common",
    # Tá»« airflow/dags/ lÃªn 2 cáº¥p Ä‘áº¿n root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "common")),
    # Tá»« airflow/dags/ lÃªn 1 cáº¥p (náº¿u airflow/ lÃ  root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "common")),
    # Tá»« workspace root (náº¿u mount vÃ o /workspace)
    "/workspace/src/common",
    # Tá»« current working directory
    os.path.join(os.getcwd(), "src", "common"),
]

for common_base in common_base_paths:
    test_analytics = os.path.join(common_base, "analytics", "aggregator.py")
    test_ai = os.path.join(common_base, "ai", "summarizer.py")
    test_notifications = os.path.join(common_base, "notifications", "discord.py")
    test_config = os.path.join(common_base, "config.py")

    if os.path.exists(test_analytics):
        analytics_path = test_analytics
    if os.path.exists(test_ai):
        ai_path = test_ai
    if os.path.exists(test_notifications):
        notifications_path = test_notifications
    if os.path.exists(test_config):
        config_path = test_config

    if analytics_path and ai_path and notifications_path and config_path:
        break

# IMPORTANT: Load config.py TRÆ¯á»šC Ä‘á»ƒ Ä‘áº£m báº£o .env Ä‘Æ°á»£c load
# Äiá»u nÃ y Ä‘áº£m báº£o cÃ¡c biáº¿n mÃ´i trÆ°á»ng tá»« .env Ä‘Æ°á»£c set trÆ°á»›c khi cÃ¡c module khÃ¡c import
if config_path and os.path.exists(config_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.config", config_path)
        if spec is not None and spec.loader is not None:
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            # Config module sáº½ tá»± Ä‘á»™ng load .env khi Ä‘Æ°á»£c import
            import warnings

            warnings.warn(
                f"âœ… ÄÃ£ load common.config tá»« {config_path}, .env sáº½ Ä‘Æ°á»£c load tá»± Ä‘á»™ng",
                stacklevel=2,
            )
    except Exception as e:
        import warnings

        warnings.warn(f"âš ï¸  KhÃ´ng thá»ƒ load common.config: {e}", stacklevel=2)

# Import DataAggregator tá»« common/analytics/
if analytics_path and os.path.exists(analytics_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.analytics.aggregator", analytics_path)
        if spec is not None and spec.loader is not None:
            aggregator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(aggregator_module)
            DataAggregator = aggregator_module.DataAggregator
        else:
            DataAggregator = None
    except Exception as e:
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import common.analytics.aggregator module: {e}", stacklevel=2)
        DataAggregator = None
else:
    DataAggregator = None

# Import AISummarizer tá»« common/ai/
if ai_path and os.path.exists(ai_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.ai.summarizer", ai_path)
        if spec is not None and spec.loader is not None:
            ai_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(ai_module)
            AISummarizer = ai_module.AISummarizer
        else:
            AISummarizer = None
    except Exception as e:
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import common.ai.summarizer module: {e}", stacklevel=2)
        AISummarizer = None
else:
    AISummarizer = None

# Import DiscordNotifier tá»« common/notifications/
if notifications_path and os.path.exists(notifications_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "common.notifications.discord", notifications_path
        )
        if spec is not None and spec.loader is not None:
            notifications_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(notifications_module)
            DiscordNotifier = notifications_module.DiscordNotifier
        else:
            DiscordNotifier = None
    except Exception as e:
        import warnings

        warnings.warn(f"KhÃ´ng thá»ƒ import common.notifications.discord module: {e}", stacklevel=2)
        DiscordNotifier = None
else:
    DiscordNotifier = None


def get_logger(context):
    """Láº¥y logger tá»« context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: sá»­ dá»¥ng logging module
        import logging

        ti = context.get("task_instance")
        if ti:
            # Táº¡o logger vá»›i task_id vÃ  dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: dÃ¹ng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: dÃ¹ng root logger
        import logging

        return logging.getLogger("airflow.task")


def _fix_sys_path_for_pipelines_import(logger=None):
    """
    Sá»­a sys.path vÃ  sys.modules Ä‘á»ƒ Ä‘áº£m báº£o pipelines cÃ³ thá»ƒ Ä‘Æ°á»£c import Ä‘Ãºng cÃ¡ch.
    XÃ³a cÃ¡c Ä‘Æ°á»ng dáº«n con nhÆ° /opt/airflow/src/pipelines khá»i sys.path,
    xÃ³a cÃ¡c fake modules khá»i sys.modules, vÃ  chá»‰ giá»¯ láº¡i /opt/airflow/src.
    """
    import logging
    if logger is None:
        logger = logging.getLogger("airflow.task")
    
    # XÃ³a cÃ¡c fake modules khá»i sys.modules (quan trá»ng!)
    # CÃ¡c fake modules nÃ y Ä‘Æ°á»£c táº¡o á»Ÿ Ä‘áº§u file vÃ  gÃ¢y lá»—i 'pipelines' is not a package
    modules_to_remove = []
    for module_name in list(sys.modules.keys()):
        if module_name.startswith('pipelines'):
            modules_to_remove.append(module_name)
    
    for module_name in modules_to_remove:
        del sys.modules[module_name]
        if logger:
            logger.info(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a fake module khá»i sys.modules: {module_name}")
    
    # XÃ³a cÃ¡c Ä‘Æ°á»ng dáº«n con khá»i sys.path (gÃ¢y lá»—i 'pipelines' is not a package)
    paths_to_remove = []
    for path in sys.path:
        # XÃ³a cÃ¡c Ä‘Æ°á»ng dáº«n nhÆ° /opt/airflow/src/pipelines hoáº·c /opt/airflow/src/pipelines/crawl
        normalized_path = path.replace('\\', '/')
        if normalized_path.endswith('/pipelines') or normalized_path.endswith('/pipelines/crawl'):
            paths_to_remove.append(path)
    
    for path in paths_to_remove:
        if path in sys.path:
            sys.path.remove(path)
            if logger:
                logger.info(f"ğŸ—‘ï¸  ÄÃ£ xÃ³a Ä‘Æ°á»ng dáº«n sai khá»i sys.path: {path}")
    
    # Äáº£m báº£o /opt/airflow/src cÃ³ trong sys.path
    possible_src_paths = [
        "/opt/airflow/src",  # Docker default path
        os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "src")),  # Local dev
    ]
    
    for src_path in possible_src_paths:
        if os.path.exists(src_path) and os.path.isdir(src_path):
            if src_path not in sys.path:
                sys.path.insert(0, src_path)
                if logger:
                    logger.info(f"âœ… ÄÃ£ thÃªm vÃ o sys.path: {src_path}")
            return src_path
    
    return None


def extract_and_load_categories_to_db(**context) -> dict[str, Any]:
    """
    Task 0: Extract categories tá»« categories_tree.json vÃ  load vÃ o database

    Returns:
        Dict: Stats vá» viá»‡c load categories
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ“ TASK: Extract & Load Categories to Database")
    logger.info("=" * 70)

    try:
        # Import extract vÃ  load modules
        try:
            # Thá»­ import tá»« Ä‘Æ°á»ng dáº«n trong Docker/Airflow
            import sys
            import importlib.util
            from pathlib import Path

            # TÃ¬m Ä‘Æ°á»ng dáº«n Ä‘áº¿n extract_categories.py
            possible_paths = [
                "/opt/airflow/src/pipelines/extract/extract_categories.py",
                os.path.join(os.path.dirname(__file__), "..", "..", "src", "pipelines", "extract", "extract_categories.py"),
                os.path.join(os.getcwd(), "src", "pipelines", "extract", "extract_categories.py"),
            ]

            extract_module_path = None
            for path in possible_paths:
                test_path = Path(path)
                if test_path.exists():
                    extract_module_path = test_path
                    break

            if extract_module_path:
                spec = importlib.util.spec_from_file_location("extract_categories", extract_module_path)
                if spec and spec.loader:
                    extract_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(extract_module)
                    extract_categories_from_tree_file = extract_module.extract_categories_from_tree_file
                else:
                    raise ImportError("KhÃ´ng thá»ƒ load extract_categories module")
            else:
                raise ImportError("KhÃ´ng tÃ¬m tháº¥y extract_categories.py")
        except Exception as e:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ import extract module: {e}")
            logger.info("Thá»­ import trá»±c tiáº¿p...")
            # Fallback: thá»­ import trá»±c tiáº¿p
            try:
                from pipelines.extract.extract_categories import extract_categories_from_tree_file
            except ImportError:
                # Sá»­a sys.path vÃ  thá»­ láº¡i
                _fix_sys_path_for_pipelines_import(logger)
                try:
                    from pipelines.extract.extract_categories import extract_categories_from_tree_file
                except ImportError as e:
                    logger.error(f"âŒ KhÃ´ng thá»ƒ import extract_categories: {e}")
                    logger.error(f"   sys.path: {sys.path}")
                    raise

        # Import DataLoader
        try:
            from pipelines.load.loader import DataLoader
            logger.info("âœ… ÄÃ£ import DataLoader thÃ nh cÃ´ng")
        except ImportError:
            # Sá»­a sys.path vÃ  thá»­ láº¡i
            _fix_sys_path_for_pipelines_import(logger)
            try:
                from pipelines.load.loader import DataLoader
                logger.info("âœ… ÄÃ£ import DataLoader thÃ nh cÃ´ng")
            except ImportError as e:
                logger.error(f"âŒ KhÃ´ng thá»ƒ import DataLoader: {e}")
                logger.error(f"   sys.path: {sys.path}")
                raise

        # 1. Extract categories tá»« tree file
        tree_file = str(CATEGORIES_TREE_FILE)
        logger.info(f"ğŸ“– Äang extract categories tá»«: {tree_file}")

        if not os.path.exists(tree_file):
            logger.warning(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y file: {tree_file}")
            logger.info("Bá» qua task nÃ y, categories cÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c load trÆ°á»›c Ä‘Ã³")
            return {
                "total_loaded": 0,
                "db_loaded": 0,
                "success_count": 0,
                "failed_count": 0,
                "skipped": True,
            }

        categories = extract_categories_from_tree_file(tree_file)
        logger.info(f"âœ… ÄÃ£ extract {len(categories)} categories")

        # 2. Load vÃ o database
        logger.info("ğŸ’¾ Äang load categories vÃ o database...")

        # Láº¥y credentials tá»« environment variables
        loader = DataLoader(
            database=os.getenv("POSTGRES_DB", "crawl_data"),
            host=os.getenv("POSTGRES_HOST", "postgres"),
            port=int(os.getenv("POSTGRES_PORT", "5432")),
            user=os.getenv("POSTGRES_USER", "airflow_user"),
            password=os.getenv("POSTGRES_PASSWORD", ""),
            batch_size=100,
            enable_db=True,
        )

        try:
            stats = loader.load_categories(
                categories,
                save_to_file=None,  # KhÃ´ng lÆ°u file, chá»‰ load vÃ o DB
                upsert=True,
                validate_before_load=True,
            )

            logger.info(f"âœ… ÄÃ£ load {stats['db_loaded']} categories vÃ o database")
            logger.info(f"   - Tá»•ng sá»‘: {stats['total_loaded']}")
            logger.info(f"   - ThÃ nh cÃ´ng: {stats['success_count']}")
            logger.info(f"   - Tháº¥t báº¡i: {stats['failed_count']}")

            if stats.get("errors"):
                logger.warning(f"âš ï¸  CÃ³ {len(stats['errors'])} lá»—i (hiá»ƒn thá»‹ 5 Ä‘áº§u tiÃªn):")
                for error in stats["errors"][:5]:
                    logger.warning(f"   - {error}")

            loader.close()
            return stats

        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load vÃ o database: {e}", exc_info=True)
            loader.close()
            raise

    except Exception as e:
        logger.error(f"âŒ Lá»—i trong extract_and_load_categories_to_db: {e}", exc_info=True)
        raise


def load_categories(**context) -> list[dict[str, Any]]:
    """
    Task 1: Load danh sÃ¡ch danh má»¥c tá»« file

    Returns:
        List[Dict]: Danh sÃ¡ch danh má»¥c
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ“– TASK: Load Categories")
    logger.info("=" * 70)

    try:
        categories_file = str(CATEGORIES_FILE)
        logger.info(f"Äang Ä‘á»c file: {categories_file}")

        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {categories_file}")

        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)

        logger.info(f"âœ… ÄÃ£ load {len(categories)} danh má»¥c")

        # Lá»c danh má»¥c náº¿u cáº§n (vÃ­ dá»¥: chá»‰ láº¥y level 2-4)
        # CÃ³ thá»ƒ cáº¥u hÃ¬nh qua Airflow Variable
        try:
            min_level = int(Variable.get("TIKI_MIN_CATEGORY_LEVEL", default_var="2"))
            max_level = int(Variable.get("TIKI_MAX_CATEGORY_LEVEL", default_var="4"))
            categories = [
                cat for cat in categories if min_level <= cat.get("level", 0) <= max_level
            ]
            logger.info(f"âœ“ Sau khi lá»c level {min_level}-{max_level}: {len(categories)} danh má»¥c")
        except Exception as e:
            logger.warning(f"KhÃ´ng thá»ƒ lá»c theo level: {e}")

        # Giá»›i háº¡n sá»‘ danh má»¥c náº¿u cáº§n (Ä‘á»ƒ test)
        # TEST MODE: Hardcode giá»›i háº¡n 2 categories cho test
        max_categories = 2  # TEST MODE: Hardcode 2 categories cho test
        if max_categories > 0 and len(categories) > max_categories:
            logger.info(f"âš ï¸  TEST MODE: Giá»›i háº¡n tá»« {len(categories)} xuá»‘ng {max_categories} categories")
            categories = categories[:max_categories]
            logger.info(f"âœ… ÄÃ£ giá»›i háº¡n: {len(categories)} categories Ä‘á»ƒ crawl")
        
        # Váº«n kiá»ƒm tra Variable náº¿u cÃ³ (Ä‘á»ƒ override náº¿u cáº§n)
        try:
            var_max_categories = int(Variable.get("TIKI_MAX_CATEGORIES", default_var="0"))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"âœ“ Giá»›i háº¡n: {max_categories} danh má»¥c")
        except Exception:
            pass

        # Push categories lÃªn XCom Ä‘á»ƒ cÃ¡c task khÃ¡c dÃ¹ng
        return categories

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task 2: Crawl sáº£n pháº©m tá»« má»™t danh má»¥c (Dynamic Task Mapping)

    Tá»‘i Æ°u hÃ³a:
    - Rate limiting: delay giá»¯a cÃ¡c request
    - Caching: sá»­ dá»¥ng cache Ä‘á»ƒ trÃ¡nh crawl láº¡i
    - Error handling: tiáº¿p tá»¥c vá»›i danh má»¥c khÃ¡c khi lá»—i
    - Timeout: giá»›i háº¡n thá»i gian crawl

    Args:
        category: ThÃ´ng tin danh má»¥c (tá»« expand_kwargs)
        context: Airflow context

    Returns:
        Dict: Káº¿t quáº£ crawl vá»›i products vÃ  metadata
    """
    logger = get_logger(context)

    # Láº¥y category tá»« keyword argument hoáº·c tá»« op_kwargs trong context
    # Khi sá»­ dá»¥ng expand vá»›i op_kwargs, category sáº½ Ä‘Æ°á»£c truyá»n qua op_kwargs
    if not category:
        # Thá»­ láº¥y tá»« ti.op_kwargs (cÃ¡ch chÃ­nh xÃ¡c nháº¥t)
        ti = context.get("ti")
        if ti:
            # op_kwargs Ä‘Æ°á»£c truyá»n vÃ o function thÃ´ng qua ti
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                category = op_kwargs.get("category")

        # Fallback: thá»­ láº¥y tá»« context trá»±c tiáº¿p
        if not category:
            category = context.get("category") or context.get("op_kwargs", {}).get("category")

    if not category:
        # Debug: log context Ä‘á»ƒ tÃ¬m lá»—i
        logger.error(f"KhÃ´ng tÃ¬m tháº¥y category. Context keys: {list(context.keys())}")
        ti = context.get("ti")
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y category. Kiá»ƒm tra expand vá»›i op_kwargs.")

    category_url = category.get("url", "")
    category_name = category.get("name", "Unknown")
    category_id = category.get("id", "")

    logger.info("=" * 70)
    logger.info(f"ğŸ›ï¸  TASK: Crawl Category - {category_name}")
    logger.info(f"ğŸ”— URL: {category_url}")
    logger.info("=" * 70)

    result = {
        "category_id": category_id,
        "category_name": category_name,
        "category_url": category_url,
        "products": [],
        "status": "failed",
        "error": None,
        "crawled_at": datetime.now().isoformat(),
        "pages_crawled": 0,
        "products_count": 0,
    }

    try:
        # Kiá»ƒm tra graceful degradation
        if tiki_degradation.should_skip():
            result["error"] = "Service Ä‘ang á»Ÿ tráº¡ng thÃ¡i FAILED, skip crawl"
            result["status"] = "degraded"
            logger.warning(f"âš ï¸  Service degraded, skip category {category_name}")
            return result

        # Láº¥y cáº¥u hÃ¬nh tá»« Airflow Variables
        max_pages = int(
            Variable.get("TIKI_MAX_PAGES_PER_CATEGORY", default_var="20")
        )  # Máº·c Ä‘á»‹nh 20 trang Ä‘á»ƒ trÃ¡nh timeout
        use_selenium = Variable.get("TIKI_USE_SELENIUM", default_var="false").lower() == "true"
        timeout = int(Variable.get("TIKI_CRAWL_TIMEOUT", default_var="300"))  # 5 phÃºt máº·c Ä‘á»‹nh
        rate_limit_delay = float(
            Variable.get("TIKI_RATE_LIMIT_DELAY", default_var="1.0")
        )  # Delay 1s giá»¯a cÃ¡c request

        # Rate limiting: delay trÆ°á»›c khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl vá»›i timeout vÃ  circuit breaker
        start_time = time.time()

        def _crawl_with_params():
            """Wrapper function Ä‘á»ƒ gá»i vá»›i circuit breaker"""
            return crawl_category_products(
                category_url,
                max_pages=max_pages if max_pages > 0 else None,
                use_selenium=use_selenium,
                cache_dir=str(CACHE_DIR),
                use_redis_cache=True,  # Sá»­ dá»¥ng Redis cache
                use_rate_limiting=True,  # Sá»­ dá»¥ng rate limiting
            )

        try:
            # Gá»i vá»›i circuit breaker
            products = tiki_circuit_breaker.call(_crawl_with_params)
            tiki_degradation.record_success()
        except CircuitBreakerOpenError as e:
            # Circuit breaker Ä‘ang má»Ÿ
            result["error"] = f"Circuit breaker open: {str(e)}"
            result["status"] = "circuit_breaker_open"
            logger.warning(f"âš ï¸  Circuit breaker open cho category {category_name}: {e}")
            # ThÃªm vÃ o DLQ
            try:
                crawl_error = classify_error(
                    e, context={"category_url": category_url, "category_id": category_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_category_{category_id}",
                    task_type="crawl_category",
                    error=crawl_error,
                    context={
                        "category_url": category_url,
                        "category_name": category_name,
                        "category_id": category_id,
                    },
                    retry_count=0,
                )
                logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_category_{category_id}")
            except Exception as dlq_error:
                logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
            return result
        except Exception:
            # Ghi nháº­n failure
            tiki_degradation.record_failure()
            raise  # Re-raise Ä‘á»ƒ xá»­ lÃ½ bÃªn dÆ°á»›i

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(f"Crawl vÆ°á»£t quÃ¡ timeout {timeout}s")

        result["products"] = products
        result["status"] = "success"
        result["products_count"] = len(products)
        result["elapsed_time"] = elapsed

        logger.info(f"âœ… Crawl thÃ nh cÃ´ng: {len(products)} sáº£n pháº©m trong {elapsed:.1f}s")

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"â±ï¸  Timeout: {e}")
        # ThÃªm vÃ o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
        # KhÃ´ng raise Ä‘á»ƒ tiáº¿p tá»¥c vá»›i danh má»¥c khÃ¡c

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        logger.error(f"âŒ Lá»—i khi crawl category {category_name}: {e}", exc_info=True)
        # ThÃªm vÃ o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
        # KhÃ´ng raise Ä‘á»ƒ tiáº¿p tá»¥c vá»›i danh má»¥c khÃ¡c

    return result


def merge_products(**context) -> dict[str, Any]:
    """
    Task 3: Merge sáº£n pháº©m tá»« táº¥t cáº£ cÃ¡c danh má»¥c

    Returns:
        Dict: Tá»•ng há»£p sáº£n pháº©m vÃ  thá»‘ng kÃª
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ”„ TASK: Merge Products")
    logger.info("=" * 70)

    try:

        ti = context["ti"]

        # Láº¥y categories tá»« task load_categories (trong TaskGroup load_and_prepare)
        # Thá»­ nhiá»u cÃ¡ch Ä‘á»ƒ láº¥y categories
        categories = None

        # CÃ¡ch 1: Láº¥y tá»« task_id vá»›i TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
            logger.info(
                f"Láº¥y categories tá»« 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
            )
        except Exception as e:
            logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'load_and_prepare.load_categories': {e}")

        # CÃ¡ch 2: Thá»­ khÃ´ng cÃ³ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_categories")
                logger.info(
                    f"Láº¥y categories tá»« 'load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'load_categories': {e}")

        if not categories:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y categories tá»« XCom")

        logger.info(f"Äang merge káº¿t quáº£ tá»« {len(categories)} danh má»¥c...")

        # Láº¥y káº¿t quáº£ tá»« cÃ¡c task crawl (Dynamic Task Mapping)
        # Vá»›i Dynamic Task Mapping, cáº§n láº¥y tá»« task_id vá»›i map_index
        all_products = []
        stats = {
            "total_categories": len(categories),
            "success_categories": 0,
            "failed_categories": 0,
            "timeout_categories": 0,
            "total_products": 0,
            "unique_products": 0,
        }

        # Láº¥y káº¿t quáº£ tá»« cÃ¡c task crawl (Dynamic Task Mapping)
        # Vá»›i Dynamic Task Mapping trong Airflow 2.x, cáº§n láº¥y tá»« task_id vá»›i map_index
        task_id = "crawl_categories.crawl_category"

        # Láº¥y tá»« XCom - thá»­ nhiá»u cÃ¡ch
        try:
            # CÃ¡ch 1: Láº¥y táº¥t cáº£ káº¿t quáº£ tá»« XCom (Airflow 2.x cÃ³ thá»ƒ tráº£ vá» list)
            all_results = ti.xcom_pull(task_ids=task_id, key="return_value")

            # Xá»­ lÃ½ káº¿t quáº£
            if isinstance(all_results, list):
                # Náº¿u lÃ  list, xá»­ lÃ½ tá»«ng pháº§n tá»­
                for result in all_results:
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"â±ï¸  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"âŒ Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif isinstance(all_results, dict):
                # Náº¿u lÃ  dict, cÃ³ thá»ƒ key lÃ  map_index hoáº·c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"â±ï¸  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"âŒ Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif all_results and isinstance(all_results, dict):
                # Náº¿u chá»‰ cÃ³ 1 káº¿t quáº£ (dict)
                if all_results.get("status") == "success":
                    stats["success_categories"] += 1
                    products = all_results.get("products", [])
                    all_products.extend(products)
                    stats["total_products"] += len(products)
                elif all_results.get("status") == "timeout":
                    stats["timeout_categories"] += 1
                    logger.warning(f"â±ï¸  Category {all_results.get('category_name')} timeout")
                else:
                    stats["failed_categories"] += 1
                    logger.warning(
                        f"âŒ Category {all_results.get('category_name')} failed: {all_results.get('error')}"
                    )

            # Náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c, thá»­ láº¥y tá»«ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                logger.info("Thá»­ láº¥y tá»«ng map_index...")
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )

                        if result and isinstance(result, dict):
                            if result.get("status") == "success":
                                stats["success_categories"] += 1
                                products = result.get("products", [])
                                all_products.extend(products)
                                stats["total_products"] += len(products)
                            elif result.get("status") == "timeout":
                                stats["timeout_categories"] += 1
                                logger.warning(f"â±ï¸  Category {result.get('category_name')} timeout")
                            else:
                                stats["failed_categories"] += 1
                                logger.warning(
                                    f"âŒ Category {result.get('category_name')} failed: {result.get('error')}"
                                )
                    except Exception as e:
                        stats["failed_categories"] += 1
                        logger.warning(f"KhÃ´ng thá»ƒ láº¥y káº¿t quáº£ tá»« map_index {map_index}: {e}")

        except Exception as e:
            logger.error(f"KhÃ´ng thá»ƒ láº¥y káº¿t quáº£ tá»« XCom: {e}", exc_info=True)
            # Náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c, Ä‘Ã¡nh dáº¥u táº¥t cáº£ lÃ  failed
            stats["failed_categories"] = len(categories)

        # Loáº¡i bá» trÃ¹ng láº·p theo product_id
        seen_ids = set()
        unique_products = []
        products_with_sales_count = 0
        for product in all_products:
            product_id = product.get("product_id")
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                # Äáº£m báº£o sales_count luÃ´n cÃ³ trong product (ká»ƒ cáº£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                elif product.get("sales_count") is not None:
                    products_with_sales_count += 1
                unique_products.append(product)

        # Log thá»‘ng kÃª sales_count
        logger.info(
            f"ğŸ“Š Products cÃ³ sales_count: {products_with_sales_count}/{len(unique_products)} ({products_with_sales_count/len(unique_products)*100:.1f}%)"
            if unique_products
            else "ğŸ“Š Products cÃ³ sales_count: 0/0"
        )

        stats["unique_products"] = len(unique_products)

        logger.info("=" * 70)
        logger.info("ğŸ“Š THá»NG KÃŠ")
        logger.info("=" * 70)
        logger.info(f"ğŸ“ Tá»•ng danh má»¥c: {stats['total_categories']}")
        logger.info(f"âœ… ThÃ nh cÃ´ng: {stats['success_categories']}")
        logger.info(f"âŒ Tháº¥t báº¡i: {stats['failed_categories']}")
        logger.info(f"â±ï¸  Timeout: {stats['timeout_categories']}")
        logger.info(f"ğŸ“¦ Tá»•ng sáº£n pháº©m (trÆ°á»›c dedup): {stats['total_products']}")
        logger.info(f"ğŸ“¦ Sáº£n pháº©m unique: {stats['unique_products']}")
        logger.info("=" * 70)

        result = {
            "products": unique_products,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an toÃ n (atomic write) Ä‘á»ƒ trÃ¡nh corrupt

    Sá»­ dá»¥ng temporary file vÃ  rename Ä‘á»ƒ Ä‘áº£m báº£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi vÃ o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Atomic rename (trÃªn Unix) hoáº·c move (trÃªn Windows)
        if os.name == "nt":  # Windows
            # TrÃªn Windows, cáº§n xÃ³a file cÅ© trÆ°á»›c
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))

        logger.info(f"âœ… ÄÃ£ ghi file atomic: {filepath}")

    except Exception as e:
        # XÃ³a temp file náº¿u cÃ³ lá»—i
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"âŒ Lá»—i khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: LÆ°u sáº£n pháº©m vÃ o file (atomic write)

    Tá»‘i Æ°u hÃ³a cho dá»¯ liá»‡u lá»›n:
    - Batch processing: chia nhá» vÃ  lÆ°u tá»«ng batch
    - Atomic write: trÃ¡nh corrupt file
    - Compression: cÃ³ thá»ƒ nÃ©n file náº¿u cáº§n

    Returns:
        str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ’¾ TASK: Save Products")
    logger.info("=" * 70)

    try:
        # Láº¥y káº¿t quáº£ tá»« task merge_products (trong TaskGroup process_and_save)
        ti = context["ti"]
        merge_result = None

        # CÃ¡ch 1: Láº¥y tá»« task_id vá»›i TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
            logger.info("Láº¥y merge_result tá»« 'process_and_save.merge_products'")
        except Exception as e:
            logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'process_and_save.merge_products': {e}")

        # CÃ¡ch 2: Thá»­ khÃ´ng cÃ³ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
                logger.info("Láº¥y merge_result tá»« 'merge_products'")
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'merge_products': {e}")

        if not merge_result:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ merge tá»« XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"Äang lÆ°u {len(products)} sáº£n pháº©m...")

        # Batch processing cho dá»¯ liá»‡u lá»›n
        batch_size = int(Variable.get("TIKI_SAVE_BATCH_SIZE", default_var="10000"))

        if len(products) > batch_size:
            logger.info(f"Chia nhá» thÃ nh batches (má»—i batch {batch_size} sáº£n pháº©m)...")
            # LÆ°u tá»«ng batch vÃ o file riÃªng, sau Ä‘Ã³ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                batch_file = OUTPUT_DIR / f"products_batch_{i // batch_size}.json"
                batch_data = {
                    "batch_index": i // batch_size,
                    "total_batches": (len(products) + batch_size - 1) // batch_size,
                    "products": batch,
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"âœ“ ÄÃ£ lÆ°u batch {i // batch_size + 1}: {len(batch)} sáº£n pháº©m")

        # Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl tá»« Airflow DAG vá»›i Dynamic Task Mapping",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"âœ… ÄÃ£ lÆ°u {len(products)} sáº£n pháº©m vÃ o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi save products: {e}", exc_info=True)
        raise


def prepare_products_for_detail(**context) -> list[dict[str, Any]]:
    """
    Task: Chuáº©n bá»‹ danh sÃ¡ch products Ä‘á»ƒ crawl detail

    Tá»‘i Æ°u cho multi-day crawling:
    - Chá»‰ crawl products chÆ°a cÃ³ detail
    - Chia thÃ nh batches theo ngÃ y (cÃ³ thá»ƒ crawl trong nhiá»u ngÃ y)
    - Kiá»ƒm tra cache vÃ  progress Ä‘á»ƒ trÃ¡nh crawl láº¡i
    - Track progress Ä‘á»ƒ resume tá»« Ä‘iá»ƒm dá»«ng

    Returns:
        List[Dict]: List cÃ¡c dict chá»©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ“‹ TASK: Prepare Products for Detail Crawling (Multi-Day Support)")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # Láº¥y products tá»« task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Thá»­ láº¥y tá»« file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y products tá»« XCom hoáº·c file")

        products = merge_result.get("products", [])
        logger.info(f"ğŸ“Š Tá»•ng sá»‘ products: {len(products)}")

        # Äá»c progress file Ä‘á»ƒ biáº¿t Ä‘Ã£ crawl Ä‘áº¿n Ä‘Ã¢u
        progress = {
            "crawled_product_ids": set(),
            "last_crawled_index": 0,
            "total_crawled": 0,
            "last_updated": None,
        }

        if PROGRESS_FILE.exists():
            try:
                with open(PROGRESS_FILE, encoding="utf-8") as f:
                    saved_progress = json.load(f)
                    progress["crawled_product_ids"] = set(
                        saved_progress.get("crawled_product_ids", [])
                    )
                    progress["last_crawled_index"] = saved_progress.get("last_crawled_index", 0)
                    progress["total_crawled"] = saved_progress.get("total_crawled", 0)
                    progress["last_updated"] = saved_progress.get("last_updated")
                    logger.info(
                        f"ğŸ“‚ ÄÃ£ load progress: {len(progress['crawled_product_ids'])} products Ä‘Ã£ crawl"
                    )
            except Exception as e:
                logger.warning(f"âš ï¸  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c progress file: {e}")

        # Lá»c products cáº§n crawl detail
        products_to_crawl = []
        cache_hits = 0
        already_crawled = 0
        db_hits = 0  # Products Ä‘Ã£ cÃ³ trong DB

        # Láº¥y cáº¥u hÃ¬nh cho multi-day crawling
        # TÃ­nh toÃ¡n: 500 products ~ 52.75 phÃºt -> 280 products ~ 30 phÃºt
        products_per_day = int(
            Variable.get("TIKI_PRODUCTS_PER_DAY", default_var="120")
        )  # Máº·c Ä‘á»‹nh 280 products/ngÃ y (~30 phÃºt)
        max_products = 10  # TEST MODE: Hardcode 10 products cho test  # 0 = khÃ´ng giá»›i háº¡n  # 0 = khÃ´ng giá»›i háº¡n

        logger.info(
            f"âš™ï¸  Cáº¥u hÃ¬nh: {products_per_day} products/ngÃ y, max: {max_products if max_products > 0 else 'khÃ´ng giá»›i háº¡n'}"
        )

        # Kiá»ƒm tra products Ä‘Ã£ cÃ³ trong database vá»›i detail Ä‘áº§y Ä‘á»§ (Ä‘á»ƒ trÃ¡nh crawl láº¡i)
        # Chá»‰ skip products cÃ³ price vÃ  sales_count (detail Ä‘áº§y Ä‘á»§)
        existing_product_ids_in_db = set()
        try:
            PostgresStorage = _import_postgres_storage()
            if PostgresStorage is None:
                logger.warning("âš ï¸  KhÃ´ng thá»ƒ import PostgresStorage, bá» qua kiá»ƒm tra database")
            else:
                # Láº¥y database config
                db_host = Variable.get("POSTGRES_HOST", default_var=os.getenv("POSTGRES_HOST", "postgres"))
                db_port = int(Variable.get("POSTGRES_PORT", default_var=os.getenv("POSTGRES_PORT", "5432")))
                db_name = Variable.get("POSTGRES_DB", default_var=os.getenv("POSTGRES_DB", "crawl_data"))
                db_user = Variable.get("POSTGRES_USER", default_var=os.getenv("POSTGRES_USER", "postgres"))
                db_password = Variable.get("POSTGRES_PASSWORD", default_var=os.getenv("POSTGRES_PASSWORD", "postgres"))
                
                storage = PostgresStorage(
                    host=db_host,
                    port=db_port,
                    database=db_name,
                    user=db_user,
                    password=db_password,
                )
                
                # Láº¥y danh sÃ¡ch product_ids tá»« products list
                product_ids_to_check = [p.get("product_id") for p in products if p.get("product_id")]
                
                if product_ids_to_check:
                    logger.info(f"ğŸ” Äang kiá»ƒm tra {len(product_ids_to_check)} products trong database...")
                    logger.info("   (chá»‰ skip products cÃ³ price vÃ  sales_count - detail Ä‘áº§y Ä‘á»§)")
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            # Chia nhá» query náº¿u cÃ³ quÃ¡ nhiá»u product_ids
                            # Chá»‰ láº¥y products cÃ³ price vÃ  sales_count (detail Ä‘áº§y Ä‘á»§)
                            for i in range(0, len(product_ids_to_check), 1000):
                                batch_ids = product_ids_to_check[i : i + 1000]
                                placeholders = ",".join(["%s"] * len(batch_ids))
                                cur.execute(
                                    f"""
                                    SELECT product_id 
                                    FROM products 
                                    WHERE product_id IN ({placeholders})
                                      AND price IS NOT NULL 
                                      AND sales_count IS NOT NULL
                                    """,
                                    batch_ids,
                                )
                                existing_product_ids_in_db.update(row[0] for row in cur.fetchall())
                    
                    logger.info(f"âœ… TÃ¬m tháº¥y {len(existing_product_ids_in_db)} products Ä‘Ã£ cÃ³ detail Ä‘áº§y Ä‘á»§ trong database")
                    logger.info("   (cÃ³ price vÃ  sales_count - sáº½ skip crawl láº¡i)")
                    storage.close()
        except Exception as e:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra database: {e}")
            logger.info("   Sáº½ tiáº¿p tá»¥c vá»›i cache vÃ  progress file")

        # Báº¯t Ä‘áº§u tá»« index Ä‘Ã£ crawl
        start_index = progress["last_crawled_index"]
        
        # Kiá»ƒm tra náº¿u start_index vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng products hiá»‡n táº¡i
        # (cÃ³ thá»ƒ do test mode giá»›i háº¡n sá»‘ lÆ°á»£ng products)
        if start_index >= len(products):
            logger.warning("=" * 70)
            logger.warning(f"âš ï¸  RESET PROGRESS INDEX!")
            logger.warning(f"   - Progress index: {start_index}")
            logger.warning(f"   - Sá»‘ products hiá»‡n táº¡i: {len(products)}")
            logger.warning(f"   - Index vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng products")
            logger.warning("   - CÃ³ thá»ƒ do test mode giá»›i háº¡n sá»‘ lÆ°á»£ng products")
            logger.warning("   - Reset vá» index 0 Ä‘á»ƒ crawl láº¡i tá»« Ä‘áº§u")
            logger.warning("=" * 70)
            start_index = 0
            # Reset progress Ä‘á»ƒ trÃ¡nh nháº§m láº«n
            progress["last_crawled_index"] = 0
            progress["total_crawled"] = 0
            # Giá»¯ láº¡i crawled_product_ids Ä‘á»ƒ trÃ¡nh crawl láº¡i products Ä‘Ã£ cÃ³
        
        products_to_check = products[start_index:]

        logger.info(
            f"ğŸ”„ Báº¯t Ä‘áº§u tá»« index {start_index} (Ä‘Ã£ crawl {progress['total_crawled']} products, tá»•ng products: {len(products)})"
        )

        # Tá»‘i Æ°u: Duyá»‡t táº¥t cáº£ products Ä‘á»ƒ tÃ¬m products chÆ°a cÃ³ trong DB
        # Thay vÃ¬ dá»«ng khi Ä‘áº¡t max_products nhÆ°ng toÃ n bá»™ lÃ  skip
        skipped_count = 0
        max_skipped_before_stop = 100  # Dá»«ng náº¿u skip liÃªn tiáº¿p 100 products
        
        for idx, product in enumerate(products_to_check):
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not product_id or not product_url:
                continue

            # Kiá»ƒm tra xem Ä‘Ã£ crawl chÆ°a (tá»« progress)
            if product_id in progress["crawled_product_ids"]:
                already_crawled += 1
                skipped_count += 1
                continue

            # Kiá»ƒm tra xem Ä‘Ã£ cÃ³ trong database chÆ°a (vá»›i detail Ä‘áº§y Ä‘á»§)
            # existing_product_ids_in_db chá»‰ chá»©a products cÃ³ price vÃ  sales_count
            if product_id in existing_product_ids_in_db:
                # ÄÃ£ cÃ³ trong DB vá»›i detail Ä‘áº§y Ä‘á»§ (cÃ³ price vÃ  sales_count)
                # â†’ Skip crawl láº¡i
                db_hits += 1
                progress["crawled_product_ids"].add(product_id)
                already_crawled += 1
                skipped_count += 1
                continue

            # Kiá»ƒm tra cache
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            has_valid_cache = False
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        # Kiá»ƒm tra cache cÃ³ Ä‘áº§y Ä‘á»§ khÃ´ng: cáº§n cÃ³ price vÃ  sales_count
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None

                        # Náº¿u Ä‘Ã£ cÃ³ detail Ä‘áº§y Ä‘á»§ (cÃ³ price vÃ  sales_count), Ä‘Ã¡nh dáº¥u Ä‘Ã£ crawl
                        if has_price and has_sales_count:
                            cache_hits += 1
                            progress["crawled_product_ids"].add(product_id)
                            already_crawled += 1
                            has_valid_cache = True
                            skipped_count += 1
                        # Náº¿u cache thiáº¿u sales_count, váº«n cáº§n crawl láº¡i
                except Exception:
                    pass

            # Náº¿u chÆ°a cÃ³ cache há»£p lá»‡, thÃªm vÃ o danh sÃ¡ch crawl
            if not has_valid_cache:
                products_to_crawl.append(
                    {
                        "product_id": product_id,
                        "url": product_url,
                        "name": product.get("name", ""),
                        "product": product,  # Giá»¯ nguyÃªn product data
                        "index": start_index + idx,  # LÆ°u index Ä‘á»ƒ track progress
                    }
                )
                skipped_count = 0  # Reset counter khi tÃ¬m tháº¥y product má»›i

            # Giá»›i háº¡n sá»‘ lÆ°á»£ng products crawl trong ngÃ y nÃ y
            if len(products_to_crawl) >= products_per_day:
                logger.info(f"âœ“ ÄÃ£ Ä‘áº¡t giá»›i háº¡n {products_per_day} products cho ngÃ y hÃ´m nay")
                break

            # Giá»›i háº¡n tá»•ng sá»‘ (náº¿u cÃ³)
            if max_products > 0 and len(products_to_crawl) >= max_products:
                logger.info(f"âœ“ ÄÃ£ Ä‘áº¡t giá»›i háº¡n tá»•ng {max_products} products")
                break
            
            # Dá»«ng náº¿u skip quÃ¡ nhiá»u products liÃªn tiáº¿p (cÃ³ thá»ƒ Ä‘Ã£ háº¿t products má»›i)
            if skipped_count >= max_skipped_before_stop:
                logger.info(f"âš ï¸  ÄÃ£ skip {skipped_count} products liÃªn tiáº¿p, cÃ³ thá»ƒ Ä‘Ã£ háº¿t products má»›i")
                logger.info(f"   - ÄÃ£ tÃ¬m Ä‘Æ°á»£c {len(products_to_crawl)} products Ä‘á»ƒ crawl")
                break

        logger.info("=" * 70)
        logger.info("ğŸ“Š THá»NG KÃŠ PREPARE PRODUCTS FOR DETAIL")
        logger.info("=" * 70)
        logger.info(f"ğŸ“¦ Tá»•ng products Ä‘áº§u vÃ o: {len(products)}")
        logger.info(f"âœ… Products cáº§n crawl hÃ´m nay: {len(products_to_crawl)}")
        logger.info(f"ğŸ“¦ Cache hits (cÃ³ cache há»£p lá»‡): {cache_hits}")
        logger.info(f"ğŸ’¾ DB hits (Ä‘Ã£ cÃ³ trong DB vá»›i detail Ä‘áº§y Ä‘á»§): {db_hits}")
        logger.info(f"âœ“ ÄÃ£ crawl trÆ°á»›c Ä‘Ã³ (tá»« progress): {already_crawled - db_hits - cache_hits}")
        logger.info(f"ğŸ“ˆ Tá»•ng Ä‘Ã£ crawl: {progress['total_crawled'] + already_crawled}")
        logger.info(
            f"ğŸ“‰ CÃ²n láº¡i: {len(products) - (progress['total_crawled'] + already_crawled + len(products_to_crawl))}"
        )
        logger.info("=" * 70)
        
        if len(products_to_crawl) == 0:
            logger.warning("=" * 70)
            logger.warning("âš ï¸  KHÃ”NG CÃ“ PRODUCTS NÃ€O Cáº¦N CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("ğŸ’¡ LÃ½ do:")
            if already_crawled > 0:
                logger.warning(f"   - ÄÃ£ cÃ³ trong progress: {already_crawled - db_hits - cache_hits} products")
            if cache_hits > 0:
                logger.warning(f"   - ÄÃ£ cÃ³ trong cache (cÃ³ price vÃ  sales_count): {cache_hits} products")
            if db_hits > 0:
                logger.warning(f"   - ÄÃ£ cÃ³ trong database (cÃ³ price vÃ  sales_count): {db_hits} products")
            logger.warning("=" * 70)
            logger.warning("ğŸ’¡ Äá»ƒ force crawl láº¡i, báº¡n cÃ³ thá»ƒ:")
            logger.warning("   1. XÃ³a progress file: data/processed/detail_crawl_progress.json")
            logger.warning("   2. XÃ³a cache files trong: data/raw/products/detail/cache/")
            logger.warning("   3. XÃ³a products trong database (náº¿u muá»‘n crawl láº¡i)")
            logger.warning("=" * 70)

        # LÆ°u progress (sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau khi crawl xong)
        if products_to_crawl:
            # LÆ°u index cá»§a product cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c crawl
            last_index = products_to_crawl[-1]["index"]
            progress["last_crawled_index"] = last_index + 1
            progress["last_updated"] = datetime.now().isoformat()

            # LÆ°u progress vÃ o file
            try:
                with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "crawled_product_ids": list(progress["crawled_product_ids"]),
                            "last_crawled_index": progress["last_crawled_index"],
                            "total_crawled": progress["total_crawled"] + already_crawled,
                            "last_updated": progress["last_updated"],
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
                logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u progress: index {progress['last_crawled_index']}")
            except Exception as e:
                logger.warning(f"âš ï¸  KhÃ´ng lÆ°u Ä‘Æ°á»£c progress: {e}")

        # Debug: Log má»™t vÃ i products Ä‘áº§u tiÃªn
        if products_to_crawl:
            logger.info("ğŸ“‹ Sample products (first 3):")
            for i, p in enumerate(products_to_crawl[:3]):
                logger.info(
                    f"  {i+1}. Product ID: {p.get('product_id')}, URL: {p.get('url')[:80]}..."
                )
        else:
            logger.warning("âš ï¸  KhÃ´ng cÃ³ products nÃ o cáº§n crawl detail hÃ´m nay!")
            logger.info("ğŸ’¡ Táº¥t cáº£ products Ä‘Ã£ Ä‘Æ°á»£c crawl hoáº·c cÃ³ cache há»£p lá»‡")

        logger.info(f"ğŸ”¢ Tráº£ vá» {len(products_to_crawl)} products cho Dynamic Task Mapping")

        return products_to_crawl

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi prepare products: {e}", exc_info=True)
        raise


def crawl_product_batch(product_batch: list[dict[str, Any]] = None, batch_index: int = -1, **context) -> list[dict[str, Any]]:
    """
    Task: Crawl detail cho má»™t batch products (Batch Processing vá»›i Driver Pooling vÃ  Async)
    
    Tá»‘i Æ°u:
    - Batch processing: 10 products/batch
    - Driver pooling: Reuse Selenium drivers trong batch
    - Async/aiohttp: Crawl parallel trong batch
    - Fallback Selenium: Náº¿u aiohttp thiáº¿u sales_count
    
    Args:
        product_batch: List products trong batch (tá»« expand_kwargs)
        batch_index: Index cá»§a batch
        context: Airflow context
    
    Returns:
        List[Dict]: List káº¿t quáº£ crawl cho batch
    """
    try:
        logger = get_logger(context)
    except Exception:
        import logging
        logger = logging.getLogger("airflow.task")
    
    # Láº¥y product_batch tá»« op_kwargs náº¿u chÆ°a cÃ³
    if not product_batch:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_batch = op_kwargs.get("product_batch")
                batch_index = op_kwargs.get("batch_index", -1)
        
        if not product_batch:
            product_batch = context.get("product_batch") or context.get("op_kwargs", {}).get("product_batch")
            batch_index = context.get("batch_index", -1)
    
    if not product_batch:
        logger.error("=" * 70)
        logger.error("âŒ KHÃ”NG TÃŒM THáº¤Y PRODUCT_BATCH TRONG CONTEXT!")
        logger.error("=" * 70)
        logger.error("ğŸ’¡ Debug info:")
        logger.error(f"   - Context keys: {list(context.keys())}")
        if ti:
            logger.error(f"   - ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        logger.error("=" * 70)
        return []
    
    # Validate product_batch
    if not isinstance(product_batch, list):
        logger.error("=" * 70)
        logger.error(f"âŒ PRODUCT_BATCH KHÃ”NG PHáº¢I LIST: {type(product_batch)}")
        logger.error(f"   - Value: {product_batch}")
        logger.error("=" * 70)
        return []
    
    if len(product_batch) == 0:
        logger.warning("=" * 70)
        logger.warning(f"âš ï¸  BATCH {batch_index} Rá»–NG - KhÃ´ng cÃ³ products nÃ o")
        logger.warning("=" * 70)
        return []
    
    logger.info("=" * 70)
    logger.info(f"ğŸ“¦ BATCH {batch_index}: Crawl {len(product_batch)} products")
    logger.info(f"   - Product IDs: {[p.get('product_id', 'unknown') for p in product_batch[:5]]}")
    if len(product_batch) > 5:
        logger.info(f"   - ... vÃ  {len(product_batch) - 5} products ná»¯a")
    logger.info("=" * 70)
    
    results = []
    
    try:
        import asyncio
        # Sá»­ dá»¥ng hÃ m Ä‘Ã£ Ä‘Æ°á»£c import á»Ÿ Ä‘áº§u file
        # crawl_product_detail_async vÃ  SeleniumDriverPool Ä‘Ã£ Ä‘Æ°á»£c import á»Ÿ Ä‘áº§u file
        if SeleniumDriverPool is None:
            raise ImportError("SeleniumDriverPool chÆ°a Ä‘Æ°á»£c import tá»« utils module")
        
        # Táº¡o driver pool cho batch
        driver_pool = SeleniumDriverPool(pool_size=5, headless=True, timeout=60)
        
        # Táº¡o event loop trÆ°á»›c
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # Session sáº½ Ä‘Æ°á»£c táº¡o bÃªn trong async function (cáº§n async context)
        session = None
        
        async def crawl_single_async(product_info: dict) -> dict[str, Any]:
            """Crawl má»™t product vá»›i async"""
            product_id = product_info.get("product_id", "unknown")
            product_url = product_info.get("url", "")
            
            result = {
                "product_id": product_id,
                "url": product_url,
                "status": "failed",
                "error": None,
                "detail": None,
                "crawled_at": datetime.now().isoformat(),
            }
            
            try:
                # Thá»­ async crawl trÆ°á»›c
                if session:
                    detail = await crawl_product_detail_async(
                        product_url,
                        session=session,
                        use_selenium_fallback=True,
                        verbose=False
                    )
                    
                    # Kiá»ƒm tra náº¿u crawl_product_detail_async tráº£ vá» HTML string (do fallback vá» Selenium)
                    if isinstance(detail, str) and detail.strip().startswith("<"):
                        # PhÃ¢n tÃ­ch HTML Ä‘á»ƒ xÃ¡c Ä‘á»‹nh loáº¡i
                        html_preview = detail[:500] if len(detail) > 500 else detail
                        html_lower = detail.lower()
                        
                        # Kiá»ƒm tra cÃ¡c trÆ°á»ng há»£p Ä‘áº·c biá»‡t
                        # Kiá»ƒm tra error page - cáº§n kiá»ƒm tra ká»¹ hÆ¡n Ä‘á»ƒ trÃ¡nh false positive
                        # Error page thÆ°á»ng cÃ³ title hoáº·c heading chá»©a "404", "not found", etc.
                        is_error_page = False
                        error_keywords = [
                            "404", "not found", "page not found", 
                            "500", "internal server error",
                            "403", "forbidden", "access denied"
                        ]
                        # Chá»‰ coi lÃ  error page náº¿u cÃ³ keyword trong title hoáº·c heading, khÃ´ng pháº£i trong toÃ n bá»™ HTML
                        # VÃ¬ má»™t sá»‘ product cÃ³ thá»ƒ cÃ³ "404" trong tÃªn hoáº·c mÃ´ táº£
                        if any(keyword in html_lower for keyword in error_keywords):
                            # Kiá»ƒm tra trong title tag hoáº·c h1 tag (nÆ¡i thÆ°á»ng cÃ³ error message)
                            title_match = re.search(r'<title[^>]*>(.*?)</title>', html_lower, re.IGNORECASE | re.DOTALL)
                            h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html_lower, re.IGNORECASE | re.DOTALL)
                            
                            title_text = title_match.group(1) if title_match else ""
                            h1_text = h1_match.group(1) if h1_match else ""
                            
                            # Chá»‰ coi lÃ  error náº¿u keyword xuáº¥t hiá»‡n trong title hoáº·c h1
                            is_error_page = any(
                                keyword in title_text or keyword in h1_text 
                                for keyword in error_keywords
                            )
                        
                        is_captcha = any(keyword in html_lower for keyword in [
                            "captcha", "recaptcha", "cloudflare", "checking your browser"
                        ])
                        has_next_data = "__next_data__" in html_lower or 'id="__NEXT_DATA__"' in html_lower
                        
                        # Kiá»ƒm tra xem cÃ³ pháº£i lÃ  HTML bÃ¬nh thÆ°á»ng cá»§a Tiki khÃ´ng
                        is_tiki_page = any(indicator in html_lower for indicator in [
                            "tiki.vn", "tiki", "pdp_product_name", "product-detail",
                            "data-view-id", "pdp-product"
                        ])
                        
                        if is_error_page:
                            logger.warning(f"âš ï¸  HTML lÃ  error page cho product {product_id}: {html_preview[:200]}...")
                            detail = None
                        elif is_captcha:
                            logger.warning(f"âš ï¸  HTML lÃ  captcha/block page cho product {product_id}")
                            detail = None
                        elif not is_tiki_page and not has_next_data:
                            # Náº¿u khÃ´ng pháº£i Tiki page vÃ  khÃ´ng cÃ³ __NEXT_DATA__, cÃ³ thá»ƒ lÃ  page láº¡
                            logger.warning(f"âš ï¸  HTML khÃ´ng giá»‘ng Tiki product page cho product {product_id}")
                            logger.warning(f"   - CÃ³ __NEXT_DATA__: {has_next_data}")
                            logger.warning(f"   - HTML preview: {html_preview[:300]}...")
                            # Váº«n thá»­ parse, cÃ³ thá»ƒ váº«n extract Ä‘Æ°á»£c má»™t sá»‘ thÃ´ng tin
                        else:
                            logger.info(f"â„¹ï¸  crawl_product_detail_async tráº£ vá» HTML (fallback Selenium) cho product {product_id}")
                            logger.info(f"   - HTML length: {len(detail)} chars")
                            logger.info(f"   - CÃ³ __NEXT_DATA__: {has_next_data}")
                            
                            # Parse HTML thÃ nh dict
                            try:
                                detail = extract_product_detail(detail, product_url, verbose=False)
                                if detail and isinstance(detail, dict):
                                    # Kiá»ƒm tra xem cÃ³ Ä‘áº§y Ä‘á»§ thÃ´ng tin khÃ´ng
                                    has_name = bool(detail.get("name"))
                                    has_price = bool(detail.get("price", {}).get("current_price"))
                                    has_sales = detail.get("sales_count") is not None
                                    logger.info(f"âœ… ÄÃ£ parse HTML thÃ nh cÃ´ng cho product {product_id}")
                                    logger.info(f"   - CÃ³ name: {has_name}, cÃ³ price: {has_price}, cÃ³ sales_count: {has_sales}")
                                else:
                                    logger.warning(f"âš ï¸  extract_product_detail tráº£ vá» None hoáº·c khÃ´ng pháº£i dict cho product {product_id}")
                                    detail = None
                            except Exception as parse_error:
                                logger.warning(f"âš ï¸  Lá»—i khi parse HTML tá»« crawl_product_detail_async: {parse_error}")
                                logger.debug(f"   HTML preview: {html_preview}")
                                detail = None
                    
                    # Äáº£m báº£o detail lÃ  dict
                    if detail and not isinstance(detail, dict):
                        logger.warning(f"âš ï¸  crawl_product_detail_async tráº£ vá» {type(detail)} thay vÃ¬ dict cho product {product_id}")
                        detail = None
                else:
                    # Fallback vá» Selenium náº¿u khÃ´ng cÃ³ aiohttp
                    # Sá»­ dá»¥ng hÃ m Ä‘Ã£ Ä‘Æ°á»£c import á»Ÿ Ä‘áº§u file
                    html = crawl_product_detail_with_selenium(
                        product_url,
                        verbose=False,
                        max_retries=2,  # TEST MODE: Giáº£m retry xuá»‘ng 2,
                        timeout=60,
                        use_redis_cache=True,
                        use_rate_limiting=True
                    )
                    if html:
                        # Sá»­ dá»¥ng hÃ m Ä‘Ã£ Ä‘Æ°á»£c import á»Ÿ Ä‘áº§u file
                        detail = extract_product_detail(html, product_url, verbose=False)
                        
                        # Kiá»ƒm tra náº¿u extract_product_detail tráº£ vá» HTML thay vÃ¬ dict
                        if isinstance(detail, str) and detail.strip().startswith("<"):
                            logger.warning(f"âš ï¸  extract_product_detail tráº£ vá» HTML thay vÃ¬ dict cho product {product_id}, thá»­ parse láº¡i")
                            # Thá»­ parse láº¡i HTML
                            try:
                                detail = extract_product_detail(html, product_url, verbose=False)
                            except Exception as parse_error:
                                logger.warning(f"âš ï¸  Lá»—i khi parse láº¡i HTML: {parse_error}")
                                detail = None
                        
                        # Äáº£m báº£o detail lÃ  dict, khÃ´ng pháº£i HTML string
                        if not isinstance(detail, dict):
                            logger.warning(f"âš ï¸  extract_product_detail tráº£ vá» {type(detail)} thay vÃ¬ dict cho product {product_id}")
                            detail = None
                    else:
                        detail = None
                
                if detail and isinstance(detail, dict):
                    result["detail"] = detail
                    result["status"] = "success"
                else:
                    result["error"] = "KhÃ´ng thá»ƒ crawl detail hoáº·c extract detail khÃ´ng há»£p lá»‡"
                    result["status"] = "failed"
                    
            except Exception as e:
                result["error"] = str(e)
                result["status"] = "failed"
                logger.warning(f"âš ï¸  Lá»—i khi crawl product {product_id}: {e}")
            
            return result
        
        # Crawl táº¥t cáº£ products trong batch song song vá»›i async
        # (Event loop Ä‘Ã£ Ä‘Æ°á»£c táº¡o á»Ÿ trÃªn)
        # Sá»­ dá»¥ng asyncio.gather() Ä‘á»ƒ crawl parallel
        rate_limit_delay = float(
            Variable.get("TIKI_DETAIL_RATE_LIMIT_DELAY", default_var="1.5")
        )
        
        # Táº¡o tasks vá»›i rate limiting: stagger start times
        async def crawl_batch_parallel():
            """Crawl batch vá»›i parallel processing vÃ  rate limiting"""
            # Táº¡o session ngay láº­p tá»©c trong async context (trÆ°á»›c khi táº¡o tasks)
            # Äáº£m báº£o session Ä‘Æ°á»£c táº¡o trong async context cÃ³ event loop
            nonlocal session
            if session is None:
                try:
                    import aiohttp
                    timeout = aiohttp.ClientTimeout(total=30)
                    # Táº¡o session trong async context (cÃ³ event loop Ä‘ang cháº¡y)
                    # ÄÃ¢y lÃ  async function nÃªn event loop Ä‘Ã£ cÃ³ sáºµn
                    session = aiohttp.ClientSession(
                        timeout=timeout,
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        }
                    )
                    logger.info("âœ… ÄÃ£ táº¡o aiohttp session trong async context")
                except RuntimeError as e:
                    # Lá»—i "no running event loop" - fallback vá» Selenium
                    logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ táº¡o aiohttp session (no event loop): {e}, sáº½ dÃ¹ng Selenium")
                    session = None
                except Exception as e:
                    logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ táº¡o aiohttp session: {e}, sáº½ dÃ¹ng Selenium")
                    session = None
            
            # Factory function Ä‘á»ƒ trÃ¡nh closure issue
            def create_crawl_task(product_info, delay_value):
                async def crawl_with_delay():
                    if delay_value > 0:
                        await asyncio.sleep(delay_value)
                    return await crawl_single_async(product_info)
                return crawl_with_delay()
            
            tasks = []
            for i, product in enumerate(product_batch):
                delay = i * rate_limit_delay / len(product_batch)  # PhÃ¢n tÃ¡n delay
                task = create_crawl_task(product, delay)
                tasks.append(task)
            
            # Cháº¡y táº¥t cáº£ tasks song song
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Xá»­ lÃ½ exceptions
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    product_info = product_batch[i]
                    processed_results.append({
                        "product_id": product_info.get("product_id", "unknown"),
                        "url": product_info.get("url", ""),
                        "status": "failed",
                        "error": str(result),
                        "detail": None,
                        "crawled_at": datetime.now().isoformat(),
                    })
                else:
                    processed_results.append(result)
            
            return processed_results
        
        results = loop.run_until_complete(crawl_batch_parallel())
        
        # ÄÃ³ng session
        if session:
            loop.run_until_complete(session.close())
        
        # Cleanup driver pool
        driver_pool.cleanup()
        
        # Thá»‘ng kÃª
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = len(results) - success_count
        
        logger.info(f"âœ… Batch {batch_index} hoÃ n thÃ nh:")
        logger.info(f"   - Success: {success_count}/{len(product_batch)}")
        logger.info(f"   - Failed: {failed_count}/{len(product_batch)}")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi crawl batch {batch_index}: {e}", exc_info=True)
        # Tráº£ vá» results vá»›i status failed cho táº¥t cáº£
        if product_batch and isinstance(product_batch, list):
            for product_info in product_batch:
                results.append({
                    "product_id": product_info.get("product_id", "unknown"),
                    "url": product_info.get("url", ""),
                    "status": "failed",
                    "error": f"Batch error: {str(e)}",
                    "detail": None,
                    "crawled_at": datetime.now().isoformat(),
                })
        else:
            logger.error("âš ï¸  KhÃ´ng thá»ƒ táº¡o failed results vÃ¬ product_batch khÃ´ng há»£p lá»‡")
    
    return results


def crawl_single_product_detail(product_info: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task: Crawl detail cho má»™t product (Dynamic Task Mapping)

    Tá»‘i Æ°u:
    - Sá»­ dá»¥ng cache Ä‘á»ƒ trÃ¡nh crawl láº¡i
    - Rate limiting
    - Error handling: tiáº¿p tá»¥c vá»›i product khÃ¡c khi lá»—i
    - Atomic write cache

    Args:
        product_info: ThÃ´ng tin product (tá»« expand_kwargs)
        context: Airflow context

    Returns:
        Dict: Káº¿t quáº£ crawl vá»›i detail vÃ  metadata
    """
    # Khá»Ÿi táº¡o result máº·c Ä‘á»‹nh
    default_result = {
        "product_id": "unknown",
        "url": "",
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    try:
        logger = get_logger(context)
    except Exception as e:
        # Náº¿u khÃ´ng thá»ƒ táº¡o logger, váº«n tiáº¿p tá»¥c vá»›i default result
        import logging

        logger = logging.getLogger("airflow.task")
        logger.error(f"KhÃ´ng thá»ƒ táº¡o logger tá»« context: {e}")

    # Láº¥y product_info tá»« keyword argument hoáº·c context
    if not product_info:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_info = op_kwargs.get("product_info")

        if not product_info:
            product_info = context.get("product_info") or context.get("op_kwargs", {}).get(
                "product_info"
            )

    if not product_info:
        logger.error(f"KhÃ´ng tÃ¬m tháº¥y product_info. Context keys: {list(context.keys())}")
        # Return result vá»›i status failed thay vÃ¬ raise exception
        return {
            "product_id": "unknown",
            "url": "",
            "status": "failed",
            "error": "KhÃ´ng tÃ¬m tháº¥y product_info trong context",
            "detail": None,
            "crawled_at": datetime.now().isoformat(),
        }

    product_id = product_info.get("product_id", "")
    product_url = product_info.get("url", "")
    product_name = product_info.get("name", "Unknown")

    logger.info("=" * 70)
    logger.info(f"ğŸ” TASK: Crawl Product Detail - {product_name}")
    logger.info(f"ğŸ†” Product ID: {product_id}")
    logger.info(f"ğŸ”— URL: {product_url}")
    logger.info("=" * 70)

    result = {
        "product_id": product_id,
        "url": product_url,
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    # Kiá»ƒm tra cache trÆ°á»›c - Æ°u tiÃªn Redis, fallback vá» file
    # Kiá»ƒm tra xem cÃ³ force refresh khÃ´ng (tá»« Airflow Variable)
    force_refresh = Variable.get("TIKI_FORCE_REFRESH_CACHE", default_var="false").lower() == "true"
    
    if force_refresh:
        logger.info(f"ğŸ”„ FORCE REFRESH MODE: Bá» qua cache cho product {product_id}")
    else:
        # Thá»­ Redis cache trÆ°á»›c (nhanh hÆ¡n, distributed)
        logger.info(f"ğŸ” Äang kiá»ƒm tra cache cho product {product_id}...")
        redis_cache = None
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_detail = redis_cache.get_cached_product_detail(product_id)
                if cached_detail:
                    # Kiá»ƒm tra cache cÃ³ Ä‘áº§y Ä‘á»§ khÃ´ng: cáº§n cÃ³ price vÃ  sales_count
                    has_price = cached_detail.get("price", {}).get("current_price")
                    has_sales_count = cached_detail.get("sales_count") is not None

                    # Náº¿u Ä‘Ã£ cÃ³ detail Ä‘áº§y Ä‘á»§ (cÃ³ price vÃ  sales_count), dÃ¹ng cache
                    if has_price and has_sales_count:
                        logger.info("=" * 70)
                        logger.info(f"âœ… SKIP CRAWL - Redis Cache Hit cho product {product_id}")
                        logger.info(f"   - CÃ³ price: {has_price}")
                        logger.info(f"   - CÃ³ sales_count: {has_sales_count}")
                        logger.info("   - Sá»­ dá»¥ng cache, khÃ´ng cáº§n crawl láº¡i")
                        logger.info("=" * 70)
                        result["detail"] = cached_detail
                        result["status"] = "cached"
                        return result
                elif has_price:
                    # Cache cÃ³ price nhÆ°ng thiáº¿u sales_count â†’ crawl láº¡i Ä‘á»ƒ láº¥y sales_count
                    logger.info(
                        f"[Redis Cache] âš ï¸  Cache thiáº¿u sales_count cho product {product_id}, sáº½ crawl láº¡i"
                    )
                else:
                    # Cache khÃ´ng Ä‘áº§y Ä‘á»§ â†’ crawl láº¡i
                    logger.info(
                        f"[Redis Cache] âš ï¸  Cache khÃ´ng Ä‘áº§y Ä‘á»§ cho product {product_id}, sáº½ crawl láº¡i"
                    )
        except Exception:
            # Redis khÃ´ng available, fallback vá» file cache
            pass
        
        # Fallback: Kiá»ƒm tra file cache náº¿u Redis khÃ´ng available hoáº·c khÃ´ng cÃ³ cache
        if not force_refresh:
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        # Kiá»ƒm tra cache cÃ³ Ä‘áº§y Ä‘á»§ khÃ´ng: cáº§n cÃ³ price vÃ  sales_count
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None

                        # Náº¿u Ä‘Ã£ cÃ³ detail Ä‘áº§y Ä‘á»§ (cÃ³ price vÃ  sales_count), dÃ¹ng cache
                        if has_price and has_sales_count:
                            logger.info("=" * 70)
                            logger.info(f"âœ… SKIP CRAWL - File Cache Hit cho product {product_id}")
                            logger.info(f"   - CÃ³ price: {has_price}")
                            logger.info(f"   - CÃ³ sales_count: {has_sales_count}")
                            logger.info("   - Sá»­ dá»¥ng cache, khÃ´ng cáº§n crawl láº¡i")
                            logger.info("=" * 70)
                            result["detail"] = cached_detail
                            result["status"] = "cached"
                            return result
                except Exception:
                    # File cache lá»—i, tiáº¿p tá»¥c crawl
                    pass

    # Tiáº¿p tá»¥c crawl náº¿u khÃ´ng cÃ³ cache hoáº·c force refresh
    # (File cache check Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ á»Ÿ trÃªn trong else block)
    
    # Báº¯t Ä‘áº§u crawl product detail
    try:
        # Kiá»ƒm tra graceful degradation
        if tiki_degradation.should_skip():
            logger.warning("=" * 70)
            logger.warning(f"âš ï¸  SKIP CRAWL - Service Degraded cho product {product_id}")
            logger.warning("   - Service Ä‘ang á»Ÿ tráº¡ng thÃ¡i FAILED")
            logger.warning("   - Graceful degradation: skip crawl Ä‘á»ƒ trÃ¡nh lÃ m tá»‡ hÆ¡n")
            logger.warning("=" * 70)
            result["error"] = "Service Ä‘ang á»Ÿ tráº¡ng thÃ¡i FAILED, skip crawl"
            result["status"] = "degraded"
            return result

        # Validate URL
        if not product_url or not product_url.startswith("http"):
            raise ValueError(f"URL khÃ´ng há»£p lá»‡: {product_url}")

        # Láº¥y cáº¥u hÃ¬nh
        rate_limit_delay = float(
            Variable.get("TIKI_DETAIL_RATE_LIMIT_DELAY", default_var="1.5")
        )  # Delay 1.5s cho detail (tá»‘i Æ°u tá»« 2.0s)
        timeout = int(
            Variable.get("TIKI_DETAIL_CRAWL_TIMEOUT", default_var="180")
        )  # 3 phÃºt má»—i product (tÄƒng tá»« 120s Ä‘á»ƒ trÃ¡nh timeout)

        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl vá»›i timeout vÃ  circuit breaker
        start_time = time.time()

        # Sá»­ dá»¥ng Selenium Ä‘á»ƒ crawl detail (cáº§n thiáº¿t cho dynamic content)
        html_content = None
        try:
            # Wrapper function Ä‘á»ƒ gá»i vá»›i circuit breaker
            def _crawl_detail_with_params():
                """Wrapper function Ä‘á»ƒ gá»i vá»›i circuit breaker"""
                return crawl_product_detail_with_selenium(
                    product_url,
                    save_html=False,
                    verbose=False,  # KhÃ´ng verbose trong Airflow
                    max_retries=2,  # TEST MODE: Giáº£m retry xuá»‘ng 2,  # Retry 3 láº§n (tÄƒng tá»« 2)
                    timeout=60,  # Timeout 60s (tÄƒng tá»« 25s Ä‘á»ƒ Ä‘á»§ thá»i gian cho Selenium)
                    use_redis_cache=True,  # Sá»­ dá»¥ng Redis cache
                    use_rate_limiting=True,  # Sá»­ dá»¥ng rate limiting
                )

            try:
                # Gá»i vá»›i circuit breaker
                html_content = tiki_circuit_breaker.call(_crawl_detail_with_params)
                tiki_degradation.record_success()
            except CircuitBreakerOpenError as e:
                # Circuit breaker Ä‘ang má»Ÿ
                result["error"] = f"Circuit breaker open: {str(e)}"
                result["status"] = "circuit_breaker_open"
                logger.warning(f"âš ï¸  Circuit breaker open cho product {product_id}: {e}")
                # ThÃªm vÃ o DLQ
                try:
                    crawl_error = classify_error(
                        e, context={"product_url": product_url, "product_id": product_id}
                    )
                    tiki_dlq.add(
                        task_id=f"crawl_detail_{product_id}",
                        task_type="crawl_product_detail",
                        error=crawl_error,
                        context={
                            "product_url": product_url,
                            "product_name": product_name,
                            "product_id": product_id,
                        },
                        retry_count=0,
                    )
                    logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
                except Exception as dlq_error:
                    logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
                return result
            except Exception:
                # Ghi nháº­n failure
                tiki_degradation.record_failure()
                raise  # Re-raise Ä‘á»ƒ xá»­ lÃ½ bÃªn dÆ°á»›i

            if not html_content or len(html_content) < 100:
                raise ValueError(
                    f"HTML content quÃ¡ ngáº¯n hoáº·c rá»—ng: {len(html_content) if html_content else 0} kÃ½ tá»±"
                )

        except Exception as selenium_error:
            # Log lá»—i Selenium chi tiáº¿t
            error_type = type(selenium_error).__name__
            error_msg = str(selenium_error)

            # RÃºt gá»n error message náº¿u quÃ¡ dÃ i
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."

            logger.error(f"âŒ Lá»—i Selenium ({error_type}): {error_msg}")

            # Kiá»ƒm tra cÃ¡c lá»—i phá»• biáº¿n vÃ  phÃ¢n loáº¡i
            error_msg_lower = error_msg.lower()
            if (
                "chrome" in error_msg_lower
                or "driver" in error_msg_lower
                or "webdriver" in error_msg_lower
            ):
                result["error"] = f"Chrome/Driver error: {error_msg}"
                result["status"] = "selenium_error"
            elif (
                "timeout" in error_msg_lower
                or "timed out" in error_msg_lower
                or "time-out" in error_msg_lower
            ):
                result["error"] = f"Timeout: {error_msg}"
                result["status"] = "timeout"
            elif (
                "connection" in error_msg_lower
                or "network" in error_msg_lower
                or "refused" in error_msg_lower
            ):
                result["error"] = f"Network error: {error_msg}"
                result["status"] = "network_error"
            elif "memory" in error_msg_lower or "out of memory" in error_msg_lower:
                result["error"] = f"Memory error: {error_msg}"
                result["status"] = "memory_error"
            else:
                result["error"] = f"Selenium error: {error_msg}"
                result["status"] = "failed"

            # Ghi nháº­n failure vÃ  thÃªm vÃ o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    selenium_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
            # KhÃ´ng raise, return result vá»›i status failed
            return result

        # Extract detail
        try:
            detail = extract_product_detail(html_content, product_url, verbose=False)

            if not detail:
                raise ValueError("KhÃ´ng extract Ä‘Æ°á»£c detail tá»« HTML")

        except Exception as extract_error:
            error_type = type(extract_error).__name__
            error_msg = str(extract_error)
            logger.error(f"âŒ Lá»—i khi extract detail ({error_type}): {error_msg}")
            result["error"] = f"Extract error: {error_msg}"
            result["status"] = "extract_error"
            # Ghi nháº­n failure vÃ  thÃªm vÃ o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    extract_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
            return result

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(
                f"Crawl detail vÆ°á»£t quÃ¡ timeout {timeout}s (elapsed: {elapsed:.1f}s)"
            )

        result["detail"] = detail
        result["status"] = "success"
        result["elapsed_time"] = elapsed

        # LÆ°u vÃ o cache - Æ°u tiÃªn Redis, fallback vá» file
        # Redis cache (nhanh, distributed)
        if redis_cache:
            try:
                redis_cache.cache_product_detail(product_id, detail, ttl=604800)  # 7 ngÃ y
                logger.info(f"[Redis Cache] âœ… ÄÃ£ cache detail cho product {product_id}")
            except Exception as e:
                logger.warning(f"[Redis Cache] âš ï¸  Lá»—i khi cache vÃ o Redis: {e}")

        # File cache (fallback)
        try:
            # Äáº£m báº£o thÆ° má»¥c cache tá»“n táº¡i
            DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

            temp_file = cache_file.with_suffix(".tmp")
            logger.debug(f"ğŸ’¾ Äang lÆ°u cache vÃ o: {cache_file}")

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)

            # Atomic move
            if os.name == "nt":  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))

            # Verify cache file was created
            if cache_file.exists():
                logger.info(f"âœ… Crawl thÃ nh cÃ´ng: {elapsed:.1f}s, Ä‘Ã£ cache vÃ o {cache_file}")
                # Log sales_count náº¿u cÃ³
                if detail.get("sales_count") is not None:
                    logger.info(f"   ğŸ“Š sales_count: {detail.get('sales_count')}")
                else:
                    logger.warning("   âš ï¸  sales_count: None (khÃ´ng tÃ¬m tháº¥y)")
            else:
                logger.error(f"âŒ Cache file khÃ´ng Ä‘Æ°á»£c táº¡o: {cache_file}")
        except Exception as e:
            logger.error(f"âŒ KhÃ´ng lÆ°u Ä‘Æ°á»£c cache: {e}", exc_info=True)
            # KhÃ´ng fail task vÃ¬ Ä‘Ã£ crawl thÃ nh cÃ´ng, chá»‰ khÃ´ng lÆ°u Ä‘Æ°á»£c cache

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"â±ï¸  Timeout: {e}")
        # ThÃªm vÃ o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")

    except ValueError as e:
        result["error"] = str(e)
        result["status"] = "validation_error"
        tiki_degradation.record_failure()
        logger.error(f"âŒ Validation error: {e}")
        # ThÃªm vÃ o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        error_type = type(e).__name__
        logger.error(f"âŒ Lá»—i khi crawl detail ({error_type}): {e}", exc_info=True)
        # ThÃªm vÃ o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"ğŸ“¬ ÄÃ£ thÃªm vÃ o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm vÃ o DLQ: {dlq_error}")
        # KhÃ´ng raise Ä‘á»ƒ tiáº¿p tá»¥c vá»›i product khÃ¡c

    # Äáº£m báº£o luÃ´n return result, khÃ´ng bao giá» raise exception
    # Kiá»ƒm tra result cÃ³ há»£p lá»‡ khÃ´ng trÆ°á»›c khi return
    if not result or not isinstance(result, dict):
        logger.warning(f"âš ï¸  Result khÃ´ng há»£p lá»‡, sá»­ dá»¥ng default_result")
        result = default_result.copy()
        result["error"] = "Result khÃ´ng há»£p lá»‡"
        result["status"] = "failed"
    
    # Äáº£m báº£o result cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c field cáº§n thiáº¿t
    if "product_id" not in result:
        result["product_id"] = product_id if "product_id" in locals() else "unknown"
    if "url" not in result:
        result["url"] = product_url if "product_url" in locals() else ""
    if "status" not in result:
        result["status"] = "failed"
    if "crawled_at" not in result:
        result["crawled_at"] = datetime.now().isoformat()
    
    try:
        return result
    except Exception as e:
        # Náº¿u cÃ³ lá»—i khi return (khÃ´ng thá»ƒ xáº£y ra nhÆ°ng Ä‘á»ƒ an toÃ n)
        logger.error(f"âŒ Lá»—i khi return result: {e}", exc_info=True)
        default_result["error"] = f"Lá»—i khi return result: {str(e)}"
        default_result["product_id"] = product_id if "product_id" in locals() else "unknown"
        default_result["url"] = product_url if "product_url" in locals() else ""
        return default_result


def merge_product_details(**context) -> dict[str, Any]:
    """
    Task: Merge product details vÃ o products list

    Returns:
        Dict: Products vá»›i detail Ä‘Ã£ merge
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ”„ TASK: Merge Product Details")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # Láº¥y products gá»‘c
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Thá»­ láº¥y tá»« file
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y products tá»« XCom hoáº·c file")

        products = merge_result.get("products", [])
        logger.info(f"Tá»•ng sá»‘ products: {len(products)}")

        # Láº¥y sá»‘ lÆ°á»£ng products thá»±c táº¿ Ä‘Æ°á»£c crawl tá»« prepare_products_for_detail
        # ÄÃ¢y lÃ  sá»‘ lÆ°á»£ng map_index thá»±c táº¿, khÃ´ng pháº£i tá»•ng sá»‘ products
        products_to_crawl = None
        try:
            products_to_crawl = ti.xcom_pull(
                task_ids="crawl_product_details.prepare_products_for_detail"
            )
        except Exception:
            try:
                products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
            except Exception:
                pass

        # Sá»‘ lÆ°á»£ng products thá»±c táº¿ Ä‘Æ°á»£c crawl
        expected_products_count = len(products_to_crawl) if products_to_crawl else 0
        # Vá»›i batch processing, sá»‘ map_index = sá»‘ batches, khÃ´ng pháº£i sá»‘ products
        batch_size = 10
        expected_crawl_count = (expected_products_count + batch_size - 1) // batch_size if expected_products_count > 0 else 0
        logger.info(f"ğŸ“Š Sá»‘ products: {expected_products_count}, Sá»‘ batches dá»± kiáº¿n: {expected_crawl_count}")

        # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ lÆ°á»£ng map_index thá»±c táº¿ cÃ³ sáºµn báº±ng cÃ¡ch thá»­ láº¥y XCom
        # Äiá»u nÃ y giÃºp xá»­ lÃ½ trÆ°á»ng há»£p má»™t sá»‘ tasks Ä‘Ã£ fail hoáº·c chÆ°a cháº¡y xong
        actual_crawl_count = expected_crawl_count
        if expected_crawl_count > 0:
            # Thá»­ láº¥y XCom tá»« map_index cuá»‘i cÃ¹ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng thá»±c táº¿
            # TÃ¬m map_index cao nháº¥t cÃ³ XCom
            task_id = "crawl_product_details.crawl_product_detail"
            max_found_index = -1

            # Binary search Ä‘á»ƒ tÃ¬m map_index cao nháº¥t cÃ³ XCom (tá»‘i Æ°u hÆ¡n linear search)
            # Thá»­ má»™t sá»‘ Ä‘iá»ƒm Ä‘á»ƒ tÃ¬m max index
            logger.info(
                f"ğŸ” Äang phÃ¡t hiá»‡n sá»‘ lÆ°á»£ng map_index thá»±c táº¿ (dá»± kiáº¿n: {expected_crawl_count})..."
            )
            test_indices = []
            if expected_crawl_count > 1000:
                # Vá»›i sá»‘ lÆ°á»£ng lá»›n, test má»™t sá»‘ Ä‘iá»ƒm Ä‘á»ƒ tÃ¬m max
                step = max(100, expected_crawl_count // 20)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            elif expected_crawl_count > 100:
                # Vá»›i sá»‘ lÆ°á»£ng trung bÃ¬nh, test nhiá»u Ä‘iá»ƒm hÆ¡n
                step = max(50, expected_crawl_count // 10)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            else:
                # Vá»›i sá»‘ lÆ°á»£ng nhá», test táº¥t cáº£
                test_indices = list(range(expected_crawl_count))

            # TÃ¬m tá»« cuá»‘i vá» Ä‘áº§u Ä‘á»ƒ tÃ¬m max index nhanh hÆ¡n
            for test_idx in reversed(test_indices):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[test_idx]
                    )
                    if result:
                        max_found_index = test_idx
                        logger.info(f"âœ… TÃ¬m tháº¥y XCom táº¡i map_index {test_idx}")
                        break
                except Exception as e:
                    logger.debug(f"   KhÃ´ng cÃ³ XCom táº¡i map_index {test_idx}: {e}")
                    pass

            if max_found_index >= 0:
                # TÃ¬m chÃ­nh xÃ¡c map_index cao nháº¥t báº±ng cÃ¡ch tÃ¬m tá»« max_found_index
                # Chá»‰ thá»­ thÃªm tá»‘i Ä‘a 200 map_index tiáº¿p theo Ä‘á»ƒ trÃ¡nh quÃ¡ lÃ¢u
                logger.info(f"ğŸ” Äang tÃ¬m chÃ­nh xÃ¡c max index tá»« {max_found_index}...")
                search_range = min(max_found_index + 200, expected_crawl_count)
                for idx in range(max_found_index + 1, search_range):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[idx]
                        )
                        if result:
                            max_found_index = idx
                        else:
                            # Náº¿u khÃ´ng cÃ³ result, dá»«ng láº¡i (cÃ³ thá»ƒ Ä‘Ã£ Ä‘áº¿n cuá»‘i)
                            break
                    except Exception as e:
                        # Náº¿u exception, cÃ³ thá»ƒ lÃ  háº¿t map_index
                        logger.debug(f"   KhÃ´ng cÃ³ XCom táº¡i map_index {idx}: {e}")
                        break

                actual_crawl_count = max_found_index + 1
                logger.info(
                    f"âœ… PhÃ¡t hiá»‡n {actual_crawl_count} map_index thá»±c táº¿ cÃ³ XCom (dá»± kiáº¿n: {expected_crawl_count})"
                )
            else:
                logger.warning(
                    f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y XCom nÃ o, sá»­ dá»¥ng expected_crawl_count: {expected_crawl_count}. "
                    f"CÃ³ thá»ƒ táº¥t cáº£ tasks Ä‘Ã£ fail hoáº·c chÆ°a cháº¡y xong."
                )
                actual_crawl_count = expected_crawl_count

        if actual_crawl_count == 0:
            logger.warning("=" * 70)
            logger.warning("âš ï¸  KHÃ”NG CÃ“ PRODUCTS NÃ€O ÄÆ¯á»¢C CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("ğŸ’¡ NguyÃªn nhÃ¢n cÃ³ thá»ƒ:")
            logger.warning("   - Táº¥t cáº£ products Ä‘Ã£ cÃ³ trong database vá»›i detail Ä‘áº§y Ä‘á»§")
            logger.warning("   - Táº¥t cáº£ products Ä‘Ã£ cÃ³ trong cache vá»›i detail Ä‘áº§y Ä‘á»§")
            logger.warning("   - Táº¥t cáº£ products Ä‘Ã£ Ä‘Æ°á»£c crawl trÆ°á»›c Ä‘Ã³ (tá»« progress file)")
            logger.warning("   - KhÃ´ng cÃ³ products nÃ o Ä‘Æ°á»£c prepare Ä‘á»ƒ crawl")
            logger.warning("=" * 70)
            logger.warning("ğŸ’¡ Äá»ƒ force crawl láº¡i, kiá»ƒm tra task 'prepare_products_for_detail' log")
            logger.warning("=" * 70)
            # Tráº£ vá» products gá»‘c khÃ´ng cÃ³ detail
            return {
                "products": products,
                "stats": {
                    "total_products": len(products),
                    "with_detail": 0,
                    "cached": 0,
                    "failed": 0,
                    "timeout": 0,
                    "crawled_count": 0,
                },
                "merged_at": datetime.now().isoformat(),
            }

        # Láº¥y detail results tá»« Dynamic Task Mapping
        task_id = "crawl_product_details.crawl_product_detail"
        all_detail_results = []

        # Láº¥y táº¥t cáº£ results báº±ng cÃ¡ch láº¥y tá»«ng map_index Ä‘á»ƒ trÃ¡nh giá»›i háº¡n XCom
        # CHá»ˆ láº¥y tá»« map_index 0 Ä‘áº¿n actual_crawl_count - 1 (khÃ´ng pháº£i len(products))
        logger.info(f"Báº¯t Ä‘áº§u láº¥y detail results tá»« {actual_crawl_count} crawled products...")

        # Láº¥y theo batch Ä‘á»ƒ tá»‘i Æ°u
        batch_size = 100
        total_batches = (actual_crawl_count + batch_size - 1) // batch_size
        logger.info(
            f"ğŸ“¦ Sáº½ láº¥y {actual_crawl_count} results trong {total_batches} batches (má»—i batch {batch_size})"
        )

        for batch_num, start_idx in enumerate(range(0, actual_crawl_count, batch_size), 1):
            end_idx = min(start_idx + batch_size, actual_crawl_count)
            batch_map_indexes = list(range(start_idx, end_idx))

            # Heartbeat: log má»—i batch Ä‘á»ƒ Airflow biáº¿t task váº«n Ä‘ang cháº¡y
            if batch_num % 5 == 0 or batch_num == 1:
                logger.info(
                    f"ğŸ’“ [Heartbeat] Äang xá»­ lÃ½ batch {batch_num}/{total_batches} (index {start_idx}-{end_idx-1})..."
                )

            try:
                batch_results = ti.xcom_pull(
                    task_ids=task_id, key="return_value", map_indexes=batch_map_indexes
                )

                if batch_results:
                    if isinstance(batch_results, list):
                        # List results theo thá»© tá»± map_indexes
                        # Má»—i result cÃ³ thá»ƒ lÃ  list (tá»« batch) hoáº·c dict (tá»« single)
                        for result in batch_results:
                            if result:
                                if isinstance(result, list):
                                    # Batch result: flatten list of results
                                    all_detail_results.extend([r for r in result if r])
                                elif isinstance(result, dict):
                                    # Single result
                                    all_detail_results.append(result)
                    elif isinstance(batch_results, dict):
                        # Dict vá»›i key lÃ  map_index hoáº·c string
                        # Láº¥y táº¥t cáº£ values, sáº¯p xáº¿p theo map_index náº¿u cÃ³ thá»ƒ
                        for value in batch_results.values():
                            if value:
                                if isinstance(value, list):
                                    # Batch result: flatten
                                    all_detail_results.extend([r for r in value if r])
                                elif isinstance(value, dict):
                                    # Single result
                                    all_detail_results.append(value)
                    else:
                        # Single result
                        if isinstance(batch_results, list):
                            # Batch result: flatten
                            all_detail_results.extend([r for r in batch_results if r])
                        else:
                            all_detail_results.append(batch_results)

                # Log progress má»—i 5 batches hoáº·c má»—i 10% progress
                if batch_num % max(5, total_batches // 10) == 0:
                    progress_pct = (
                        (len(all_detail_results) / actual_crawl_count * 100)
                        if actual_crawl_count > 0
                        else 0
                    )
                    logger.info(
                        f"ğŸ“Š ÄÃ£ láº¥y {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%)..."
                    )
            except Exception as e:
                logger.warning(f"âš ï¸  Lá»—i khi láº¥y batch {start_idx}-{end_idx}: {e}")
                logger.warning("   Sáº½ thá»­ láº¥y tá»«ng map_index riÃªng láº» trong batch nÃ y...")
                # Thá»­ láº¥y tá»«ng map_index riÃªng láº» trong batch nÃ y
                for map_index in batch_map_indexes:
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )
                        if result:
                            if isinstance(result, list):
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                    except Exception as e2:
                        # Bá» qua náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c (cÃ³ thá»ƒ task chÆ°a cháº¡y xong hoáº·c failed)
                        logger.debug(f"   KhÃ´ng láº¥y Ä‘Æ°á»£c map_index {map_index}: {e2}")
                        pass

        logger.info(
            f"âœ… Láº¥y Ä‘Æ°á»£c {len(all_detail_results)} detail results qua batch (mong Ä‘á»£i {actual_crawl_count})"
        )

        # Náº¿u khÃ´ng láº¥y Ä‘á»§ hoáº·c cÃ³ lá»—i khi láº¥y batch, thá»­ láº¥y tá»«ng map_index má»™t Ä‘á»ƒ bÃ¹ vÃ o pháº§n thiáº¿u
        # KHÃ”NG reset all_detail_results, chá»‰ láº¥y thÃªm nhá»¯ng map_index chÆ°a cÃ³
        if len(all_detail_results) < actual_crawl_count * 0.8:  # Náº¿u thiáº¿u hÆ¡n 20%
            # Log cáº£nh bÃ¡o náº¿u thiáº¿u nhiá»u
            missing_pct = (
                ((actual_crawl_count - len(all_detail_results)) / actual_crawl_count * 100)
                if actual_crawl_count > 0
                else 0
            )
            if missing_pct > 30:
                logger.warning(
                    f"âš ï¸  Thiáº¿u {missing_pct:.1f}% results ({actual_crawl_count - len(all_detail_results)}/{actual_crawl_count}), "
                    f"cÃ³ thá»ƒ do nhiá»u tasks failed hoáº·c timeout"
                )
            logger.warning(
                f"âš ï¸  Chá»‰ láº¥y Ä‘Æ°á»£c {len(all_detail_results)}/{actual_crawl_count} results qua batch, "
                f"thá»­ láº¥y tá»«ng map_index Ä‘á»ƒ bÃ¹ vÃ o pháº§n thiáº¿u..."
            )

            # Táº¡o set cÃ¡c product_id Ä‘Ã£ cÃ³ Ä‘á»ƒ trÃ¡nh duplicate
            existing_product_ids = set()
            for result in all_detail_results:
                if isinstance(result, dict) and result.get("product_id"):
                    existing_product_ids.add(result.get("product_id"))

            missing_count = actual_crawl_count - len(all_detail_results)
            logger.info(
                f"ğŸ“Š Cáº§n láº¥y thÃªm ~{missing_count} results tá»« {actual_crawl_count} map_indexes"
            )

            # Heartbeat: log thÆ°á»ng xuyÃªn trong vÃ²ng láº·p dÃ i
            fetched_count = 0
            for map_index in range(actual_crawl_count):  # CHá»ˆ láº¥y tá»« 0 Ä‘áº¿n actual_crawl_count - 1
                # Heartbeat má»—i 100 items Ä‘á»ƒ trÃ¡nh timeout
                if map_index % 100 == 0 and map_index > 0:
                    logger.info(
                        f"ğŸ’“ [Heartbeat] Äang láº¥y tá»«ng map_index: {map_index}/{actual_crawl_count} "
                        f"(Ä‘Ã£ láº¥y {len(all_detail_results)}/{actual_crawl_count})..."
                    )

                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[map_index]
                    )
                    if result:
                        # Chá»‰ thÃªm náº¿u chÆ°a cÃ³ (trÃ¡nh duplicate)
                        product_id_to_check = None
                        if isinstance(result, dict):
                            product_id_to_check = result.get("product_id")
                        elif (
                            isinstance(result, list)
                            and len(result) > 0
                            and isinstance(result[0], dict)
                        ):
                            product_id_to_check = result[0].get("product_id")

                        # Chá»‰ thÃªm náº¿u product_id chÆ°a cÃ³ trong danh sÃ¡ch
                        if (
                            not product_id_to_check
                            or product_id_to_check not in existing_product_ids
                        ):
                            if isinstance(result, list):
                                for r in result:
                                    if isinstance(r, dict) and r.get("product_id"):
                                        existing_product_ids.add(r.get("product_id"))
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                if product_id_to_check:
                                    existing_product_ids.add(product_id_to_check)
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                            fetched_count += 1

                    # Log progress má»—i 200 items
                    if (map_index + 1) % 200 == 0:
                        progress_pct = (
                            (len(all_detail_results) / actual_crawl_count * 100)
                            if actual_crawl_count > 0
                            else 0
                        )
                        logger.info(
                            f"ğŸ“Š ÄÃ£ láº¥y tá»•ng {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%) tá»«ng map_index..."
                        )
                except Exception as e:
                    # Bá» qua náº¿u khÃ´ng láº¥y Ä‘Æ°á»£c (cÃ³ thá»ƒ task chÆ°a cháº¡y xong hoáº·c failed)
                    logger.debug(f"   KhÃ´ng láº¥y Ä‘Æ°á»£c map_index {map_index}: {e}")
                    pass

            logger.info(
                f"âœ… Sau khi láº¥y tá»«ng map_index: tá»•ng {len(all_detail_results)} detail results (láº¥y thÃªm {fetched_count})"
            )

        # Táº¡o dict Ä‘á»ƒ lookup nhanh
        detail_dict = {}
        stats = {
            "total_products": len(products),
            "crawled_count": 0,  # Sá»‘ lÆ°á»£ng products thá»±c sá»± Ä‘Æ°á»£c crawl detail
            "with_detail": 0,
            "cached": 0,
            "failed": 0,
            "timeout": 0,
            "degraded": 0,
            "circuit_breaker_open": 0,
        }

        logger.info(f"ğŸ“Š Äang xá»­ lÃ½ {len(all_detail_results)} detail results...")

        # Kiá»ƒm tra náº¿u cÃ³ quÃ¡ nhiá»u káº¿t quáº£ None hoáº·c invalid
        valid_results = 0
        error_details = {}  # Thá»‘ng kÃª chi tiáº¿t cÃ¡c loáº¡i lá»—i
        failed_products = []  # Danh sÃ¡ch products bá»‹ fail Ä‘á»ƒ phÃ¢n tÃ­ch

        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get("product_id")
                if product_id:
                    detail_dict[product_id] = detail_result
                    valid_results += 1
                    status = detail_result.get("status", "failed")
                    error = detail_result.get("error")

                    # Äáº¿m sá»‘ lÆ°á»£ng products Ä‘Æ°á»£c crawl (táº¥t cáº£ cÃ¡c status trá»« "not_crawled")
                    if status in ["success", "cached", "failed", "timeout", "degraded", "circuit_breaker_open", "selenium_error", "network_error", "extract_error", "validation_error", "memory_error"]:
                        stats["crawled_count"] += 1
                    
                    if status == "success":
                        stats["with_detail"] += 1
                    elif status == "cached":
                        stats["cached"] += 1
                    elif status == "timeout":
                        stats["timeout"] += 1
                        error_details["timeout"] = error_details.get("timeout", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "degraded":
                        stats["degraded"] += 1
                        error_details["degraded"] = error_details.get("degraded", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "circuit_breaker_open":
                        stats["circuit_breaker_open"] += 1
                        error_details["circuit_breaker_open"] = (
                            error_details.get("circuit_breaker_open", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "selenium_error":
                        stats["failed"] += 1
                        error_details["selenium_error"] = error_details.get("selenium_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "extract_error":
                        stats["failed"] += 1
                        error_details["extract_error"] = error_details.get("extract_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "network_error":
                        stats["failed"] += 1
                        error_details["network_error"] = error_details.get("network_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "memory_error":
                        stats["failed"] += 1
                        error_details["memory_error"] = error_details.get("memory_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "validation_error":
                        stats["failed"] += 1
                        error_details["validation_error"] = (
                            error_details.get("validation_error", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    else:
                        stats["failed"] += 1
                        error_type = status if status else "unknown"
                        error_details[error_type] = error_details.get(error_type, 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )

        logger.info(
            f"ğŸ“Š CÃ³ {valid_results} detail results há»£p lá»‡ tá»« {len(all_detail_results)} results"
        )

        if valid_results < len(all_detail_results):
            logger.warning(
                f"âš ï¸  CÃ³ {len(all_detail_results) - valid_results} results khÃ´ng há»£p lá»‡ hoáº·c thiáº¿u product_id"
            )

        # Log chi tiáº¿t vá» cÃ¡c lá»—i
        if error_details:
            logger.info("=" * 70)
            logger.info("ğŸ“‹ PHÃ‚N TÃCH CÃC LOáº I Lá»–I")
            logger.info("=" * 70)
            for error_type, count in sorted(
                error_details.items(), key=lambda x: x[1], reverse=True
            ):
                logger.info(f"  âŒ {error_type}: {count} products")
            logger.info("=" * 70)

            # Log má»™t sá»‘ products bá»‹ fail Ä‘áº§u tiÃªn Ä‘á»ƒ phÃ¢n tÃ­ch
            if failed_products:
                logger.info(f"ğŸ“ Máº«u {min(10, len(failed_products))} products bá»‹ fail Ä‘áº§u tiÃªn:")
                for i, failed in enumerate(failed_products[:10], 1):
                    logger.info(
                        f"  {i}. Product ID: {failed['product_id']}, Status: {failed['status']}, Error: {failed.get('error', 'N/A')[:100]}"
                    )

        # LÆ°u thÃ´ng tin lá»—i vÃ o stats Ä‘á»ƒ phÃ¢n tÃ­ch sau
        stats["error_details"] = error_details
        stats["failed_products_count"] = len(failed_products)

        # Merge detail vÃ o products
        # CHá»ˆ lÆ°u products cÃ³ detail VÃ€ status == "success" (khÃ´ng lÆ°u cached hoáº·c failed)
        products_with_detail = []
        products_without_detail = 0
        products_cached = 0
        products_failed = 0

        for product in products:
            product_id = product.get("product_id")
            detail_result = detail_dict.get(product_id)

            if detail_result and detail_result.get("detail"):
                status = detail_result.get("status", "failed")

                # CHá»ˆ lÆ°u products cÃ³ status == "success" (Ä‘Ã£ crawl thÃ nh cÃ´ng, khÃ´ng pháº£i tá»« cache)
                if status == "success":
                    # Merge detail vÃ o product
                    detail = detail_result["detail"]
                    
                    # Kiá»ƒm tra náº¿u detail lÃ  None hoáº·c rá»—ng
                    if detail is None:
                        logger.warning(f"âš ï¸  Detail lÃ  None cho product {product_id}")
                        products_failed += 1
                        continue
                    
                    # Kiá»ƒm tra náº¿u detail lÃ  string (JSON), parse nÃ³
                    if isinstance(detail, str):
                        # Bá» qua string rá»—ng
                        if not detail.strip():
                            logger.warning(f"âš ï¸  Detail lÃ  string rá»—ng cho product {product_id}")
                            products_failed += 1
                            continue
                        
                        try:
                            import json
                            detail = json.loads(detail)
                        except (json.JSONDecodeError, TypeError) as e:
                            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ parse detail JSON cho product {product_id}: {e}, detail type: {type(detail)}, detail value: {str(detail)[:100]}")
                            products_failed += 1
                            continue
                    
                    # Kiá»ƒm tra náº¿u detail khÃ´ng pháº£i lÃ  dict
                    if not isinstance(detail, dict):
                        logger.warning(f"âš ï¸  Detail khÃ´ng pháº£i lÃ  dict cho product {product_id}: {type(detail)}, value: {str(detail)[:100]}")
                        products_failed += 1
                        continue
                    
                    product_with_detail = {**product}

                    # Update cÃ¡c trÆ°á»ng tá»« detail
                    if detail.get("price"):
                        product_with_detail["price"] = detail["price"]
                    if detail.get("rating"):
                        product_with_detail["rating"] = detail["rating"]
                    if detail.get("description"):
                        product_with_detail["description"] = detail["description"]
                    if detail.get("specifications"):
                        product_with_detail["specifications"] = detail["specifications"]
                    if detail.get("images"):
                        product_with_detail["images"] = detail["images"]
                    if detail.get("brand"):
                        product_with_detail["brand"] = detail["brand"]
                    if detail.get("seller"):
                        product_with_detail["seller"] = detail["seller"]
                    if detail.get("stock"):
                        product_with_detail["stock"] = detail["stock"]
                    if detail.get("shipping"):
                        product_with_detail["shipping"] = detail["shipping"]
                    # Cáº­p nháº­t sales_count: Æ°u tiÃªn tá»« detail, náº¿u khÃ´ng cÃ³ thÃ¬ dÃ¹ng tá»« product gá»‘c
                    # Chá»‰ cáº§n cÃ³ trong má»™t trong hai lÃ  Ä‘á»§
                    if detail.get("sales_count") is not None:
                        product_with_detail["sales_count"] = detail["sales_count"]
                    elif product.get("sales_count") is not None:
                        product_with_detail["sales_count"] = product["sales_count"]
                    # Náº¿u cáº£ hai Ä‘á»u khÃ´ng cÃ³, giá»¯ None (Ä‘Ã£ cÃ³ trong product gá»‘c)

                    # ThÃªm metadata
                    product_with_detail["detail_crawled_at"] = detail_result.get("crawled_at")
                    product_with_detail["detail_status"] = status

                    products_with_detail.append(product_with_detail)
                elif status == "cached":
                    # KhÃ´ng lÆ°u products tá»« cache (chá»‰ lÆ°u products Ä‘Ã£ crawl má»›i)
                    products_cached += 1
                else:
                    # KhÃ´ng lÆ°u products bá»‹ fail
                    products_failed += 1
            else:
                # KhÃ´ng lÆ°u products khÃ´ng cÃ³ detail
                products_without_detail += 1

        logger.info("=" * 70)
        logger.info("ğŸ“Š THá»NG KÃŠ MERGE DETAIL")
        logger.info("=" * 70)
        logger.info(f"ğŸ“¦ Tá»•ng products ban Ä‘áº§u: {stats['total_products']}")
        logger.info(f"ğŸ”„ Products Ä‘Æ°á»£c crawl detail: {stats['crawled_count']}")
        logger.info(f"âœ… CÃ³ detail (success): {stats['with_detail']}")
        logger.info(f"ğŸ“¦ CÃ³ detail (cached): {stats['cached']}")
        logger.info(f"âš ï¸  Degraded: {stats['degraded']}")
        logger.info(f"âš¡ Circuit breaker open: {stats['circuit_breaker_open']}")
        logger.info(f"âŒ Failed: {stats['failed']}")
        logger.info(f"â±ï¸  Timeout: {stats['timeout']}")

        # TÃ­nh tá»•ng cÃ³ detail (success + cached)
        total_with_detail = stats["with_detail"] + stats["cached"]
        
        # Tá»· lá»‡ thÃ nh cÃ´ng dá»±a trÃªn sá»‘ lÆ°á»£ng Ä‘Æ°á»£c crawl (quan trá»ng hÆ¡n)
        if stats["crawled_count"] > 0:
            success_rate = (stats["with_detail"] / stats["crawled_count"]) * 100
            logger.info(
                f"ğŸ“ˆ Tá»· lá»‡ thÃ nh cÃ´ng (dá»±a trÃªn crawled): {stats['with_detail']}/{stats['crawled_count']} ({success_rate:.1f}%)"
            )
        
        # Tá»· lá»‡ cÃ³ detail trong tá»•ng products (Ä‘á»ƒ tham kháº£o)
        if stats["total_products"] > 0:
            detail_coverage = total_with_detail / stats["total_products"] * 100
            logger.info(
                f"ğŸ“Š Tá»· lá»‡ cÃ³ detail (trong tá»•ng products): {total_with_detail}/{stats['total_products']} ({detail_coverage:.1f}%)"
            )

        logger.info("=" * 70)
        logger.info(
            f"ğŸ’¾ Products Ä‘Æ°á»£c lÆ°u vÃ o file: {len(products_with_detail)} (chá»‰ lÆ°u products cÃ³ status='success')"
        )
        logger.info(f"ğŸ“¦ Products tá»« cache (Ä‘Ã£ bá» qua): {products_cached}")
        logger.info(f"âŒ Products bá»‹ fail (Ä‘Ã£ bá» qua): {products_failed}")
        logger.info(f"ğŸš« Products khÃ´ng cÃ³ detail (Ä‘Ã£ bá» qua): {products_without_detail}")
        logger.info("=" * 70)

        # Cáº­p nháº­t stats Ä‘á»ƒ pháº£n Ã¡nh sá»‘ lÆ°á»£ng products thá»±c táº¿ Ä‘Æ°á»£c lÆ°u
        stats["products_saved"] = len(products_with_detail)
        stats["products_skipped"] = products_without_detail
        stats["products_cached_skipped"] = products_cached
        stats["products_failed_skipped"] = products_failed

        result = {
            "products": products_with_detail,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
            "note": f"Chá»‰ lÆ°u {len(products_with_detail)} products cÃ³ status='success' (Ä‘Ã£ bá» qua {products_cached} cached, {products_failed} failed, {products_without_detail} khÃ´ng cÃ³ detail)",
        }

        return result

    except ValueError as e:
        logger.error(f"âŒ Validation error khi merge details: {e}", exc_info=True)
        # Náº¿u lÃ  validation error (thiáº¿u products), return empty result thay vÃ¬ raise
        return {
            "products": [],
            "stats": {
                "total_products": 0,
                "crawled_count": 0,  # Sá»‘ lÆ°á»£ng products Ä‘Æ°á»£c crawl detail
                "with_detail": 0,
                "cached": 0,
                "failed": 0,
                "timeout": 0,
            },
            "merged_at": datetime.now().isoformat(),
            "error": str(e),
        }
    except Exception as e:
        logger.error(f"âŒ Lá»—i khi merge details: {e}", exc_info=True)
        # Log chi tiáº¿t context Ä‘á»ƒ debug
        logger.error(f"   Context keys: {list(context.keys()) if context else 'None'}")
        try:
            ti = context.get("ti")
            if ti:
                logger.error(f"   Task ID: {ti.task_id}, DAG ID: {ti.dag_id}, Run ID: {ti.run_id}")
        except Exception:
            pass
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: LÆ°u products vá»›i detail vÃ o file

    Returns:
        str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ’¾ TASK: Save Products with Detail")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # Láº¥y káº¿t quáº£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="crawl_product_details.merge_product_details")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_product_details")
            except Exception:
                pass

        if not merge_result:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y merge result tá»« XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})
        note = merge_result.get("note", "Crawl tá»« Airflow DAG vá»›i product details")

        logger.info(f"ğŸ’¾ Äang lÆ°u {len(products)} products vá»›i detail...")
        
        # Log thÃ´ng tin vá» crawl detail
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"ğŸ”„ Products Ä‘Æ°á»£c crawl detail: {crawled_count}")
            logger.info(f"âœ… Products cÃ³ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"â±ï¸  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"âŒ Products failed: {stats.get('failed', 0)}")
        
        if stats.get("products_skipped"):
            logger.info(f"ğŸš« ÄÃ£ bá» qua {stats.get('products_skipped')} products khÃ´ng cÃ³ detail")

        # Chuáº©n bá»‹ dá»¯ liá»‡u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": note,
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"âœ… ÄÃ£ lÆ°u {len(products)} products vá»›i detail vÃ o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi save products with detail: {e}", exc_info=True)
        raise


def transform_products(**context) -> dict[str, Any]:
    """
    Task: Transform dá»¯ liá»‡u sáº£n pháº©m (normalize, validate, compute fields)

    Returns:
        Dict: Káº¿t quáº£ transform vá»›i transformed products vÃ  stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ”„ TASK: Transform Products")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # Láº¥y file tá»« save_products_with_detail
        output_file = None
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
        except Exception:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
            except Exception:
                pass

        if not output_file:
            output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file: {output_file}")

        logger.info(f"ğŸ“‚ Äang Ä‘á»c file: {output_file}")

        # Äá»c products tá»« file
        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})
        logger.info(f"ğŸ“Š Tá»•ng sá»‘ products trong file: {len(products)}")
        
        # TEST MODE: Giá»›i háº¡n sá»‘ lÆ°á»£ng products Ä‘á»ƒ test
        max_products = 10  # TEST MODE: Hardcode 10 products cho test
        if max_products > 0 and len(products) > max_products:
            logger.info(f"âš ï¸  TEST MODE: Giá»›i háº¡n tá»« {len(products)} xuá»‘ng {max_products} products")
            products = products[:max_products]
            logger.info(f"âœ… ÄÃ£ giá»›i háº¡n: {len(products)} products Ä‘á»ƒ transform")
        
        # Log thÃ´ng tin vá» crawl detail náº¿u cÃ³
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"ğŸ”„ Products Ä‘Æ°á»£c crawl detail: {crawled_count}")
            logger.info(f"âœ… Products cÃ³ detail (success): {stats.get('with_detail', 0)}")

        # Bá»• sung category_url vÃ  category_id trÆ°á»›c khi transform
        logger.info("ğŸ”— Äang bá»• sung category_url vÃ  category_id...")
        
        # BÆ°á»›c 1: Load category_url mapping tá»« products.json (náº¿u cÃ³)
        category_url_mapping = {}  # product_id -> category_url
        products_file = OUTPUT_DIR / "products.json"
        if products_file.exists():
            try:
                logger.info(f"ğŸ“– Äang Ä‘á»c category_url mapping tá»«: {products_file}")
                with open(products_file, encoding="utf-8") as f:
                    products_data = json.load(f)
                
                products_list = []
                if isinstance(products_data, list):
                    products_list = products_data
                elif isinstance(products_data, dict):
                    if "products" in products_data:
                        products_list = products_data["products"]
                    elif "data" in products_data and isinstance(products_data["data"], dict):
                        products_list = products_data["data"].get("products", [])
                
                for product in products_list:
                    product_id = product.get("product_id")
                    category_url = product.get("category_url")
                    if product_id and category_url:
                        category_url_mapping[product_id] = category_url
                
                logger.info(f"âœ… ÄÃ£ load {len(category_url_mapping)} category_url mappings tá»« products.json")
            except Exception as e:
                logger.warning(f"âš ï¸  Lá»—i khi Ä‘á»c products.json: {e}")
        
        # BÆ°á»›c 2: Import utility Ä‘á»ƒ extract category_id
        try:
            # TÃ¬m Ä‘Æ°á»ng dáº«n utils module
            utils_paths = [
                "/opt/airflow/src/pipelines/crawl/utils.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl", "utils.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "crawl", "utils.py"),
            ]
            
            utils_path = None
            for path in utils_paths:
                if os.path.exists(path):
                    utils_path = path
                    break
            
            if utils_path:
                import importlib.util
                spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                extract_category_id_from_url = utils_module.extract_category_id_from_url
            else:
                # Fallback: Ä‘á»‹nh nghÄ©a hÃ m Ä‘Æ¡n giáº£n
                import re
                def extract_category_id_from_url(url: str) -> str | None:
                    if not url:
                        return None
                    match = re.search(r"/c(\d+)", url)
                    if match:
                        return f"c{match.group(1)}"
                    return None
        except Exception as e:
            logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ import extract_category_id_from_url: {e}")
            import re
            def extract_category_id_from_url(url: str) -> str | None:
                if not url:
                    return None
                match = re.search(r"/c(\d+)", url)
                if match:
                    return f"c{match.group(1)}"
                return None
        
        # BÆ°á»›c 3: Bá»• sung category_url, category_id vÃ  Ä‘áº£m báº£o category_path cho products
        updated_count = 0
        category_id_added = 0
        category_path_count = 0
        
        for product in products:
            product_id = product.get("product_id")
            
            # Bá»• sung category_url náº¿u chÆ°a cÃ³
            if not product.get("category_url") and product_id in category_url_mapping:
                product["category_url"] = category_url_mapping[product_id]
                updated_count += 1
            
            # Extract category_id tá»« category_url náº¿u cÃ³
            category_url = product.get("category_url")
            if category_url and not product.get("category_id"):
                category_id = extract_category_id_from_url(category_url)
                if category_id:
                    product["category_id"] = category_id
                    category_id_added += 1
            
            # Äáº£m báº£o category_path Ä‘Æ°á»£c giá»¯ láº¡i (Ä‘Ã£ cÃ³ tá»« cache, khÃ´ng cáº§n xá»­ lÃ½)
            if product.get("category_path"):
                category_path_count += 1
        
        if updated_count > 0:
            logger.info(f"âœ… ÄÃ£ bá»• sung category_url cho {updated_count} products")
        if category_id_added > 0:
            logger.info(f"âœ… ÄÃ£ bá»• sung category_id cho {category_id_added} products")
        if category_path_count > 0:
            logger.info(f"âœ… CÃ³ {category_path_count} products cÃ³ category_path (breadcrumb)")
        
        # Import DataTransformer
        try:
            # TÃ¬m Ä‘Æ°á»ng dáº«n transform module
            transform_paths = [
                "/opt/airflow/src/pipelines/transform/transformer.py",
                os.path.abspath(
                    os.path.join(
                        dag_file_dir, "..", "..", "src", "pipelines", "transform", "transformer.py"
                    )
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "transform", "transformer.py"),
            ]

            transformer_path = None
            for path in transform_paths:
                if os.path.exists(path):
                    transformer_path = path
                    break

            if not transformer_path:
                raise ImportError("KhÃ´ng tÃ¬m tháº¥y transformer.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("transformer", transformer_path)
            transformer_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(transformer_module)
            DataTransformer = transformer_module.DataTransformer

            # Transform products
            transformer = DataTransformer(
                strict_validation=False, remove_invalid=True, normalize_fields=True
            )

            transformed_products, transform_stats = transformer.transform_products(
                products, validate=True
            )

            logger.info("=" * 70)
            logger.info("ğŸ“Š TRANSFORM RESULTS")
            logger.info("=" * 70)
            logger.info(f"âœ… Valid products: {transform_stats['valid_products']}")
            logger.info(f"âŒ Invalid products: {transform_stats['invalid_products']}")
            logger.info(f"ğŸ”„ Duplicates removed: {transform_stats['duplicates_removed']}")
            logger.info("=" * 70)

            # LÆ°u transformed products vÃ o file
            processed_dir = DATA_DIR / "processed"
            processed_dir.mkdir(parents=True, exist_ok=True)
            transformed_file = processed_dir / "products_transformed.json"

            output_data = {
                "transformed_at": datetime.now().isoformat(),
                "source_file": output_file,
                "total_products": len(products),
                "transform_stats": transform_stats,
                "products": transformed_products,
            }

            atomic_write_file(str(transformed_file), output_data, **context)
            logger.info(
                f"âœ… ÄÃ£ lÆ°u {len(transformed_products)} transformed products vÃ o: {transformed_file}"
            )

            return {
                "transformed_file": str(transformed_file),
                "transformed_count": len(transformed_products),
                "transform_stats": transform_stats,
            }

        except ImportError as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ import DataTransformer: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi transform products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"âŒ Lá»—i trong transform_products task: {e}", exc_info=True)
        raise


def _import_postgres_storage():
    """
    Helper function Ä‘á»ƒ import PostgresStorage vá»›i fallback logic
    Há»— trá»£ cáº£ mÃ´i trÆ°á»ng Airflow (importlib) vÃ  mÃ´i trÆ°á»ng bÃ¬nh thÆ°á»ng
    
    Returns:
        PostgresStorage class hoáº·c None náº¿u khÃ´ng thá»ƒ import
    """
    try:
        # Thá»­ import tá»« __init__.py cá»§a storage module
        from pipelines.crawl.storage import PostgresStorage
        return PostgresStorage
    except ImportError:
        try:
            # Thá»­ import trá»±c tiáº¿p tá»« file
            from pipelines.crawl.storage.postgres_storage import PostgresStorage
            return PostgresStorage
        except ImportError:
            try:
                import importlib.util
                from pathlib import Path
                
                # TÃ¬m Ä‘Æ°á»ng dáº«n Ä‘áº¿n postgres_storage.py
                possible_paths = [
                    # Tá»« /opt/airflow/src (Docker default - Æ°u tiÃªn)
                    Path("/opt/airflow/src/pipelines/crawl/storage/postgres_storage.py"),
                    # Tá»« dag_file_dir
                    Path(dag_file_dir).parent.parent / "src" / "pipelines" / "crawl" / "storage" / "postgres_storage.py",
                    # Tá»« current working directory
                    Path(os.getcwd()) / "src" / "pipelines" / "crawl" / "storage" / "postgres_storage.py",
                    # Tá»« workspace root
                    Path("/workspace/src/pipelines/crawl/storage/postgres_storage.py"),
                ]
                
                postgres_storage_path = None
                for path in possible_paths:
                    if path.exists() and path.is_file():
                        postgres_storage_path = path
                        break
                
                if postgres_storage_path:
                    # Sá»­ dá»¥ng importlib Ä‘á»ƒ load trá»±c tiáº¿p tá»« file
                    spec = importlib.util.spec_from_file_location(
                        "postgres_storage", postgres_storage_path
                    )
                    if spec and spec.loader:
                        postgres_storage_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(postgres_storage_module)
                        return postgres_storage_module.PostgresStorage
                
                # Náº¿u khÃ´ng tÃ¬m tháº¥y file, thá»­ thÃªm src vÃ o path vÃ  import absolute
                src_paths = [
                    Path("/opt/airflow/src"),
                    Path(dag_file_dir).parent.parent / "src",
                    Path(os.getcwd()) / "src",
                ]
                
                for src_path in src_paths:
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))
                        try:
                            from pipelines.crawl.storage import PostgresStorage
                            return PostgresStorage
                        except ImportError:
                            try:
                                from pipelines.crawl.storage.postgres_storage import PostgresStorage
                                return PostgresStorage
                            except ImportError:
                                continue
                
                return None
            except Exception:
                return None


def load_products(**context) -> dict[str, Any]:
    """
    Task: Load dá»¯ liá»‡u Ä‘Ã£ transform vÃ o database

    Returns:
        Dict: Káº¿t quáº£ load vá»›i stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ’¾ TASK: Load Products to Database")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # Láº¥y transformed file tá»« transform_products task
        transform_result = None
        try:
            transform_result = ti.xcom_pull(task_ids="transform_and_load.transform_products")
        except Exception:
            try:
                transform_result = ti.xcom_pull(task_ids="transform_products")
            except Exception:
                pass

        if not transform_result:
            # Fallback: tÃ¬m file transformed
            processed_dir = DATA_DIR / "processed"
            transformed_file = processed_dir / "products_transformed.json"
            if transformed_file.exists():
                transform_result = {"transformed_file": str(transformed_file)}
            else:
                raise ValueError("KhÃ´ng tÃ¬m tháº¥y transform result tá»« XCom hoáº·c file")

        transformed_file = transform_result.get("transformed_file")
        if not transformed_file or not os.path.exists(transformed_file):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file transformed: {transformed_file}")

        logger.info(f"ğŸ“‚ Äang Ä‘á»c transformed file: {transformed_file}")

        # Äá»c transformed products
        with open(transformed_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        logger.info(f"ğŸ“Š Tá»•ng sá»‘ products Ä‘á»ƒ load: {len(products)}")

        # Import DataLoader
        try:
            # TÃ¬m Ä‘Æ°á»ng dáº«n load module
            load_paths = [
                "/opt/airflow/src/pipelines/load/loader.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "load", "loader.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "load", "loader.py"),
            ]

            loader_path = None
            for path in load_paths:
                if os.path.exists(path):
                    loader_path = path
                    break

            if not loader_path:
                raise ImportError("KhÃ´ng tÃ¬m tháº¥y loader.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("loader", loader_path)
            loader_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(loader_module)
            DataLoader = loader_module.DataLoader

            # Láº¥y database config tá»« Airflow Variables hoáº·c environment variables
            # Æ¯u tiÃªn: Airflow Variables > Environment Variables > Default
            db_host = Variable.get("POSTGRES_HOST", default_var=os.getenv("POSTGRES_HOST", "postgres"))
            db_port = int(Variable.get("POSTGRES_PORT", default_var=os.getenv("POSTGRES_PORT", "5432")))
            db_name = Variable.get("POSTGRES_DB", default_var=os.getenv("POSTGRES_DB", "crawl_data"))
            db_user = Variable.get("POSTGRES_USER", default_var=os.getenv("POSTGRES_USER", "postgres"))
            db_password = Variable.get("POSTGRES_PASSWORD", default_var=os.getenv("POSTGRES_PASSWORD", "postgres"))

            # Load vÃ o database
            loader = DataLoader(
                host=db_host,
                port=db_port,
                database=db_name,
                user=db_user,
                password=db_password,
                batch_size=100,
                enable_db=True,
            )

            try:
                # LÆ°u vÃ o processed directory
                processed_dir = DATA_DIR / "processed"
                processed_dir.mkdir(parents=True, exist_ok=True)
                final_file = processed_dir / "products_final.json"

                # Khá»Ÿi táº¡o biáº¿n Ä‘á»ƒ lÆ°u sá»‘ lÆ°á»£ng products
                count_before = None
                count_after = None

                # Kiá»ƒm tra sá»‘ lÆ°á»£ng products trong DB trÆ°á»›c khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is None:
                        raise ImportError("KhÃ´ng thá»ƒ import PostgresStorage")
                    storage = PostgresStorage(
                        host=db_host,
                        port=db_port,
                        database=db_name,
                        user=db_user,
                        password=db_password,
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM products;")
                            count_before = cur.fetchone()[0]
                    storage.close()
                    logger.info(f"ğŸ“Š Sá»‘ products trong DB trÆ°á»›c khi load: {count_before}")
                except Exception as e:
                    logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra sá»‘ lÆ°á»£ng products trong DB: {e}")
                    count_before = None

                load_stats = loader.load_products(
                    products,
                    save_to_file=str(final_file),
                    upsert=True,  # UPDATE náº¿u Ä‘Ã£ tá»“n táº¡i, INSERT náº¿u má»›i
                    validate_before_load=True,
                )

                # Kiá»ƒm tra sá»‘ lÆ°á»£ng products trong DB sau khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is None:
                        raise ImportError("KhÃ´ng thá»ƒ import PostgresStorage")
                    storage = PostgresStorage(
                        host=db_host,
                        port=db_port,
                        database=db_name,
                        user=db_user,
                        password=db_password,
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM products;")
                            count_after = cur.fetchone()[0]
                    storage.close()
                    logger.info(f"ğŸ“Š Sá»‘ products trong DB sau khi load: {count_after}")
                    if count_before is not None:
                        diff = count_after - count_before
                        if diff > 0:
                            logger.info(f"âœ… ÄÃ£ thÃªm {diff} products má»›i vÃ o DB")
                        elif diff == 0:
                            logger.info(f"â„¹ï¸  KhÃ´ng cÃ³ products má»›i (chá»‰ UPDATE cÃ¡c products Ä‘Ã£ cÃ³)")
                        else:
                            logger.warning(f"âš ï¸  Sá»‘ lÆ°á»£ng products giáº£m {abs(diff)} (cÃ³ thá»ƒ do xÃ³a hoáº·c lá»—i)")
                except Exception as e:
                    logger.warning(f"âš ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra sá»‘ lÆ°á»£ng products sau khi load: {e}")
                    count_after = None

                logger.info("=" * 70)
                logger.info("ğŸ“Š LOAD RESULTS")
                logger.info("=" * 70)
                logger.info(f"âœ… DB loaded: {load_stats['db_loaded']} products")
                if load_stats.get("inserted_count") is not None:
                    logger.info(f"   - INSERT (products má»›i): {load_stats.get('inserted_count', 0)}")
                    logger.info(f"   - UPDATE (products Ä‘Ã£ cÃ³): {load_stats.get('updated_count', 0)}")
                logger.info(f"âœ… File loaded: {load_stats['file_loaded']}")
                logger.info(f"âŒ Failed: {load_stats['failed_count']}")
                if count_before is not None and count_after is not None:
                    diff = count_after - count_before
                    logger.info(f"ğŸ“ˆ DB count: {count_before} â†’ {count_after} (thay Ä‘á»•i: {diff:+d})")
                    if diff == 0 and load_stats.get("inserted_count", 0) == 0:
                        logger.info("â„¹ï¸  KhÃ´ng cÃ³ products má»›i - chá»‰ UPDATE cÃ¡c products Ä‘Ã£ cÃ³")
                    elif diff > 0:
                        logger.info(f"âœ… ÄÃ£ thÃªm {diff} products má»›i vÃ o DB")
                logger.info("=" * 70)
                logger.info("â„¹ï¸  LÆ°u Ã½: Vá»›i upsert=True, products Ä‘Ã£ cÃ³ sáº½ Ä‘Æ°á»£c UPDATE (khÃ´ng tÄƒng sá»‘ lÆ°á»£ng)")
                logger.info("â„¹ï¸  Chá»‰ products má»›i (product_id chÆ°a cÃ³) má»›i Ä‘Æ°á»£c INSERT vÃ  tÄƒng sá»‘ lÆ°á»£ng")
                logger.info("=" * 70)

                return {
                    "final_file": str(final_file),
                    "load_stats": load_stats,
                }

            finally:
                loader.close()

        except ImportError as e:
            logger.error(f"âŒ KhÃ´ng thá»ƒ import DataLoader: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"âŒ Lá»—i trong load_products task: {e}", exc_info=True)
        raise


def validate_data(**context) -> dict[str, Any]:
    """
    Task 5: Validate dá»¯ liá»‡u Ä‘Ã£ crawl

    Returns:
        Dict: Káº¿t quáº£ validation
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("âœ… TASK: Validate Data")
    logger.info("=" * 70)

    try:
        ti = context["ti"]
        output_file = None

        # Æ¯u tiÃªn: Láº¥y tá»« save_products_with_detail (cÃ³ detail)
        # CÃ¡ch 1: Láº¥y tá»« task_id vá»›i TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
            logger.info(f"Láº¥y output_file tá»« 'crawl_product_details.save_products_with_detail': {output_file}")
        except Exception as e:
            logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'crawl_product_details.save_products_with_detail': {e}")

        # CÃ¡ch 2: Thá»­ khÃ´ng cÃ³ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                logger.info(f"Láº¥y output_file tá»« 'save_products_with_detail': {output_file}")
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'save_products_with_detail': {e}")

        # Fallback: Láº¥y tá»« save_products (khÃ´ng cÃ³ detail) náº¿u khÃ´ng cÃ³ file vá»›i detail
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="process_and_save.save_products")
                logger.info(f"Láº¥y output_file tá»« 'process_and_save.save_products' (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'process_and_save.save_products': {e}")

        # CÃ¡ch 3: Thá»­ khÃ´ng cÃ³ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products")
                logger.info(f"Láº¥y output_file tá»« 'save_products' (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'save_products': {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file output: {output_file}")

        logger.info(f"Äang validate file: {output_file}")

        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})

        # Validation
        validation_result = {
            "file_exists": True,
            "total_products": len(products),
            "crawled_count": stats.get("crawled_count", 0),  # Sá»‘ lÆ°á»£ng products Ä‘Æ°á»£c crawl detail
            "valid_products": 0,
            "invalid_products": 0,
            "errors": [],
        }

        required_fields = ["product_id", "name", "url"]

        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []

            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)

            if is_valid:
                validation_result["valid_products"] += 1
            else:
                validation_result["invalid_products"] += 1
                validation_result["errors"].append(
                    {
                        "index": i,
                        "product_id": product.get("product_id"),
                        "missing_fields": missing_fields,
                    }
                )

        logger.info("=" * 70)
        logger.info("ğŸ“Š VALIDATION RESULTS")
        logger.info("=" * 70)
        logger.info(f"ğŸ“¦ Tá»•ng sá»‘ products trong file: {validation_result['total_products']}")
        
        # Log thÃ´ng tin vá» crawl detail náº¿u cÃ³
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"ğŸ”„ Products Ä‘Æ°á»£c crawl detail: {crawled_count}")
            logger.info(f"âœ… Products cÃ³ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"â±ï¸  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"âŒ Products failed: {stats.get('failed', 0)}")
        
        logger.info(f"âœ… Valid products: {validation_result['valid_products']}")
        logger.info(f"âŒ Invalid products: {validation_result['invalid_products']}")
        logger.info("=" * 70)

        if validation_result["invalid_products"] > 0:
            logger.warning(f"CÃ³ {validation_result['invalid_products']} sáº£n pháº©m khÃ´ng há»£p lá»‡")
            # KhÃ´ng fail task, chá»‰ warning

        return validation_result

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi validate data: {e}", exc_info=True)
        raise


def aggregate_and_notify(**context) -> dict[str, Any]:
    """
    Task: Tá»•ng há»£p dá»¯ liá»‡u vá»›i AI vÃ  gá»­i thÃ´ng bÃ¡o qua Discord

    Returns:
        Dict: Káº¿t quáº£ tá»•ng há»£p vÃ  gá»­i thÃ´ng bÃ¡o
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ğŸ¤– TASK: Aggregate Data and Send Discord Notification")
    logger.info("=" * 70)

    result = {
        "aggregation_success": False,
        "ai_summary_success": False,
        "discord_notification_success": False,
        "summary": None,
        "ai_summary": None,
    }

    try:
        # Láº¥y Ä‘Æ°á»ng dáº«n file products_with_detail.json
        output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            logger.warning(f"âš ï¸  File khÃ´ng tá»“n táº¡i: {output_file}")
            logger.info("   Thá»­ láº¥y tá»« XCom...")

            ti = context["ti"]
            try:
                output_file = ti.xcom_pull(
                    task_ids="crawl_product_details.save_products_with_detail"
                )
                logger.info(f"   Láº¥y tá»« XCom: {output_file}")
            except Exception:
                try:
                    output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                    logger.info(f"   Láº¥y tá»« XCom (khÃ´ng cÃ³ prefix): {output_file}")
                except Exception as e:
                    logger.warning(f"   KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« XCom: {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file output: {output_file}")

        logger.info(f"ğŸ“Š Äang tá»•ng há»£p dá»¯ liá»‡u tá»«: {output_file}")

        # 1. Tá»•ng há»£p dá»¯ liá»‡u
        if DataAggregator is None:
            logger.warning("âš ï¸  DataAggregator module chÆ°a Ä‘Æ°á»£c import, bá» qua tá»•ng há»£p")
        else:
            try:
                aggregator = DataAggregator(output_file)
                if aggregator.load_data():
                    summary = aggregator.aggregate()
                    result["summary"] = summary
                    result["aggregation_success"] = True
                    logger.info("âœ… Tá»•ng há»£p dá»¯ liá»‡u thÃ nh cÃ´ng")

                    # Log thá»‘ng kÃª
                    stats = summary.get("statistics", {})
                    total_products = stats.get('total_products', 0)
                    crawled_count = stats.get('crawled_count', 0)
                    with_detail = stats.get('with_detail', 0)
                    failed = stats.get('failed', 0)
                    timeout = stats.get('timeout', 0)
                    
                    logger.info(f"   ğŸ“¦ Tá»•ng sáº£n pháº©m: {total_products}")
                    logger.info(f"   ğŸ”„ Products Ä‘Æ°á»£c crawl detail: {crawled_count}")
                    logger.info(f"   âœ… CÃ³ chi tiáº¿t (success): {with_detail}")
                    logger.info(f"   âŒ Tháº¥t báº¡i: {failed}")
                    logger.info(f"   â±ï¸  Timeout: {timeout}")
                    
                    # TÃ­nh vÃ  hiá»ƒn thá»‹ tá»· lá»‡ thÃ nh cÃ´ng
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        logger.info(f"   ğŸ“ˆ Tá»· lá»‡ thÃ nh cÃ´ng: {with_detail}/{crawled_count} ({success_rate:.1f}%)")
                    else:
                        logger.warning("   âš ï¸  KhÃ´ng cÃ³ products nÃ o Ä‘Æ°á»£c crawl detail")
                else:
                    logger.error("âŒ KhÃ´ng thá»ƒ load dá»¯ liá»‡u Ä‘á»ƒ tá»•ng há»£p")
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi tá»•ng há»£p dá»¯ liá»‡u: {e}", exc_info=True)

        # 2. Tá»•ng há»£p vá»›i AI
        if AISummarizer is None:
            logger.warning("âš ï¸  AISummarizer module chÆ°a Ä‘Æ°á»£c import, bá» qua tá»•ng há»£p AI")
        elif result.get("summary"):
            try:
                summarizer = AISummarizer()
                ai_summary = summarizer.summarize_data(result["summary"])
                if ai_summary:
                    result["ai_summary"] = ai_summary
                    result["ai_summary_success"] = True
                    logger.info("âœ… Tá»•ng há»£p vá»›i AI thÃ nh cÃ´ng")
                    logger.info(f"   Äá»™ dÃ i summary: {len(ai_summary)} kÃ½ tá»±")
                else:
                    logger.warning("âš ï¸  KhÃ´ng nháº­n Ä‘Æ°á»£c summary tá»« AI")
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi tá»•ng há»£p vá»›i AI: {e}", exc_info=True)

        # 3. Gá»­i thÃ´ng bÃ¡o qua Discord
        if DiscordNotifier is None:
            logger.warning("âš ï¸  DiscordNotifier module chÆ°a Ä‘Æ°á»£c import, bá» qua gá»­i thÃ´ng bÃ¡o")
        else:
            try:
                notifier = DiscordNotifier()

                # Chuáº©n bá»‹ ná»™i dung
                if result.get("ai_summary"):
                    # Gá»­i vá»›i AI summary
                    stats = result.get("summary", {}).get("statistics", {})
                    crawled_at = result.get("summary", {}).get("metadata", {}).get("crawled_at", "")
                    footer_text = f"Crawl lÃºc: {crawled_at}" if crawled_at else "Tiki Data Pipeline"
                    
                    success = notifier.send_summary(
                        ai_summary=result["ai_summary"],
                        stats=stats,
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("âœ… ÄÃ£ gá»­i thÃ´ng bÃ¡o qua Discord (vá»›i AI summary)")
                    else:
                        logger.warning("âš ï¸  KhÃ´ng thá»ƒ gá»­i thÃ´ng bÃ¡o qua Discord")
                elif result.get("summary"):
                    # Gá»­i vá»›i summary thÃ´ng thÆ°á»ng (khÃ´ng cÃ³ AI) - sá»­ dá»¥ng fields thay vÃ¬ text
                    stats = result.get("summary", {}).get("statistics", {})
                    total_products = stats.get('total_products', 0)
                    crawled_count = stats.get('crawled_count', 0)
                    with_detail = stats.get('with_detail', 0)
                    failed = stats.get('failed', 0)
                    timeout = stats.get('timeout', 0)
                    products_saved = stats.get('products_saved', 0)
                    crawled_at = result.get("summary", {}).get("metadata", {}).get("crawled_at", "N/A")
                    
                    # TÃ­nh tá»· lá»‡ thÃ nh cÃ´ng Ä‘á»ƒ chá»n mÃ u
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        if success_rate >= 80:
                            color = 0x00FF00  # Xanh lÃ¡
                        elif success_rate >= 50:
                            color = 0xFFA500  # Cam
                        else:
                            color = 0xFF0000  # Äá»
                    else:
                        color = 0x808080  # XÃ¡m
                        success_rate = 0
                    
                    # Táº¡o fields cho Discord embed
                    fields = []
                    
                    # Row 1: Tá»•ng quan
                    if total_products > 0:
                        fields.append({
                            "name": "ğŸ“¦ Tá»•ng sáº£n pháº©m",
                            "value": f"**{total_products:,}**",
                            "inline": True,
                        })
                    
                    if crawled_count > 0:
                        fields.append({
                            "name": "ğŸ”„ ÄÃ£ crawl detail",
                            "value": f"**{crawled_count:,}**",
                            "inline": True,
                        })
                    
                    if products_saved > 0:
                        fields.append({
                            "name": "ğŸ’¾ ÄÃ£ lÆ°u",
                            "value": f"**{products_saved:,}**",
                            "inline": True,
                        })
                    
                    # Row 2: Káº¿t quáº£ crawl
                    if crawled_count > 0:
                        fields.append({
                            "name": "âœ… ThÃ nh cÃ´ng",
                            "value": f"**{with_detail:,}** ({success_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    if timeout > 0:
                        timeout_rate = (timeout / crawled_count * 100) if crawled_count > 0 else 0
                        fields.append({
                            "name": "â±ï¸ Timeout",
                            "value": f"**{timeout:,}** ({timeout_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    if failed > 0:
                        failed_rate = (failed / crawled_count * 100) if crawled_count > 0 else 0
                        fields.append({
                            "name": "âŒ Tháº¥t báº¡i",
                            "value": f"**{failed:,}** ({failed_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    # Táº¡o content ngáº¯n gá»n
                    content = "ğŸ“Š **Tá»•ng há»£p dá»¯ liá»‡u crawl tá»« Tiki.vn**\n\n"
                    if crawled_count > 0:
                        content += f"Tá»· lá»‡ thÃ nh cÃ´ng: **{success_rate:.1f}%** ({with_detail}/{crawled_count} products)"
                    else:
                        content += "ChÆ°a cÃ³ products nÃ o Ä‘Æ°á»£c crawl detail."
                    
                    success = notifier.send_message(
                        content=content,
                        title="ğŸ“Š Tá»•ng há»£p dá»¯ liá»‡u Tiki",
                        color=color,
                        fields=fields if fields else None,
                        footer=f"Crawl lÃºc: {crawled_at}",
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("âœ… ÄÃ£ gá»­i thÃ´ng bÃ¡o qua Discord (khÃ´ng cÃ³ AI)")
                    else:
                        logger.warning("âš ï¸  KhÃ´ng thá»ƒ gá»­i thÃ´ng bÃ¡o qua Discord")
                else:
                    logger.warning("âš ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ gá»­i thÃ´ng bÃ¡o")
            except Exception as e:
                logger.error(f"âŒ Lá»—i khi gá»­i thÃ´ng bÃ¡o Discord: {e}", exc_info=True)

        logger.info("=" * 70)
        logger.info("ğŸ“Š Káº¾T QUáº¢ Tá»”NG Há»¢P VÃ€ THÃ”NG BÃO")
        logger.info("=" * 70)
        logger.info(
            f"âœ… Tá»•ng há»£p dá»¯ liá»‡u: {'ThÃ nh cÃ´ng' if result['aggregation_success'] else 'Tháº¥t báº¡i'}"
        )
        logger.info(
            f"âœ… Tá»•ng há»£p AI: {'ThÃ nh cÃ´ng' if result['ai_summary_success'] else 'Tháº¥t báº¡i'}"
        )
        logger.info(
            f"âœ… Gá»­i Discord: {'ThÃ nh cÃ´ng' if result['discord_notification_success'] else 'Tháº¥t báº¡i'}"
        )
        logger.info("=" * 70)

        return result

    except Exception as e:
        logger.error(f"âŒ Lá»—i khi tá»•ng há»£p vÃ  gá»­i thÃ´ng bÃ¡o: {e}", exc_info=True)
        # KhÃ´ng fail task, chá»‰ log lá»—i
        return result


# Táº¡o DAG duy nháº¥t vá»›i schedule cÃ³ thá»ƒ config qua Variable
with DAG(**DAG_CONFIG) as dag:

    # TaskGroup: Load vÃ  Prepare
    with TaskGroup("load_and_prepare") as load_group:
        # Task 0: Extract vÃ  load categories vÃ o database (cháº¡y Ä‘áº§u tiÃªn)
        task_extract_and_load_categories = PythonOperator(
            task_id="extract_and_load_categories_to_db",
            python_callable=extract_and_load_categories_to_db,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 10 phÃºt
            pool="default_pool",
        )

        # Task 1: Load danh sÃ¡ch categories tá»« file Ä‘á»ƒ crawl
        task_load_categories = PythonOperator(
            task_id="load_categories",
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 5 phÃºt
            pool="default_pool",
        )

        # Äáº£m báº£o extract_and_load_categories cháº¡y trÆ°á»›c load_categories
        task_extract_and_load_categories >> task_load_categories

    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup("crawl_categories") as crawl_group:
        # Sá»­ dá»¥ng expand Ä‘á»ƒ Dynamic Task Mapping
        # Cáº§n má»™t task helper Ä‘á»ƒ láº¥y categories vÃ  táº¡o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function Ä‘á»ƒ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # Thá»­ nhiá»u cÃ¡ch láº¥y categories tá»« XCom
            categories = None

            # CÃ¡ch 1: Láº¥y tá»« task_id vá»›i TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"Láº¥y categories tá»« 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'load_and_prepare.load_categories': {e}")

            # CÃ¡ch 2: Thá»­ khÃ´ng cÃ³ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids="load_categories")
                    logger.info(
                        f"Láº¥y categories tá»« 'load_categories': {len(categories) if categories else 0} items"
                    )
                except Exception as e:
                    logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« 'load_categories': {e}")

            # CÃ¡ch 3: Thá»­ láº¥y tá»« upstream task (Ä‘Æ¡n giáº£n hÃ³a Ä‘á»ƒ trÃ¡nh timeout)
            if not categories:
                try:
                    # Láº¥y tá»« task trong cÃ¹ng DAG run - Ä‘Æ¡n giáº£n hÃ³a
                    from airflow.models import TaskInstance

                    dag_run = context["dag_run"]
                    # Láº¥y DAG tá»« context thay vÃ¬ dÃ¹ng biáº¿n global
                    dag_obj = context.get("dag")
                    if dag_obj:
                        upstream_task = dag_obj.get_task("load_and_prepare.load_categories")
                        upstream_ti = TaskInstance(task=upstream_task, run_id=dag_run.run_id)
                        categories = upstream_ti.xcom_pull(key="return_value")
                        logger.info(
                            f"Láº¥y categories tá»« TaskInstance: {len(categories) if categories else 0} items"
                        )
                except Exception as e:
                    logger.warning(f"KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« TaskInstance: {e}")

            if not categories:
                logger.error("âŒ KhÃ´ng thá»ƒ láº¥y categories tá»« XCom!")
                return []

            if not isinstance(categories, list):
                logger.error(f"âŒ Categories khÃ´ng pháº£i list: {type(categories)}")
                return []

            logger.info(
                f"âœ… ÄÃ£ láº¥y {len(categories)} categories, táº¡o {len(categories)} tasks cho Dynamic Task Mapping"
            )

            # Tráº£ vá» list cÃ¡c dict Ä‘á»ƒ expand
            return [{"category": cat} for cat in categories]

        task_prepare_crawl = PythonOperator(
            task_id="prepare_crawl_kwargs",
            python_callable=prepare_crawl_kwargs,
            execution_timeout=timedelta(minutes=1),  # TEST MODE: Giáº£m timeout xuá»‘ng 1 phÃºt,
        )

        # Dynamic Task Mapping vá»›i expand
        # Sá»­ dá»¥ng expand vá»›i op_kwargs Ä‘á»ƒ trÃ¡nh lá»—i vá»›i PythonOperator constructor
        task_crawl_category = PythonOperator.partial(
            task_id="crawl_category",
            python_callable=crawl_single_category,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 10 phÃºt má»—i category
            pool="default_pool",  # CÃ³ thá»ƒ táº¡o pool riÃªng náº¿u cáº§n
            retries=1,  # Retry 1 láº§n (tá»•ng 2 láº§n thá»­: 1 láº§n Ä‘áº§u + 1 retry)
        ).expand(op_kwargs=task_prepare_crawl.output)

    # TaskGroup: Process vÃ  Save
    with TaskGroup("process_and_save") as process_group:
        task_merge_products = PythonOperator(
            task_id="merge_products",
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=10),  # TEST MODE: Giáº£m timeout xuá»‘ng 10 phÃºt,  # Timeout 30 phÃºt
            pool="default_pool",
            trigger_rule="all_done",  # QUAN TRá»ŒNG: Cháº¡y khi táº¥t cáº£ upstream tasks done (success hoáº·c failed)
        )

        task_save_products = PythonOperator(
            task_id="save_products",
            python_callable=save_products,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 10 phÃºt
            pool="default_pool",
        )

    # TaskGroup: Crawl Product Details (Dynamic Task Mapping)
    with TaskGroup("crawl_product_details") as detail_group:

        def prepare_detail_kwargs(**context):
            """Helper function Ä‘á»ƒ prepare op_kwargs cho Dynamic Task Mapping detail"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # Láº¥y products tá»« prepare_products_for_detail
            # Task nÃ y náº±m trong TaskGroup 'crawl_product_details', nÃªn task_id Ä‘áº§y Ä‘á»§ lÃ  'crawl_product_details.prepare_products_for_detail'
            products_to_crawl = None

            # Láº¥y tá»« upstream task (prepare_products_for_detail) - cÃ¡ch Ä‘Ã¡ng tin cáº­y nháº¥t
            # Thá»­ láº¥y upstream_task_ids tá»« nhiá»u nguá»“n khÃ¡c nhau (tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c phiÃªn báº£n Airflow)
            upstream_task_ids = []
            try:
                task_instance = context.get("task_instance")
                if task_instance:
                    # Thá»­ vá»›i RuntimeTaskInstance (Airflow SDK má»›i)
                    if hasattr(task_instance, "upstream_task_ids"):
                        upstream_task_ids = list(task_instance.upstream_task_ids)
                    # Thá»­ vá»›i ti.task (cÃ¡ch khÃ¡c)
                    elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                        upstream_task_ids = list(ti.task.upstream_task_ids)
            except (AttributeError, TypeError) as e:
                logger.debug(f"   KhÃ´ng thá»ƒ láº¥y upstream_task_ids: {e}")

            if upstream_task_ids:
                logger.info(f"ğŸ” Upstream tasks: {upstream_task_ids}")
                # Thá»­ láº¥y tá»« táº¥t cáº£ upstream tasks
                for task_id in upstream_task_ids:
                    try:
                        products_to_crawl = ti.xcom_pull(task_ids=task_id)
                        if products_to_crawl:
                            logger.info(f"âœ… Láº¥y XCom tá»« upstream task: {task_id}")
                            break
                    except Exception as e:
                        logger.debug(f"   KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« {task_id}: {e}")
                        continue

            # Náº¿u váº«n khÃ´ng láº¥y Ä‘Æ°á»£c, thá»­ cÃ¡c cÃ¡ch khÃ¡c
            if not products_to_crawl:
                try:
                    # Thá»­ vá»›i task_id Ä‘áº§y Ä‘á»§ (cÃ³ TaskGroup prefix)
                    products_to_crawl = ti.xcom_pull(
                        task_ids="crawl_product_details.prepare_products_for_detail"
                    )
                    logger.info(
                        "âœ… Láº¥y XCom tá»« task_id: crawl_product_details.prepare_products_for_detail"
                    )
                except Exception as e1:
                    logger.warning(f"âš ï¸  KhÃ´ng láº¥y Ä‘Æ°á»£c vá»›i task_id Ä‘áº§y Ä‘á»§: {e1}")
                    try:
                        # Thá»­ vá»›i task_id khÃ´ng cÃ³ prefix (fallback)
                        products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
                        logger.info("âœ… Láº¥y XCom tá»« task_id: prepare_products_for_detail")
                    except Exception as e2:
                        logger.error(f"âŒ KhÃ´ng thá»ƒ láº¥y XCom vá»›i cáº£ 2 cÃ¡ch: {e1}, {e2}")

            if not products_to_crawl:
                logger.error("âŒ KhÃ´ng thá»ƒ láº¥y products tá»« XCom!")
                try:
                    task_instance = context.get("task_instance")
                    upstream_info = []
                    if task_instance:
                        if hasattr(task_instance, "upstream_task_ids"):
                            upstream_info = list(task_instance.upstream_task_ids)
                        elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                            upstream_info = list(ti.task.upstream_task_ids)
                    logger.error(f"   Upstream tasks: {upstream_info}")
                except Exception as e:
                    logger.error(f"   KhÃ´ng thá»ƒ láº¥y thÃ´ng tin upstream tasks: {e}")
                return []

            if not isinstance(products_to_crawl, list):
                logger.error(f"âŒ Products khÃ´ng pháº£i list: {type(products_to_crawl)}")
                logger.error(f"   Value: {products_to_crawl}")
                return []

            logger.info(f"âœ… ÄÃ£ láº¥y {len(products_to_crawl)} products tá»« XCom")

            # Batch Processing: Chia products thÃ nh batches 10 products/batch
            batch_size = 10
            batches = []
            for i in range(0, len(products_to_crawl), batch_size):
                batch = products_to_crawl[i : i + batch_size]
                batches.append(batch)
            
            logger.info(f"ğŸ“¦ ÄÃ£ chia thÃ nh {len(batches)} batches (má»—i batch {batch_size} products)")
            logger.info(f"   - Batch Ä‘áº§u tiÃªn: {len(batches[0]) if batches else 0} products")
            logger.info(f"   - Batch cuá»‘i cÃ¹ng: {len(batches[-1]) if batches else 0} products")

            # Tráº£ vá» list cÃ¡c dict Ä‘á»ƒ expand (má»—i dict lÃ  1 batch)
            op_kwargs_list = [{"product_batch": batch, "batch_index": idx} for idx, batch in enumerate(batches)]

            logger.info(f"ğŸ”¢ Táº¡o {len(op_kwargs_list)} op_kwargs cho Dynamic Task Mapping (batches)")
            if op_kwargs_list:
                logger.info("ğŸ“‹ Sample batches (first 2):")
                for i, kwargs in enumerate(op_kwargs_list[:2]):
                    batch = kwargs.get("product_batch", [])
                    batch_idx = kwargs.get("batch_index", -1)
                    logger.info(
                        f"  Batch {batch_idx}: {len(batch)} products - IDs: {[p.get('product_id') for p in batch[:3]]}..."
                    )

            return op_kwargs_list

        task_prepare_detail = PythonOperator(
            task_id="prepare_products_for_detail",
            python_callable=prepare_products_for_detail,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,
        )

        task_prepare_detail_kwargs = PythonOperator(
            task_id="prepare_detail_kwargs",
            python_callable=prepare_detail_kwargs,
            execution_timeout=timedelta(minutes=1),  # TEST MODE: Giáº£m timeout xuá»‘ng 1 phÃºt,
        )

        # Dynamic Task Mapping cho crawl detail (Batch Processing)
        task_crawl_product_detail = PythonOperator.partial(
            task_id="crawl_product_detail",
            python_callable=crawl_product_batch,  # DÃ¹ng batch function thay vÃ¬ single
            execution_timeout=timedelta(
                minutes=20
            ),  # TÄƒng timeout lÃªn 20 phÃºt cho batch (10 products Ã— 2 phÃºt)
            pool="default_pool",
            retries=2,  # Giáº£m retry xuá»‘ng 2 vÃ¬ batch cÃ³ thá»ƒ retry individual products
            retry_delay=timedelta(minutes=2),  # Delay 2 phÃºt giá»¯a cÃ¡c retry
        ).expand(op_kwargs=task_prepare_detail_kwargs.output)

        task_merge_product_details = PythonOperator(
            task_id="merge_product_details",
            python_callable=merge_product_details,
            execution_timeout=timedelta(minutes=10),  # TEST MODE: Giáº£m timeout xuá»‘ng 10 phÃºt,  # TÄƒng timeout lÃªn 60 phÃºt cho nhiá»u products
            pool="default_pool",
            trigger_rule="all_done",  # Cháº¡y khi táº¥t cáº£ upstream tasks done
            # TÄƒng heartbeat interval Ä‘á»ƒ trÃ¡nh timeout khi xá»­ lÃ½ nhiá»u dá»¯ liá»‡u
        )

        task_save_products_with_detail = PythonOperator(
            task_id="save_products_with_detail",
            python_callable=save_products_with_detail,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 10 phÃºt
            pool="default_pool",
        )

        # Dependencies trong detail group
        (
            task_prepare_detail
            >> task_prepare_detail_kwargs
            >> task_crawl_product_detail
            >> task_merge_product_details
            >> task_save_products_with_detail
        )

    # TaskGroup: Transform and Load
    with TaskGroup("transform_and_load") as transform_load_group:
        task_transform_products = PythonOperator(
            task_id="transform_products",
            python_callable=transform_products,
            execution_timeout=timedelta(minutes=10),  # TEST MODE: Giáº£m timeout xuá»‘ng 10 phÃºt,  # Timeout 30 phÃºt
            pool="default_pool",
        )

        task_load_products = PythonOperator(
            task_id="load_products",
            python_callable=load_products,
            execution_timeout=timedelta(minutes=10),  # TEST MODE: Giáº£m timeout xuá»‘ng 10 phÃºt,  # Timeout 30 phÃºt
            pool="default_pool",
        )

        # Dependencies trong transform_load group
        task_transform_products >> task_load_products

    # TaskGroup: Validate
    with TaskGroup("validate") as validate_group:
        task_validate_data = PythonOperator(
            task_id="validate_data",
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 5 phÃºt
            pool="default_pool",
        )

    # TaskGroup: Aggregate and Notify
    with TaskGroup("aggregate_and_notify") as aggregate_group:
        task_aggregate_and_notify = PythonOperator(
            task_id="aggregate_and_notify",
            python_callable=aggregate_and_notify,
            execution_timeout=timedelta(minutes=5),  # TEST MODE: Giáº£m timeout xuá»‘ng 5 phÃºt,  # Timeout 10 phÃºt
            pool="default_pool",
            trigger_rule="all_done",  # Cháº¡y ngay cáº£ khi cÃ³ task upstream fail
        )

    # Äá»‹nh nghÄ©a dependencies
    # Flow: Load -> Crawl Categories -> Merge & Save -> Prepare Detail -> Crawl Detail -> Merge & Save Detail -> Transform -> Load -> Validate -> Aggregate

    # Dependencies giá»¯a cÃ¡c TaskGroup
    # Load categories trÆ°á»›c, sau Ä‘Ã³ prepare crawl kwargs
    task_load_categories >> task_prepare_crawl

    # Prepare crawl kwargs -> crawl category (dynamic mapping)
    task_prepare_crawl >> task_crawl_category

    # Crawl category -> merge products (merge cháº¡y khi táº¥t cáº£ crawl tasks done)
    task_crawl_category >> task_merge_products

    # Merge -> save products
    task_merge_products >> task_save_products

    # Save products -> prepare detail -> crawl detail -> merge detail -> save detail -> transform -> load -> validate -> aggregate and notify
    task_save_products >> task_prepare_detail
    # Dependencies trong detail group Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ dÃ²ng 1800
    # Flow: save_products_with_detail -> transform -> load -> validate -> aggregate_and_notify
    (
        task_save_products_with_detail
        >> task_transform_products
        >> task_load_products
        >> task_validate_data
        >> task_aggregate_and_notify
    )
