from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from ..bootstrap import (
    CATEGORIES_FILE,
    DATA_DIR,
    OUTPUT_DIR,
    OUTPUT_FILE,
    OUTPUT_FILE_WITH_DETAIL,
    Any,
    dag_file_dir,
    datetime,
    ensure_output_dirs,
    json,
    os,
    re,
)
from .common import _fix_sys_path_for_pipelines_import  # noqa: F401
from .common import atomic_write_file  # noqa: F401
from .common import get_logger  # noqa: F401


def merge_products(**context) -> dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c

    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("üîÑ TASK: Merge Products")

    try:
        ensure_output_dirs()

        ti = context["ti"]

        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix (pre_crawl.load_categories)
        try:
            categories = ti.xcom_pull(task_ids="pre_crawl.load_categories")
            logger.info(
                f"L·∫•y categories t·ª´ 'pre_crawl.load_categories': {len(categories) if categories else 0} items"
            )
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'pre_crawl.load_categories': {e}")

        # Fallback: C√°c check c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception:
                pass

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")

        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            "total_categories": len(categories),
            "success_categories": 0,
            "failed_categories": 0,
            "timeout_categories": 0,
            "total_products": 0,
            "unique_products": 0,
        }

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = "crawl_categories.crawl_category"

        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(task_ids=task_id, key="return_value")

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                # V·ªõi batch processing, m·ªói ph·∫ßn t·ª≠ c√≥ th·ªÉ l√† list of results (t·ª´ 1 batch)
                for result in all_results:
                    # Check if result is a list (batch result) ho·∫∑c dict (single category result)
                    if isinstance(result, list):
                        # Batch result - flatten it
                        for single_result in result:
                            if single_result and isinstance(single_result, dict):
                                if single_result.get("status") == "success":
                                    stats["success_categories"] += 1
                                    products = single_result.get("products", [])
                                    all_products.extend(products)
                                    stats["total_products"] += len(products)
                                elif single_result.get("status") == "timeout":
                                    stats["timeout_categories"] += 1
                                    logger.warning(
                                        f"‚è±Ô∏è  Category {single_result.get('category_name')} timeout"
                                    )
                                else:
                                    stats["failed_categories"] += 1
                                    logger.warning(
                                        f"‚ùå Category {single_result.get('category_name')} failed: {single_result.get('error')}"
                                    )
                    elif result and isinstance(result, dict):
                        # Single category result
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get("status") == "success":
                    stats["success_categories"] += 1
                    products = all_results.get("products", [])
                    all_products.extend(products)
                    stats["total_products"] += len(products)
                elif all_results.get("status") == "timeout":
                    stats["timeout_categories"] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats["failed_categories"] += 1
                    logger.warning(
                        f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}"
                    )

            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                # Try fetching individual map_index results
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )

                        if result and isinstance(result, dict):
                            if result.get("status") == "success":
                                stats["success_categories"] += 1
                                products = result.get("products", [])
                                all_products.extend(products)
                                stats["total_products"] += len(products)
                            elif result.get("status") == "timeout":
                                stats["timeout_categories"] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats["failed_categories"] += 1
                                logger.warning(
                                    f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                                )
                    except Exception as e:
                        stats["failed_categories"] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")

        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats["failed_categories"] = len(categories)

        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        products_with_sales_count = 0
        for product in all_products:
            product_id = product.get("product_id")
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                # ƒê·∫£m b·∫£o sales_count lu√¥n c√≥ trong product (k·ªÉ c·∫£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                elif product.get("sales_count") is not None:
                    products_with_sales_count += 1
                unique_products.append(product)

        # Log th·ªëng k√™ sales_count
        logger.info(
            f"üìä Products c√≥ sales_count: {products_with_sales_count}/{len(unique_products)} ({products_with_sales_count / len(unique_products) * 100:.1f}%)"
            if unique_products
            else "üìä Products c√≥ sales_count: 0/0"
        )

        stats["unique_products"] = len(unique_products)

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("=" * 70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("=" * 70)

        result = {
            "products": unique_products,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)

    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products")
    logger.info("=" * 70)

    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context["ti"]
        merge_result = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
            # Get merge result from upstream task
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
                # Fallback to merge_products without prefix
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")

        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(get_variable("TIKI_SAVE_BATCH_SIZE", default="10000"))

        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                batch_file = OUTPUT_DIR / f"products_batch_{i // batch_size}.json"
                batch_data = {
                    "batch_index": i // batch_size,
                    "total_batches": (len(products) + batch_size - 1) // batch_size,
                    "products": batch,
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def transform_products(**context) -> dict[str, Any]:
    """
    Task: Transform d·ªØ li·ªáu s·∫£n ph·∫©m (normalize, validate, compute fields)

    Returns:
        Dict: K·∫øt qu·∫£ transform v·ªõi transformed products v√† stats
    """
    logger = get_logger(context)
    logger.info("üîÑ TASK: Transform Products")

    try:
        ensure_output_dirs()
        ti = context["ti"]

        # L·∫•y file t·ª´ save_products_with_detail
        output_file = None
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
        except Exception:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
            except Exception:
                pass

        if not output_file:
            output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {output_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc file: {output_file}")

        # ƒê·ªçc products t·ª´ file
        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})
        logger.info(f"üìä T·ªïng s·ªë products trong file: {len(products)}")

        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")

        # B·ªï sung category_url v√† category_id tr∆∞·ªõc khi transform
        logger.info("üîó ƒêang b·ªï sung category_url v√† category_id...")

        # B∆∞·ªõc 1: Load category_url mapping t·ª´ products.json (n·∫øu c√≥)
        category_url_mapping = {}  # product_id -> category_url
        products_file = OUTPUT_DIR / "products.json"
        if products_file.exists():
            try:
                logger.info(f"üìñ ƒêang ƒë·ªçc category_url mapping t·ª´: {products_file}")
                with open(products_file, encoding="utf-8") as f:
                    products_data = json.load(f)

                products_list = []
                if isinstance(products_data, list):
                    products_list = products_data
                elif isinstance(products_data, dict):
                    if "products" in products_data:
                        products_list = products_data["products"]
                    elif "data" in products_data and isinstance(products_data["data"], dict):
                        products_list = products_data["data"].get("products", [])

                for product in products_list:
                    product_id = product.get("product_id")
                    category_url = product.get("category_url")
                    if product_id and category_url:
                        category_url_mapping[product_id] = category_url

                logger.info(
                    f"‚úÖ ƒê√£ load {len(category_url_mapping)} category_url mappings t·ª´ products.json"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc products.json: {e}")

        # B∆∞·ªõc 2: Import utility ƒë·ªÉ extract category_id
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n utils module
            utils_paths = [
                "/opt/airflow/src/pipelines/crawl/utils.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl", "utils.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "crawl", "utils.py"),
            ]

            utils_path = None
            for path in utils_paths:
                if os.path.exists(path):
                    utils_path = path
                    break

            if utils_path:
                import importlib.util

                spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                extract_category_id_from_url = utils_module.extract_category_id_from_url
            else:
                # Fallback: ƒë·ªãnh nghƒ©a h√†m ƒë∆°n gi·∫£n
                import re

                def extract_category_id_from_url(url: str) -> str | None:
                    if not url:
                        return None
                    match = re.search(r"/c(\d+)", url)
                    if match:
                        return f"c{match.group(1)}"
                    return None

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import extract_category_id_from_url: {e}")
            import re

            def extract_category_id_from_url(url: str) -> str | None:
                if not url:
                    return None
                match = re.search(r"/c(\d+)", url)
                if match:
                    return f"c{match.group(1)}"
                return None

        # B∆∞·ªõc 3: B·ªï sung category_url, category_id v√† ENRICH category_path cho products
        updated_count = 0
        category_id_added = 0
        category_path_count = 0
        category_path_enriched = 0

        # B∆∞·ªõc 3a: Build category_path lookup t·ª´ categories file
        category_path_lookup: dict[str, list] = {}  # category_id -> category_path

        if CATEGORIES_FILE.exists():
            try:
                logger.info(f"üìñ ƒêang load categories t·ª´: {CATEGORIES_FILE}")
                with open(CATEGORIES_FILE, encoding="utf-8") as cf:
                    raw_categories = json.load(cf)

                for cat in raw_categories:
                    cat_id = cat.get("category_id")
                    cat_path = cat.get("category_path")

                    # Ch·ªâ th√™m v√†o lookup n·∫øu c√≥ category_id v√† category_path
                    if cat_id and cat_path:
                        category_path_lookup[cat_id] = cat_path

                logger.info(f"‚úÖ Loaded {len(category_path_lookup)} category_path t·ª´ file")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è L·ªói ƒë·ªçc categories file: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è Categories file kh√¥ng t·ªìn t·∫°i: {CATEGORIES_FILE}")

        for product in products:
            product_id = product.get("product_id")

            # B·ªï sung category_url n·∫øu ch∆∞a c√≥
            if not product.get("category_url") and product_id in category_url_mapping:
                product["category_url"] = category_url_mapping[product_id]
                updated_count += 1

            # Extract category_id t·ª´ category_url n·∫øu c√≥
            category_url = product.get("category_url")
            if category_url and not product.get("category_id"):
                category_id = extract_category_id_from_url(category_url)
                if category_id:
                    product["category_id"] = category_id
                    category_id_added += 1

            # Enrich category_path t·ª´ lookup map (n·∫øu ch∆∞a c√≥)
            if product.get("category_id") and not product.get("category_path"):
                cat_id = product["category_id"]
                if cat_id in category_path_lookup:
                    product["category_path"] = category_path_lookup[cat_id]
                    category_path_enriched += 1

            # ƒê·∫£m b·∫£o category_path ƒë∆∞·ª£c gi·ªØ l·∫°i
            if product.get("category_path"):
                category_path_count += 1

        if updated_count > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_url cho {updated_count} products")
        if category_id_added > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_id cho {category_id_added} products")
        if category_path_enriched > 0:
            logger.info(f"‚úÖ ƒê√£ enrich category_path cho {category_path_enriched} products")
        if category_path_count > 0:
            logger.info(f"‚úÖ T·ªïng products c√≥ category_path: {category_path_count}/{len(products)}")

        # Import DataTransformer
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n transform module
            transform_paths = [
                "/opt/airflow/src/pipelines/transform/transformer.py",
                os.path.abspath(
                    os.path.join(
                        dag_file_dir, "..", "..", "src", "pipelines", "transform", "transformer.py"
                    )
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "transform", "transformer.py"),
            ]

            transformer_path = None
            for path in transform_paths:
                if os.path.exists(path):
                    transformer_path = path
                    break

            if not transformer_path:
                raise ImportError("Kh√¥ng t√¨m th·∫•y transformer.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("transformer", transformer_path)
            transformer_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(transformer_module)
            DataTransformer = transformer_module.DataTransformer

            # Transform products
            transformer = DataTransformer(
                strict_validation=False, remove_invalid=True, normalize_fields=True
            )

            transformed_products, transform_stats = transformer.transform_products(
                products, validate=True
            )

            logger.info(
                f"üìä TRANSFORM: Valid={transform_stats['valid_products']} | Invalid={transform_stats['invalid_products']} | Dupes={transform_stats['duplicates_removed']}"
            )

            # L∆∞u transformed products v√†o file
            processed_dir = DATA_DIR / "processed"
            processed_dir.mkdir(parents=True, exist_ok=True)
            transformed_file = processed_dir / "products_transformed.json"

            output_data = {
                "transformed_at": datetime.now().isoformat(),
                "source_file": output_file,
                "total_products": len(products),
                "transform_stats": transform_stats,
                "products": transformed_products,
            }

            atomic_write_file(str(transformed_file), output_data, **context)
            logger.info(
                f"‚úÖ ƒê√£ l∆∞u {len(transformed_products)} transformed products v√†o: {transformed_file}"
            )

            return {
                "transformed_file": str(transformed_file),
                "transformed_count": len(transformed_products),
                "transform_stats": transform_stats,
            }

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataTransformer: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi transform products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong transform_products task: {e}", exc_info=True)
        raise
