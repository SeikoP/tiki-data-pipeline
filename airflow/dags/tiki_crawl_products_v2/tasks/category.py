from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from tiki_crawl_products_v2.bootstrap import (
    CACHE_DIR,
    CATEGORIES_FILE,
    DEBUG_LOAD_CATEGORIES,
    Any,
    CircuitBreakerOpenError,
    classify_error,
    crawl_category_products,
    datetime,
    ensure_output_dirs,
    get_int_variable,
    get_tiki_circuit_breaker,
    get_tiki_degradation,
    get_tiki_dlq,
    get_variable,
    json,
    os,
    sys,
    time,
)

from .common import (
    _fix_sys_path_for_pipelines_import,  # noqa: F401
    get_logger,  # noqa: F401
)


def load_categories(**context) -> list[dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file

    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    if DEBUG_LOAD_CATEGORIES:
        logger.debug("DEBUG: Task load_categories starting...")

    logger.info("=" * 70)
    logger.info("üìñ TASK: Load Categories")

    if DEBUG_LOAD_CATEGORIES:
        logger.debug(f"DEBUG: CWD: {os.getcwd()}")
        logger.debug(f"DEBUG: PYTHONPATH: {sys.path}")

    try:
        categories_file = str(CATEGORIES_FILE)
        if DEBUG_LOAD_CATEGORIES:
            logger.debug(f"DEBUG: Reading file {categories_file}")

        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")

        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)

        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")

        # L·ªçc level
        min_level = get_int_variable("TIKI_MIN_CATEGORY_LEVEL", default=2)
        max_level = get_int_variable("TIKI_MAX_CATEGORY_LEVEL", default=4)
        categories = [cat for cat in categories if min_level <= cat.get("level", 0) <= max_level]

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng x·ª≠ l√Ω
        max_categories = get_int_variable("TIKI_MAX_CATEGORIES", default=0)
        if max_categories > 0:
            categories = categories[:max_categories]
            logger.info(f"üìä X·ª≠ l√Ω {len(categories)} danh m·ª•c (Level {min_level}-{max_level} | Gi·ªõi h·∫°n: {max_categories})")
        else:
            logger.info(f"üìä X·ª≠ l√Ω {len(categories)} danh m·ª•c (Level {min_level}-{max_level})")

        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)

    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl

    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)

    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get("ti")
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                category = op_kwargs.get("category")

        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get("category") or context.get("op_kwargs", {}).get("category")

    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get("ti")
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")

    category_url = category.get("url", "")
    category_name = category.get("name", "Unknown")
    category_id = category.get("id", "")

    # Clean log for workers
    logger.info(f"‚ñ∂Ô∏è  START: Category [{category_id}] {category_name}")

    result = {
        "category_id": category_id,
        "category_name": category_name,
        "category_url": category_url,
        "products": [],
        "status": "failed",
        "error": None,
        "crawled_at": datetime.now().isoformat(),
        "pages_crawled": 0,
        "products_count": 0,
    }

    try:
        # Lazily initialize expensive singletons only when the task runs
        ensure_output_dirs()
        tiki_degradation = get_tiki_degradation()
        tiki_circuit_breaker = get_tiki_circuit_breaker()
        tiki_dlq = get_tiki_dlq()

        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            logger.warning(f"‚ö†Ô∏è  Service degraded, skip category {category_name}")
            return result

        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(
            get_variable("TIKI_MAX_PAGES_PER_CATEGORY", default="20")
        )  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = get_variable("TIKI_USE_SELENIUM", default="false").lower() == "true"
        timeout = int(get_variable("TIKI_CRAWL_TIMEOUT", default="300"))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(
            get_variable("TIKI_RATE_LIMIT_DELAY", default="1.0")
        )  # Delay 1s gi·ªØa c√°c request

        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        def _crawl_with_params():
            """
            Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker.
            """
            return crawl_category_products(
                category_url,
                max_pages=max_pages if max_pages > 0 else None,
                use_selenium=use_selenium,
                cache_dir=str(CACHE_DIR),
                use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
            )

        try:
            # G·ªçi v·ªõi circuit breaker
            products = tiki_circuit_breaker.call(_crawl_with_params)
            tiki_degradation.record_success()
        except CircuitBreakerOpenError as e:
            # Circuit breaker ƒëang m·ªü
            result["error"] = f"Circuit breaker open: {str(e)}"
            result["status"] = "circuit_breaker_open"
            logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho category {category_name}: {e}")
            # Th√™m v√†o DLQ
            try:
                crawl_error = classify_error(
                    e, context={"category_url": category_url, "category_id": category_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_category_{category_id}",
                    task_type="crawl_category",
                    error=crawl_error,
                    context={
                        "category_url": category_url,
                        "category_name": category_name,
                        "category_id": category_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result
        except Exception:
            # Ghi nh·∫≠n failure
            tiki_degradation.record_failure()
            raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")

        result["products"] = products
        result["status"] = "success"
        result["products_count"] = len(products)
        result["elapsed_time"] = elapsed

        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    return result
