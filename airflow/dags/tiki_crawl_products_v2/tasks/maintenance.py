from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from ..bootstrap import CATEGORIES_FILE, Any, Path, datetime, json, os, redis_cache, src_path, sys
from .common import _fix_sys_path_for_pipelines_import  # noqa: F401
from .common import get_logger  # noqa: F401


def cleanup_incomplete_products_wrapper(**context):
    """
    Task wrapper to cleanup products with missing required fields (seller and/or brand).
    Run this BEFORE crawling to allow re-crawling of incomplete data.

    This is a PREVENTIVE cleanup - better to clean before crawl than after load.
    """
    logger = get_logger(context)

    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        logger.info("üßπ Starting cleanup of incomplete products (missing seller/brand)...")
        storage = PostgresStorage()

        # Clean up products missing seller OR brand (or both)
        # Both are required for quality data
        # require_rating=True to satisfy user request for deleting null ratings
        stats = storage.cleanup_incomplete_products(
            require_seller=True, require_brand=True, require_rating=True
        )

        deleted_count = stats["deleted_count"]
        deleted_no_seller = stats.get("deleted_no_seller", 0)
        deleted_no_brand = stats.get("deleted_no_brand", 0)
        deleted_both = stats.get("deleted_both", 0)

        logger.info("=" * 70)
        logger.info(f"‚úÖ Cleanup complete: {deleted_count} products deleted")
        if deleted_count > 0:
            logger.info(f"   - Missing seller only: {deleted_no_seller}")
            logger.info(f"   - Missing brand only: {deleted_no_brand}")
            logger.info(f"   - Missing both: {deleted_both}")
        logger.info("üí° These products will be re-crawled in the next run")
        logger.info("=" * 70)

        return {
            "status": "success",
            "deleted_count": deleted_count,
            "deleted_no_seller": deleted_no_seller,
            "deleted_no_brand": deleted_no_brand,
            "deleted_both": deleted_both,
        }

    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "deleted_count": 0,
            "deleted_no_seller": 0,
            "deleted_no_brand": 0,
            "deleted_both": 0,
        }


def cleanup_orphan_categories_wrapper(**context):
    """
    Task wrapper to cleanup categories that don't have any matching products.
    Run this after loading categories to keep the table clean.

    X√≥a:
    1. Categories c√≥ product_count = 0 (ho·∫∑c NULL)
    2. Leaf categories kh√¥ng c√≥ products trong b·∫£ng products
    """
    logger = get_logger(context)

    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        logger.info("=" * 70)
        logger.info("üßπ CLEANUP ORPHAN CATEGORIES")
        logger.info("=" * 70)

        storage = PostgresStorage()

        # 1. X√≥a categories c√≥ product_count = 0 ho·∫∑c NULL
        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                # X√≥a categories c√≥ product_count = 0 ho·∫∑c NULL
                cur.execute("""
                    DELETE FROM categories
                    WHERE (product_count = 0 OR product_count IS NULL)
                    AND is_leaf = true
                """)
                deleted_zero_count = cur.rowcount

                # 2. X√≥a leaf categories kh√¥ng c√≥ products trong b·∫£ng products
                cur.execute("""
                    DELETE FROM categories
                    WHERE is_leaf = true
                    AND NOT EXISTS (
                        SELECT 1 FROM products p
                        WHERE p.category_id = categories.category_id
                    )
                """)
                deleted_no_products = cur.rowcount

                conn.commit()

                total_deleted = deleted_zero_count + deleted_no_products

                logger.info("üìä Cleanup results:")
                logger.info(f"   - Categories v·ªõi product_count = 0: {deleted_zero_count}")
                logger.info(f"   - Categories kh√¥ng c√≥ products: {deleted_no_products}")
                logger.info(f"   - T·ªïng c·ªông: {total_deleted} categories ƒë√£ x√≥a")
                logger.info("=" * 70)

        # G·ªçi cleanup_orphan_categories t·ª´ storage ƒë·ªÉ ƒë·∫£m b·∫£o consistency
        # (n√≥ s·∫Ω x√≥a c√°c categories kh√¥ng c√≥ products)
        additional_deleted = storage.cleanup_orphan_categories()

        total_deleted = total_deleted + additional_deleted

        logger.info(f"‚úÖ Cleanup complete: {total_deleted} orphan categories deleted")
        return {
            "status": "success",
            "deleted_count": total_deleted,
            "deleted_zero_count": deleted_zero_count,
            "deleted_no_products": deleted_no_products + additional_deleted,
        }

    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "deleted_count": 0}


def cleanup_redundant_categories_wrapper(**context):
    """
    Task wrapper to remove non-leaf categories.
    """
    logger = get_logger(context)
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        storage = PostgresStorage()
        removed = storage.cleanup_redundant_categories()
        logger.info(f"‚úÖ Removed {removed} non-leaf categories")
        return {"status": "success", "removed": removed}
    except Exception as e:
        logger.error(f"‚ùå Error during redundant category cleanup: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def reconcile_categories_wrapper(**context):
    """
    Task wrapper to reconcile categories from JSON.
    Updates names, removes orphans, and updates product counts.
    """
    logger = get_logger(context)
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        storage = PostgresStorage()
        json_path = str(CATEGORIES_FILE)

        if not os.path.exists(json_path):
            logger.warning(f"‚ö†Ô∏è  File {json_path} not found, skipping name updates")
            return {"status": "skipped", "message": "File not found"}

        with open(json_path, encoding="utf-8") as f:
            categories_data = json.load(f)

        id_to_name = {
            cat.get("category_id"): cat.get("name")
            for cat in categories_data
            if cat.get("category_id") and cat.get("name")
        }

        updated_names = 0
        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                # 1. Update names
                for cat_id, name in id_to_name.items():
                    cur.execute(
                        "UPDATE categories SET name = %s WHERE category_id = %s AND name = category_id",
                        (name, cat_id),
                    )
                    updated_names += cur.rowcount
                conn.commit()

        # 2. Update product counts (already available in storage)
        updated_counts = storage.update_category_product_counts()

        logger.info(f"‚úÖ Reconciled: updated {updated_names} names, {updated_counts} counts")
        return {
            "status": "success",
            "updated_names": updated_names,
            "updated_counts": updated_counts,
        }

    except Exception as e:
        logger.error(f"‚ùå Error during category reconciliation: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def cleanup_old_history_wrapper(**context):
    """
    Task wrapper to archive and delete old crawl history.
    """
    logger = get_logger(context)
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        storage = PostgresStorage()

        # Config via variables
        archive_months = int(get_variable("HISTORY_ARCHIVE_MONTHS", "6"))
        delete_months = int(get_variable("HISTORY_DELETE_MONTHS", "12"))

        logger.info(f"üßπ Cleaning history (Archive: {archive_months}m, Delete: {delete_months}m)")
        result = storage.cleanup_old_history(archive_months, delete_months)

        logger.info(
            f"‚úÖ Done: Archived {result['archived_count']}, Deleted {result['deleted_count']}"
        )
        return result
    except Exception as e:
        logger.error(f"‚ùå Error during history cleanup: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def validate_data(**context) -> dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl

    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("=" * 70)

    try:
        ti = context["ti"]
        output_file = None

        # ∆Øu ti√™n: L·∫•y t·ª´ save_products_with_detail (c√≥ detail)
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
            logger.info(
                f"L·∫•y output_file t·ª´ 'crawl_product_details.save_products_with_detail': {output_file}"
            )
        except Exception as e:
            logger.warning(
                f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'crawl_product_details.save_products_with_detail': {e}"
            )

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                logger.debug(f"Output from save_products_with_detail: {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products_with_detail': {e}")

        # Fallback: L·∫•y t·ª´ save_products (kh√¥ng c√≥ detail) n·∫øu kh√¥ng c√≥ file v·ªõi detail
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="process_and_save.save_products")
                logger.info(
                    f"L·∫•y output_file t·ª´ 'process_and_save.save_products' (fallback): {output_file}"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")

        # C√°ch 3: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products")
                logger.debug(f"Output from save_products (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"ƒêang validate file: {output_file}")

        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})

        # Validation
        validation_result = {
            "file_exists": True,
            "total_products": len(products),
            "crawled_count": stats.get("crawled_count", 0),  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
            "valid_products": 0,
            "invalid_products": 0,
            "errors": [],
        }

        required_fields = ["product_id", "name", "url"]

        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []

            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)

            if is_valid:
                validation_result["valid_products"] += 1
            else:
                validation_result["invalid_products"] += 1
                validation_result["errors"].append(
                    {
                        "index": i,
                        "product_id": product.get("product_id"),
                        "missing_fields": missing_fields,
                    }
                )

        logger.info("=" * 70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng s·ªë products trong file: {validation_result['total_products']}")

        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")

        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("=" * 70)

        if validation_result["invalid_products"] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning

        return validation_result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


def cleanup_redis_cache(**context) -> dict[str, Any]:
    """
    Task: Cleanup Redis cache

    Cleanup Redis cache ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ v√† ƒë·∫£m b·∫£o cache kh√¥ng qu√° c≈©.
    Task n√†y ch·∫°y v·ªõi trigger_rule="all_done" ƒë·ªÉ ch·∫°y ngay c·∫£ khi upstream tasks fail.

    Returns:
        Dict: K·∫øt qu·∫£ cleanup
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üßπ TASK: Cleanup Redis Cache")
    logger.info("=" * 70)

    result = {
        "status": "failed",
        "redis_reset": False,
        "stats_before": {},
        "stats_after": {},
    }

    try:
        # Ensure src path in sys.path for package-style imports
        import sys
        from pathlib import Path

        src_path = Path("/opt/airflow/src")
        if src_path.exists() and str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache  # type: ignore
        except Exception as import_err:
            logger.warning(
                f"‚ö†Ô∏è  Import get_redis_cache failed: {import_err} -> trying dynamic import"
            )
            # Dynamic import fallback
            import importlib.util

            rc_path = src_path / "pipelines" / "crawl" / "storage" / "redis_cache.py"
            get_redis_cache = None  # type: ignore
            if rc_path.exists():
                spec = importlib.util.spec_from_file_location("redis_cache_dyn", rc_path)
                if spec and spec.loader:
                    mod = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(mod)  # type: ignore
                    get_redis_cache = getattr(mod, "get_redis_cache", None)  # type: ignore
            if not get_redis_cache:
                raise RuntimeError(
                    "Kh√¥ng th·ªÉ import get_redis_cache (dynamic import c≈©ng th·∫•t b·∫°i)"
                ) from import_err

        # K·∫øt n·ªëi Redis
        redis_cache = get_redis_cache("redis://redis:6379/1")  # type: ignore
        if not redis_cache:
            logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ k·∫øt n·ªëi Redis, skip cleanup")
            result["status"] = "skipped"
            result["reason"] = "Redis not available"
            return result

        # L·∫•y stats tr∆∞·ªõc khi cleanup (d√πng client.info())
        try:
            info_before = redis_cache.client.info()
            db_key = f"db{redis_cache.client.connection_pool.connection_kwargs.get('db', 1)}"
            keys_before = info_before.get(db_key, {}).get("keys", 0)
            hits = info_before.get("keyspace_hits", 0)
            misses = info_before.get("keyspace_misses", 0)
            hit_rate = (hits / (hits + misses) * 100) if (hits + misses) > 0 else 0.0
            stats_before = {
                "keys": keys_before,
                "used_memory_human": info_before.get("used_memory_human"),
                "hit_rate": hit_rate,
                "keyspace_hits": hits,
                "keyspace_misses": misses,
            }
            result["stats_before"] = stats_before
            logger.info("üìä Redis stats tr∆∞·ªõc cleanup:")
            logger.info(f"   - Keys: {keys_before}")
            logger.info(f"   - Memory used: {stats_before.get('used_memory_human', 'N/A')}")
            logger.info(f"   - Hit rate: {hit_rate:.1f}%")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y stats tr∆∞·ªõc cleanup: {e}")

        # Reset cache
        logger.info("üßπ ƒêang cleanup Redis cache...")
        try:
            redis_cache.client.flushdb()
            result["redis_reset"] = True
            logger.info("‚úÖ ƒê√£ flush DB Redis cache th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå Flush DB th·∫•t b·∫°i: {e}")

        # L·∫•y stats sau khi cleanup
        try:
            import time as _t

            _t.sleep(1)
            info_after = redis_cache.client.info()
            db_key = f"db{redis_cache.client.connection_pool.connection_kwargs.get('db', 1)}"
            keys_after = info_after.get(db_key, {}).get("keys", 0)
            hits_a = info_after.get("keyspace_hits", 0)
            misses_a = info_after.get("keyspace_misses", 0)
            hit_rate_a = (hits_a / (hits_a + misses_a) * 100) if (hits_a + misses_a) > 0 else 0.0
            stats_after = {
                "keys": keys_after,
                "used_memory_human": info_after.get("used_memory_human"),
                "hit_rate": hit_rate_a,
                "keyspace_hits": hits_a,
                "keyspace_misses": misses_a,
            }
            result["stats_after"] = stats_after
            logger.info("üìä Redis stats sau cleanup:")
            logger.info(f"   - Keys: {keys_after}")
            logger.info(f"   - Memory used: {stats_after.get('used_memory_human', 'N/A')}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y stats sau cleanup: {e}")

        result["status"] = "success"
        logger.info("‚úÖ Cleanup Redis cache ho√†n t·∫•t")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi cleanup Redis cache: {e}", exc_info=True)
        result["error"] = str(e)

    logger.info("=" * 70)
    return result


def backup_database(**context) -> dict[str, Any]:
    """
    Task: Backup PostgreSQL database

    Backup database crawl_data v√†o th∆∞ m·ª•c backups/postgres sau khi c√°c tasks kh√°c ho√†n th√†nh.

    Returns:
        Dict: K·∫øt qu·∫£ backup
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Backup Database")
    logger.info("=" * 70)

    try:
        import subprocess
        from pathlib import Path

        # ƒê∆∞·ªùng d·∫´n script backup
        script_path = Path("/opt/airflow/scripts/helper/backup_postgres.py")
        if not script_path.exists():
            # Fallback: th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
            script_path = (
                Path(__file__).parent.parent.parent / "scripts" / "helper" / "backup_postgres.py"
            )

        if not script_path.exists():
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y script backup t·∫°i: {script_path}")
            logger.info("üí° S·ª≠ d·ª•ng pg_dump tr·ª±c ti·∫øp...")

            # Fallback: s·ª≠ d·ª•ng pg_dump tr·ª±c ti·∫øp
            container_name = "tiki-data-pipeline-postgres-1"
            # Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n backup
            backup_dirs = [
                Path("/opt/airflow/backups/postgres"),  # Trong container Airflow
                Path("/backups"),  # Mount t·ª´ postgres container
                Path("/opt/airflow/data/backups/postgres"),  # Fallback
            ]
            backup_dir = None
            for bd in backup_dirs:
                try:
                    bd.mkdir(parents=True, exist_ok=True)
                    # Test write
                    test_file = bd / ".test_write"
                    test_file.write_text("test")
                    test_file.unlink()
                    backup_dir = bd
                    break
                except Exception:
                    continue

            if not backup_dir:
                logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c backup c√≥ th·ªÉ ghi, s·ª≠ d·ª•ng /tmp")
                backup_dir = Path("/tmp/backups")
                backup_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = backup_dir / f"crawl_data_{timestamp}.sql"  # ƒê·ªïi .dump -> .sql

            # L·∫•y th√¥ng tin t·ª´ environment variables
            postgres_user = os.getenv("POSTGRES_USER", "airflow_user")
            postgres_password = os.getenv("POSTGRES_PASSWORD", "")

            if not postgres_password:
                logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y POSTGRES_PASSWORD trong environment")
                return {"status": "skipped", "reason": "No password"}

            logger.info("üì¶ ƒêang backup database: crawl_data...")
            logger.info(f"   File: {backup_file}")

            # Ch·∫°y pg_dump trong container - d√πng plain SQL format
            cmd = [
                "docker",
                "exec",
                "-e",
                f"PGPASSWORD={postgres_password}",
                container_name,
                "pg_dump",
                "-U",
                postgres_user,
                "--format=plain",  # Plain SQL format - d·ªÖ restore, t∆∞∆°ng th√≠ch
                "--no-owner",  # Kh√¥ng dump owner info
                "--no-acl",  # Kh√¥ng dump access privileges
                "crawl_data",
            ]

            try:
                with open(backup_file, "wb") as f:
                    result = subprocess.run(
                        cmd,
                        stdout=f,
                        stderr=subprocess.PIPE,
                        check=False,
                        timeout=600,  # 10 ph√∫t timeout
                    )

                if result.returncode == 0:
                    file_size = backup_file.stat().st_size
                    size_mb = file_size / (1024 * 1024)
                    logger.info(f"‚úÖ ƒê√£ backup th√†nh c√¥ng: {backup_file.name}")
                    logger.info(f"   Size: {size_mb:.2f} MB")
                    return {
                        "status": "success",
                        "backup_file": str(backup_file),
                        "size_mb": round(size_mb, 2),
                    }
                else:
                    error_msg = result.stderr.decode("utf-8", errors="ignore")
                    logger.error(f"‚ùå L·ªói khi backup: {error_msg}")
                    if backup_file.exists():
                        backup_file.unlink()
                    return {"status": "failed", "error": error_msg}

            except subprocess.TimeoutExpired:
                logger.error("‚ùå Timeout khi backup database")
                if backup_file.exists():
                    backup_file.unlink()
                return {"status": "failed", "error": "Timeout"}
            except Exception as e:
                logger.error(f"‚ùå Exception khi backup: {e}")
                if backup_file.exists():
                    backup_file.unlink()
                return {"status": "failed", "error": str(e)}
        else:
            # S·ª≠ d·ª•ng script backup (d√πng format sql ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ version dump)
            logger.info(f"üì¶ ƒêang backup database b·∫±ng script: {script_path}")

            cmd = ["python", str(script_path), "--database", "crawl_data", "--format", "sql"]

            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    check=False,
                    timeout=600,  # 10 ph√∫t timeout
                )

                if result.returncode == 0:
                    logger.info("‚úÖ Backup th√†nh c√¥ng!")
                    if result.stdout:
                        logger.info(result.stdout)
                    return {
                        "status": "success",
                        "output": result.stdout,
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è  Backup c√≥ l·ªói (exit code: {result.returncode})")
                    if result.stdout:
                        logger.info("--- STDOUT ---")
                        logger.info(result.stdout)
                    if result.stderr:
                        logger.warning("--- STDERR ---")
                        logger.warning(result.stderr)
                    # Kh√¥ng fail task, ch·ªâ log warning
                    return {
                        "status": "warning",
                        "error": result.stderr or result.stdout,
                    }
            except subprocess.TimeoutExpired:
                logger.error("‚ùå Timeout khi backup database")
                return {"status": "failed", "error": "Timeout"}
            except Exception as e:
                logger.error(f"‚ùå Exception khi backup: {e}")
                return {"status": "failed", "error": str(e)}

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong backup_database task: {e}", exc_info=True)
        # Kh√¥ng fail task, ch·ªâ log l·ªói
        return {"status": "failed", "error": str(e)}
