from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from tiki_crawl_products_v2.bootstrap import (
    OUTPUT_FILE_WITH_DETAIL,
    Any,
    datetime,
    get_AISummarizer,
    get_DataAggregator,
    get_DiscordNotifier,
    os,
)

from .common import (
    _fix_sys_path_for_pipelines_import,  # noqa: F401
    get_logger,  # noqa: F401
)


def aggregate_and_notify(**context) -> dict[str, Any]:
    """
    Task: T·ªïng h·ª£p d·ªØ li·ªáu v·ªõi AI v√† g·ª≠i th√¥ng b√°o qua Discord

    Returns:
        Dict: K·∫øt qu·∫£ t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ü§ñ TASK: Aggregate Data and Send Discord Notification")
    logger.info("=" * 70)

    result = {
        "aggregation_success": False,
        "ai_summary_success": False,
        "discord_notification_success": False,
        "summary": None,
        "ai_summary": None,
    }

    try:
        # L·∫•y ƒë∆∞·ªùng d·∫´n file products_with_detail.json
        output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            logger.warning(f"‚ö†Ô∏è  File kh√¥ng t·ªìn t·∫°i: {output_file}")
            logger.info("   Th·ª≠ l·∫•y t·ª´ XCom...")

            ti = context["ti"]
            try:
                output_file = ti.xcom_pull(
                    task_ids="crawl_product_details.save_products_with_detail"
                )
                logger.info(f"   L·∫•y t·ª´ XCom: {output_file}")
            except Exception:
                try:
                    output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                    logger.info(f"   L·∫•y t·ª´ XCom (kh√¥ng c√≥ prefix): {output_file}")
                except Exception as e:
                    logger.warning(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ XCom: {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"üìä ƒêang t·ªïng h·ª£p d·ªØ li·ªáu t·ª´: {output_file}")

        # 1. T·ªïng h·ª£p d·ªØ li·ªáu
        DataAggregator = get_DataAggregator()
        if DataAggregator is None:
            logger.warning("‚ö†Ô∏è  DataAggregator module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p")
        else:
            try:
                aggregator = DataAggregator(output_file)
                if aggregator.load_data():
                    summary = aggregator.aggregate()
                    result["summary"] = summary
                    result["aggregation_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu th√†nh c√¥ng")

                    # Log th·ªëng k√™
                    stats = summary.get("statistics", {})
                    total_products = stats.get("total_products", 0)
                    crawled_count = stats.get("crawled_count", 0)
                    with_detail = stats.get("with_detail", 0)
                    failed = stats.get("failed", 0)
                    timeout = stats.get("timeout", 0)

                    logger.info(f"   üì¶ T·ªïng s·∫£n ph·∫©m: {total_products}")
                    logger.info(f"   üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
                    logger.info(f"   ‚úÖ C√≥ chi ti·∫øt (success): {with_detail}")
                    logger.info(f"   ‚ùå Th·∫•t b·∫°i: {failed}")
                    logger.info(f"   ‚è±Ô∏è  Timeout: {timeout}")

                    # T√≠nh v√† hi·ªÉn th·ªã t·ª∑ l·ªá th√†nh c√¥ng
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        logger.info(
                            f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {with_detail}/{crawled_count} ({success_rate:.1f}%)"
                        )
                    else:
                        logger.warning("   ‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c crawl detail")
                else:
                    logger.error("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu ƒë·ªÉ t·ªïng h·ª£p")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p d·ªØ li·ªáu: {e}", exc_info=True)

        # 2. T·ªïng h·ª£p v·ªõi AI
        AISummarizer = get_AISummarizer()
        if AISummarizer is None:
            logger.warning("‚ö†Ô∏è  AISummarizer module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p AI")
        elif result.get("summary"):
            try:
                summarizer = AISummarizer()
                ai_summary = summarizer.summarize_data(result["summary"])
                if ai_summary:
                    result["ai_summary"] = ai_summary
                    result["ai_summary_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p v·ªõi AI th√†nh c√¥ng")
                    logger.info(f"   ƒê·ªô d√†i summary: {len(ai_summary)} k√Ω t·ª±")
                else:
                    logger.warning("‚ö†Ô∏è  Kh√¥ng nh·∫≠n ƒë∆∞·ª£c summary t·ª´ AI")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v·ªõi AI: {e}", exc_info=True)

        # 3. G·ª≠i th√¥ng b√°o qua Discord (r√∫t g·ªçn n·ªôi dung nh∆∞ng gi·ªØ l·∫°i l·ªói chi ti·∫øt)
        DiscordNotifier = get_DiscordNotifier()
        if DiscordNotifier is None:
            logger.warning("DiscordNotifier module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua g·ª≠i th√¥ng b√°o")
        else:
            try:
                notifier = DiscordNotifier()

                if result.get("summary"):
                    # L·∫•y stats
                    stats = result["summary"].get("statistics", {})
                    total_products = stats.get("total_products", 0)
                    crawled_count = stats.get("crawled_count", 0)
                    with_detail = stats.get("with_detail", 0)
                    failed = stats.get("failed", 0)
                    timeout = stats.get("timeout", 0)
                    products_saved = stats.get("products_saved", 0)
                    crawled_at = result["summary"].get("metadata", {}).get("crawled_at", "N/A")

                    # T√≠nh m√†u theo success rate
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        color = (
                            0x00B894
                            if success_rate >= 80
                            else (0xF39C12 if success_rate >= 50 else 0xE74C3C)
                        )
                    else:
                        success_rate = 0
                        color = 0x95A5A6

                    # Fields v·ªõi error analysis ƒë·∫ßy ƒë·ªß
                    fields = []
                    fields.append({"name": "Total", "value": f"{total_products:,}", "inline": True})
                    fields.append(
                        {"name": "Crawled", "value": f"{crawled_count:,}", "inline": True}
                    )
                    fields.append(
                        {
                            "name": "Success",
                            "value": f"{with_detail:,} ({success_rate:.1f}%)",
                            "inline": True,
                        }
                    )

                    # Th√™m error analysis chi ti·∫øt
                    if failed > 0 or timeout > 0:
                        total_errors = failed + timeout
                        error_rate = (
                            (total_errors / crawled_count * 100) if crawled_count > 0 else 0
                        )
                        err_info = f"**Total Errors: {total_errors}** ({error_rate:.1f}%)\n"
                        if failed > 0:
                            failed_rate = (failed / crawled_count * 100) if crawled_count > 0 else 0
                            err_info += f"‚Ä¢ Failed: {failed} ({failed_rate:.1f}%)\n"
                        if timeout > 0:
                            timeout_rate = (
                                (timeout / crawled_count * 100) if crawled_count > 0 else 0
                            )
                            err_info += f"‚Ä¢ Timeout: {timeout} ({timeout_rate:.1f}%)"
                        fields.append(
                            {"name": "Error Analysis", "value": err_info.strip(), "inline": False}
                        )

                    if products_saved:
                        fields.append(
                            {"name": "Saved to DB", "value": f"{products_saved:,}", "inline": True}
                        )

                    # N·ªôi dung r√µ r√†ng
                    content = "T·ªïng h·ª£p d·ªØ li·ªáu crawl Tiki.vn\n"
                    if crawled_count > 0:
                        content += f"```\nTh√†nh c√¥ng: {success_rate:.1f}% ({with_detail}/{crawled_count})\n```"
                    else:
                        content += "Ch∆∞a c√≥ s·∫£n ph·∫©m ƒë∆∞·ª£c crawl detail."

                    success = notifier.send_message(
                        content=content,
                        title="T·ªïng h·ª£p d·ªØ li·ªáu Tiki",
                        color=color,
                        fields=fields,
                        footer=f"Crawl l√∫c: {crawled_at}",
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("ƒê√£ g·ª≠i th√¥ng b√°o Discord")
                    else:
                        logger.warning("Kh√¥ng th·ªÉ g·ª≠i th√¥ng b√°o qua Discord")
                else:
                    logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ g·ª≠i th√¥ng b√°o")
            except Exception as e:
                logger.error(f"L·ªói khi g·ª≠i th√¥ng b√°o Discord: {e}", exc_info=True)

        logger.info("=" * 70)
        logger.info("üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P V√Ä TH√îNG B√ÅO")
        logger.info("=" * 70)
        logger.info(
            f"‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu: {'Th√†nh c√¥ng' if result['aggregation_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ T·ªïng h·ª£p AI: {'Th√†nh c√¥ng' if result['ai_summary_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ G·ª≠i Discord: {'Th√†nh c√¥ng' if result['discord_notification_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info("=" * 70)

        # Performance Summary
        try:
            dag_run = context.get("dag_run")
            if dag_run and dag_run.start_date:
                start_time = dag_run.start_date
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                total_products = result.get("with_detail", 0)  # Use crawled products count

                # Calculate throughput
                throughput = total_products / duration if duration > 0 else 0
                avg_time = duration / total_products if total_products > 0 else 0

                logger.info("=" * 70)
                logger.info("‚ö° PERFORMANCE SUMMARY")
                logger.info(f"‚è±Ô∏è  Duration: {duration / 60:.1f} min | Products: {total_products}")
                if throughput > 0:
                    logger.info(
                        f"üìà Throughput: {throughput:.2f} products/s | Avg: {avg_time:.1f}s/product"
                    )
                logger.info("=" * 70)

                result["performance"] = {
                    "duration_minutes": round(duration / 60, 2),
                    "total_products": total_products,
                    "throughput": round(throughput, 2),
                    "avg_time_per_product": round(avg_time, 2),
                }
        except Exception as perf_error:
            logger.warning(f"‚ö†Ô∏è  Performance summary error: {perf_error}")

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o: {e}", exc_info=True)
        # Kh√¥ng fail task, ch·ªâ log l·ªói
        return result
