from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from tiki_crawl_products_v2.bootstrap import (
    DETAIL_CACHE_DIR,
    OUTPUT_FILE,
    OUTPUT_FILE_WITH_DETAIL,
    PROGRESS_FILE,
    Any,
    CircuitBreakerOpenError,
    Path,
    SeleniumDriverPool,
    atomic_write_file,
    classify_error,
    crawl_product_detail_async,
    crawl_product_detail_with_driver,
    crawl_product_detail_with_selenium,
    dag_file_dir,
    datetime,
    ensure_output_dirs,
    extract_product_detail,
    get_hierarchy_map,
    get_int_variable,
    get_logger,
    get_tiki_circuit_breaker,
    get_tiki_degradation,
    get_tiki_dlq,
    get_variable,
    json,
    os,
    re,
    redis_cache,
    shutil,
    sys,
    time,
)

from .common import (
    _fix_sys_path_for_pipelines_import,  # noqa: F401
)
from .loader import _import_postgres_storage  # noqa: F401


def prepare_products_for_detail(**context) -> list[dict[str, Any]]:
    """
    Task: Chu·∫©n b·ªã danh s√°ch products ƒë·ªÉ crawl detail

    T·ªëi ∆∞u cho multi-day crawling:
    - Ch·ªâ crawl products ch∆∞a c√≥ detail
    - Chia th√†nh batches theo ng√†y (c√≥ th·ªÉ crawl trong nhi·ªÅu ng√†y)
    - Ki·ªÉm tra cache v√† progress ƒë·ªÉ tr√°nh crawl l·∫°i
    - Track progress ƒë·ªÉ resume t·ª´ ƒëi·ªÉm d·ª´ng

    Returns:
        List[Dict]: List c√°c dict ch·ª©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìã TASK: Prepare Products for Detail Crawling (Multi-Day Support)")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products t·ª´ task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products: {len(products)}")

        # ƒê·ªçc progress file ƒë·ªÉ bi·∫øt ƒë√£ crawl ƒë·∫øn ƒë√¢u
        progress = {
            "crawled_product_ids": set(),
            "last_crawled_index": 0,
            "total_crawled": 0,
            "last_updated": None,
        }

        if PROGRESS_FILE.exists():
            try:
                with open(PROGRESS_FILE, encoding="utf-8") as f:
                    saved_progress = json.load(f)
                    progress["crawled_product_ids"] = set(
                        saved_progress.get("crawled_product_ids", [])
                    )
                    progress["last_crawled_index"] = saved_progress.get("last_crawled_index", 0)
                    progress["total_crawled"] = saved_progress.get("total_crawled", 0)
                    progress["last_updated"] = saved_progress.get("last_updated")
                    logger.info(
                        f"üìÇ ƒê√£ load progress: {len(progress['crawled_product_ids'])} products ƒë√£ crawl"
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c progress file: {e}")

        # L·ªçc products c·∫ßn crawl detail
        products_to_crawl = []
        cache_hits = 0
        already_crawled = 0
        db_hits = 0  # Products ƒë√£ c√≥ trong DB

        products_per_day = get_int_variable("TIKI_PRODUCTS_PER_DAY", default=1000)
        # M·∫∑c ƒë·ªãnh gi·ªõi h·∫°n s·ªë products/ng√†y ƒë·ªÉ tr√°nh qu√° t·∫£i server
        max_products = int(
            get_variable("TIKI_MAX_PRODUCTS_FOR_DETAIL", default="0")
        )  # 0 = kh√¥ng gi·ªõi h·∫°n

        logger.info(
            f"‚öôÔ∏è  C·∫•u h√¨nh: {products_per_day} products/ng√†y, max: {max_products if max_products > 0 else 'kh√¥ng gi·ªõi h·∫°n'}"
        )

        # Ki·ªÉm tra products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß (ƒë·ªÉ tr√°nh crawl l·∫°i)
        # Ch·ªâ skip products c√≥ price v√† sales_count (detail ƒë·∫ßy ƒë·ªß)
        existing_product_ids_in_db = set()
        try:
            PostgresStorage = _import_postgres_storage()
            if PostgresStorage is None:
                logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ import PostgresStorage, b·ªè qua ki·ªÉm tra database")
            else:
                # L·∫•y database config
                db_host = get_variable(
                    "POSTGRES_HOST", default=os.getenv("POSTGRES_HOST", "postgres")
                )
                db_port = int(
                    get_variable("POSTGRES_PORT", default=os.getenv("POSTGRES_PORT", "5432"))
                )
                db_name = get_variable(
                    "POSTGRES_DB", default=os.getenv("POSTGRES_DB", "crawl_data")
                )
                db_user = get_variable(
                    "POSTGRES_USER", default=os.getenv("POSTGRES_USER", "postgres")
                )
                # trufflehog:ignore - Fallback for development, production uses Airflow Variables
                db_password = get_variable(
                    "POSTGRES_PASSWORD", default=os.getenv("POSTGRES_PASSWORD", "postgres")
                )

                storage = PostgresStorage(
                    host=db_host,
                    port=db_port,
                    database=db_name,
                    user=db_user,
                    password=db_password,
                )

                # L·∫•y danh s√°ch product_ids t·ª´ products list
                product_ids_to_check = [
                    p.get("product_id") for p in products if p.get("product_id")
                ]

                if product_ids_to_check:
                    logger.info(
                        f"üîç ƒêang ki·ªÉm tra {len(product_ids_to_check)} products trong database..."
                    )

                    # C·∫•u h√¨nh th·ªùi gian relaxation (m·∫∑c ƒë·ªãnh 7 ng√†y)
                    cache_relax_days = get_int_variable("TIKI_CACHE_RELAX_DAYS", default=7)

                    # Skip check info reduced to avoid spam
                    # logger.info("Checking skip conditions...")

                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            # Chia nh·ªè query n·∫øu c√≥ qu√° nhi·ªÅu product_ids
                            for i in range(0, len(product_ids_to_check), 1000):
                                batch_ids = product_ids_to_check[i : i + 1000]
                                placeholders = ",".join(["%s"] * len(batch_ids))

                                # Logic check:
                                # Normal: Skip if (Has Full Detail) OR (Is Recent)
                                # Strict Recency (Force Update Old): Skip if (Is Recent) ONLY.

                                check_recency_only = (
                                    get_variable("TIKI_CHECK_RECENCY_ONLY", default="false").lower()
                                    == "true"
                                )

                                if check_recency_only:
                                    logger.info("üïí RECENCY MODE: Ch·ªâ check updated_at")
                                    filter_condition = (
                                        f"updated_at > NOW() - INTERVAL '{cache_relax_days} days'"
                                    )
                                else:
                                    filter_condition = f"""
                                        (brand IS NOT NULL AND brand != '' AND seller_name IS NOT NULL AND seller_name != '')
                                        OR (updated_at > NOW() - INTERVAL '{cache_relax_days} days')
                                    """

                                cur.execute(
                                    f"""
                                    SELECT product_id
                                    FROM products
                                    WHERE product_id IN ({placeholders})
                                      AND price IS NOT NULL
                                      AND sales_count IS NOT NULL
                                      AND ({filter_condition})
                                    """,
                                    batch_ids,
                                )
                                existing_product_ids_in_db.update(row[0] for row in cur.fetchall())

                    if len(existing_product_ids_in_db) > 0:
                        logger.info(
                            f"‚úÖ DB Check: Found {len(existing_product_ids_in_db)} valid/recent products to skip"
                        )
                    storage.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra database: {e}")
            logger.info("   S·∫Ω ti·∫øp t·ª•c v·ªõi cache v√† progress file")

        # B·∫Øt ƒë·∫ßu iteration t·ª´ ƒë·∫ßu (Stateless iteration)
        # Thay v√¨ d·ª±a v√†o index, ch√∫ng ta d·ª±a v√†o crawled_product_ids

        # Check Force Crawl flag
        force_crawl = get_variable("TIKI_FORCE_CRAWL", default="false").lower() == "true"
        # Check Ignore Progress flag (Soft Force)
        ignore_progress = get_variable("TIKI_IGNORE_PROGRESS", default="false").lower() == "true"

        if force_crawl:
            logger.warning("üî• FORCE CRAWL: ON")
        elif ignore_progress:
            logger.warning("üîÑ IGNORE PROGRESS: ON (Re-scan list)")

        logger.info(
            f"üîÑ B·∫Øt ƒë·∫ßu ki·ªÉm tra {len(products)} products (ƒë√£ crawl {progress['total_crawled']} products)"
        )

        skipped_count = 0
        products_to_crawl = []

        # Reset counters
        db_hits = 0
        cache_hits = 0
        already_crawled = 0

        # T·ªëi ∆∞u: Duy·ªát t·∫•t c·∫£ products ƒë·ªÉ t√¨m products ch∆∞a c√≥ trong DB
        for idx, product in enumerate(products):
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not product_id or not product_url:
                continue

            # 1. Ki·ªÉm tra xem ƒë√£ crawl ch∆∞a (t·ª´ progress) - Tr·ª´ khi force crawl ho·∫∑c ignore progress
            if (
                not force_crawl
                and not ignore_progress
                and product_id in progress["crawled_product_ids"]
            ):
                already_crawled += 1
                skipped_count += 1
                continue

            # 2. Ki·ªÉm tra xem ƒë√£ c√≥ trong database ch∆∞a (v·ªõi detail ƒë·∫ßy ƒë·ªß) - Tr·ª´ khi force crawl
            if not force_crawl and product_id in existing_product_ids_in_db:
                # ƒê√£ c√≥ trong DB v·ªõi detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count)
                # ‚Üí Skip crawl l·∫°i
                db_hits += 1
                progress["crawled_product_ids"].add(product_id)
                already_crawled += 1
                skipped_count += 1
                continue

            # 3. Ki·ªÉm tra cache v·ªõi Redis (thay v√¨ file cache)
            cache_hit = False

            if not force_crawl and redis_cache:
                # Chu·∫©n h√≥a URL tr∆∞·ªõc khi check cache (CRITICAL)
                product_id_for_cache = product_id

                # Th·ª≠ l·∫•y t·ª´ Redis cache v·ªõi flexible validation
                cached_detail, is_valid = redis_cache.get_product_detail_with_validation(
                    product_id_for_cache
                )

                if is_valid:
                    cache_hits += 1
                    cache_hit = True
                    progress["crawled_product_ids"].add(product_id)
                    already_crawled += 1
                    skipped_count += 1

            # N·∫øu ch∆∞a c√≥ valid cache (ho·∫∑c force crawl), th√™m v√†o danh s√°ch crawl
            if cache_hit:
                continue

            products_to_crawl.append(
                {
                    "product_id": product_id,
                    "url": product_url,
                    "name": product.get("name", ""),
                    "product": product,  # Gi·ªØ nguy√™n product data
                    "index": idx,  # L∆∞u index ƒë·ªÉ track progress (n·∫øu c·∫ßn debug)
                }
            )
            skipped_count = 0  # Reset counter khi t√¨m th·∫•y product m·ªõi

            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products crawl trong ng√†y n√†y
            if len(products_to_crawl) >= products_per_day:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {products_per_day} products cho ng√†y h√¥m nay")
                break

            # Gi·ªõi h·∫°n t·ªïng s·ªë (n·∫øu c√≥)
            if max_products > 0 and len(products_to_crawl) >= max_products:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n t·ªïng {max_products} products")
                break

        # Stats Summary
        total_checked = idx + 1
        cache_hit_rate = (cache_hits / total_checked * 100) if total_checked > 0 else 0.0
        total_skipped = already_crawled

        progress["total_crawled"] = len(progress["crawled_product_ids"])

        logger.info("-" * 30)
        logger.info(
            f"üìä SUMMARY: Input={len(products)} | ToCrawl={len(products_to_crawl)} | Skipped={total_skipped}"
        )
        logger.info(f"   Hits: DB={db_hits}, Cache={cache_hits} ({cache_hit_rate:.1f}%)")
        logger.info(f"   Total Unique Crawled: {progress['total_crawled']}")
        logger.info("-" * 30)

        if len(products_to_crawl) == 0:
            logger.warning(
                f"‚ö†Ô∏è  NO PRODUCTS TO CRAWL! (Check: Progress={not ignore_progress}, DB/Cache={not force_crawl})"
            )
            logger.info(
                "   Hint: Set TIKI_FORCE_CRAWL=true to recrawl all, or TIKI_IGNORE_PROGRESS=true to rescan."
            )

        # L∆∞u progress (list of IDs)
        if products_to_crawl or skipped_count > 0:  # L∆∞u n·∫øu c√≥ thay ƒë·ªïi ho·∫∑c skip
            # Update last_crawled_index is mostly meaningless now, set to last checked
            progress["last_crawled_index"] = idx
            progress["last_updated"] = datetime.now().isoformat()

            # L∆∞u progress v√†o file
            try:
                with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "crawled_product_ids": list(progress["crawled_product_ids"]),
                            "last_crawled_index": progress["last_crawled_index"],
                            "total_crawled": progress["total_crawled"] + already_crawled,
                            "last_updated": progress["last_updated"],
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
                logger.info(f"üíæ ƒê√£ l∆∞u progress: index {progress['last_crawled_index']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng l∆∞u ƒë∆∞·ª£c progress: {e}")

        # Debug: Log m·ªôt v√†i products ƒë·∫ßu ti√™n
        if products_to_crawl:
            sample_names = [p.get("product_id", "N/A") for p in products_to_crawl[:3]]
            logger.info(f"üìã Sample products: {', '.join(sample_names)}...")
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o c·∫ßn crawl detail h√¥m nay!")
            logger.info("üí° T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl ho·∫∑c c√≥ cache h·ª£p l·ªá")

        logger.info(f"üî¢ Tr·∫£ v·ªÅ {len(products_to_crawl)} products cho Dynamic Task Mapping")

        return products_to_crawl

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi prepare products: {e}", exc_info=True)
        raise


def crawl_product_batch(
    product_batch: list[dict[str, Any]] = None, batch_index: int = -1, **context
) -> list[dict[str, Any]]:
    """
    Task: Crawl detail cho m·ªôt batch products (Batch Processing v·ªõi Driver Pooling v√† Async)

    T·ªëi ∆∞u:
    - Batch processing: 10 products/batch
    - Driver pooling: Reuse Selenium drivers trong batch
    - Async/aiohttp: Crawl parallel trong batch
    - Fallback Selenium: N·∫øu aiohttp thi·∫øu sales_count

    Args:
        product_batch: List products trong batch (t·ª´ expand_kwargs)
        batch_index: Index c·ªßa batch
        context: Airflow context

    Returns:
        List[Dict]: List k·∫øt qu·∫£ crawl cho batch
    """
    try:
        logger = get_logger(context)
    except Exception:
        import logging

        logger = logging.getLogger("airflow.task")

    # L·∫•y product_batch t·ª´ op_kwargs n·∫øu ch∆∞a c√≥
    if not product_batch:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_batch = op_kwargs.get("product_batch")
                batch_index = op_kwargs.get("batch_index", -1)

        if not product_batch:
            product_batch = context.get("product_batch") or context.get("op_kwargs", {}).get(
                "product_batch"
            )
            batch_index = context.get("batch_index", -1)

    if not product_batch:
        logger.error(f"‚ùå MISSING PRODUCT_BATCH! Context keys: {list(context.keys())}")
        return []

    # Validate product_batch
    if not isinstance(product_batch, list):
        logger.error(f"‚ùå INVALID BATCH TYPE: {type(product_batch)} (Value: {product_batch})")
        return []

    if len(product_batch) == 0:
        logger.warning(f"‚ö†Ô∏è  BATCH {batch_index} EMPTY")
        return []

    ids_preview = [p.get("product_id", "unknown") for p in product_batch[:3]]
    logger.info(f"üì¶ BATCH {batch_index}: {len(product_batch)} products. IDs={ids_preview}...")

    results = []

    try:
        import asyncio

        # Import SeleniumDriverPool t·ª´ utils n·∫øu ch∆∞a c√≥ (cho task scope)
        global SeleniumDriverPool
        _SeleniumDriverPool = SeleniumDriverPool
        if _SeleniumDriverPool is None:
            # Fallback: th·ª≠ import t·ª´ utils tr·ª±c ti·∫øp n·∫øu kh√¥ng th√†nh c√¥ng
            try:
                _fix_sys_path_for_pipelines_import(logger)
                # utils l√† file (.py), kh√¥ng ph·∫£i package
                import importlib.util

                src_path = Path("/opt/airflow/src")
                if not src_path.exists():
                    src_path = Path(dag_file_dir).parent.parent.parent / "src"
                utils_path = src_path / "pipelines" / "crawl" / "utils.py"
                if utils_path.exists():
                    spec = importlib.util.spec_from_file_location(
                        "crawl_utils_fallback", str(utils_path)
                    )
                    if spec and spec.loader:
                        crawl_utils = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(crawl_utils)
                        _SeleniumDriverPool = getattr(crawl_utils, "SeleniumDriverPool", None)
                if _SeleniumDriverPool:
                    logger.info("‚úÖ Imported SeleniumDriverPool from utils.py file")
                else:
                    raise ImportError("Kh√¥ng t√¨m th·∫•y SeleniumDriverPool trong utils.py")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import SeleniumDriverPool t·ª´ pipelines: {e}")
                raise ImportError("SeleniumDriverPool ch∆∞a ƒë∆∞·ª£c import t·ª´ utils module") from e

        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        # crawl_product_detail_async v√† SeleniumDriverPool ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        pool_size = int(
            get_variable("TIKI_DETAIL_POOL_SIZE", default="2")
        )  # T·ªëi ∆∞u: tƒÉng t·ª´ 5 -> 15
        driver_pool = _SeleniumDriverPool(
            pool_size=pool_size, headless=True, timeout=120
        )  # T·ªëi ∆∞u: tƒÉng t·ª´ 60 -> 120s ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß

        # T·∫°o event loop tr∆∞·ªõc
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Session s·∫Ω ƒë∆∞·ª£c t·∫°o b√™n trong async function (c·∫ßn async context)
        session = None

        async def crawl_single_async(product_info: dict) -> dict[str, Any]:
            """
            Crawl m·ªôt product v·ªõi async.
            """
            product_id = product_info.get("product_id", "unknown")
            product_url = product_info.get("url", "")

            result = {
                "product_id": product_id,
                "url": product_url,
                "status": "failed",
                "error": None,
                "detail": None,
                "crawled_at": datetime.now().isoformat(),
            }

            try:
                # Th·ª≠ async crawl tr∆∞·ªõc
                if session:
                    detail = await crawl_product_detail_async(
                        product_url, session=session, use_selenium_fallback=True, verbose=False
                    )

                    # Ki·ªÉm tra n·∫øu crawl_product_detail_async tr·∫£ v·ªÅ HTML string (do fallback v·ªÅ Selenium)
                    if isinstance(detail, str) and detail.strip().startswith("<"):
                        # Ph√¢n t√≠ch HTML ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i
                        html_preview = detail[:500] if len(detail) > 500 else detail
                        html_lower = detail.lower()

                        # Ki·ªÉm tra c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
                        # Ki·ªÉm tra error page - c·∫ßn ki·ªÉm tra k·ªπ h∆°n ƒë·ªÉ tr√°nh false positive
                        # Error page th∆∞·ªùng c√≥ title ho·∫∑c heading ch·ª©a "404", "not found", etc.
                        is_error_page = False
                        error_keywords = [
                            "404",
                            "not found",
                            "page not found",
                            "500",
                            "internal server error",
                            "403",
                            "forbidden",
                            "access denied",
                        ]
                        # Ch·ªâ coi l√† error page n·∫øu c√≥ keyword trong title ho·∫∑c heading, kh√¥ng ph·∫£i trong to√†n b·ªô HTML
                        # V√¨ m·ªôt s·ªë product c√≥ th·ªÉ c√≥ "404" trong t√™n ho·∫∑c m√¥ t·∫£
                        if any(keyword in html_lower for keyword in error_keywords):
                            # Ki·ªÉm tra trong title tag ho·∫∑c h1 tag (n∆°i th∆∞·ªùng c√≥ error message)
                            title_match = re.search(
                                r"<title[^>]*>(.*?)</title>", html_lower, re.IGNORECASE | re.DOTALL
                            )
                            h1_match = re.search(
                                r"<h1[^>]*>(.*?)</h1>", html_lower, re.IGNORECASE | re.DOTALL
                            )

                            title_text = title_match.group(1) if title_match else ""
                            h1_text = h1_match.group(1) if h1_match else ""

                            # Ch·ªâ coi l√† error n·∫øu keyword xu·∫•t hi·ªán trong title ho·∫∑c h1
                            is_error_page = any(
                                keyword in title_text or keyword in h1_text
                                for keyword in error_keywords
                            )

                        is_captcha = any(
                            keyword in html_lower
                            for keyword in [
                                "captcha",
                                "recaptcha",
                                "cloudflare",
                                "checking your browser",
                            ]
                        )
                        has_next_data = (
                            "__next_data__" in html_lower or 'id="__NEXT_DATA__"' in html_lower
                        )

                        # Ki·ªÉm tra xem c√≥ ph·∫£i l√† HTML b√¨nh th∆∞·ªùng c·ªßa Tiki kh√¥ng
                        is_tiki_page = any(
                            indicator in html_lower
                            for indicator in [
                                "tiki.vn",
                                "tiki",
                                "pdp_product_name",
                                "product-detail",
                                "data-view-id",
                                "pdp-product",
                            ]
                        )

                        if is_error_page:
                            logger.warning(
                                f"‚ö†Ô∏è  HTML l√† error page cho product {product_id}: {html_preview[:200]}..."
                            )
                            detail = None
                        elif is_captcha:
                            logger.warning(
                                f"‚ö†Ô∏è  HTML l√† captcha/block page cho product {product_id}"
                            )
                            detail = None
                        elif not is_tiki_page and not has_next_data:
                            # N·∫øu kh√¥ng ph·∫£i Tiki page v√† kh√¥ng c√≥ __NEXT_DATA__, c√≥ th·ªÉ l√† page l·∫°
                            logger.warning(
                                f"‚ö†Ô∏è  HTML kh√¥ng gi·ªëng Tiki product page cho product {product_id}"
                            )
                            logger.warning(f"   - C√≥ __NEXT_DATA__: {has_next_data}")
                            logger.warning(f"   - HTML preview: {html_preview[:300]}...")
                            # V·∫´n th·ª≠ parse, c√≥ th·ªÉ v·∫´n extract ƒë∆∞·ª£c m·ªôt s·ªë th√¥ng tin
                        else:
                            logger.info(
                                f"‚ÑπÔ∏è  crawl_product_detail_async tr·∫£ v·ªÅ HTML (fallback Selenium) cho product {product_id}"
                            )
                            logger.info(f"   - HTML length: {len(detail)} chars")
                            logger.info(f"   - C√≥ __NEXT_DATA__: {has_next_data}")

                            # Parse HTML th√†nh dict
                            try:
                                hierarchy_map = get_hierarchy_map()
                                detail = extract_product_detail(
                                    detail, product_url, verbose=False, hierarchy_map=hierarchy_map
                                )
                                if detail and isinstance(detail, dict):
                                    # Ki·ªÉm tra xem c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin kh√¥ng
                                    has_name = bool(detail.get("name"))
                                    has_price = bool(detail.get("price", {}).get("current_price"))
                                    has_sales = detail.get("sales_count") is not None
                                    logger.info(
                                        f"‚úÖ ƒê√£ parse HTML th√†nh c√¥ng cho product {product_id}"
                                    )
                                    logger.info(
                                        f"   - C√≥ name: {has_name}, c√≥ price: {has_price}, c√≥ sales_count: {has_sales}"
                                    )
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ None ho·∫∑c kh√¥ng ph·∫£i dict cho product {product_id}"
                                    )
                                    detail = None
                            except Exception as parse_error:
                                logger.warning(
                                    f"‚ö†Ô∏è  L·ªói khi parse HTML t·ª´ crawl_product_detail_async: {parse_error}"
                                )
                                logger.debug(f"   HTML preview: {html_preview}")
                                detail = None

                    # ƒê·∫£m b·∫£o detail l√† dict
                    if detail and not isinstance(detail, dict):
                        logger.warning(
                            f"‚ö†Ô∏è  crawl_product_detail_async tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}"
                        )
                        detail = None
                else:
                    # Fallback v·ªÅ Selenium n·∫øu kh√¥ng c√≥ aiohttp
                    # ∆Øu ti√™n d√πng driver pool n·∫øu c√≥
                    html = None
                    try:
                        if "crawl_product_detail_with_driver" in globals() and callable(
                            crawl_product_detail_with_driver
                        ):
                            drv = driver_pool.get_driver()
                            if drv is not None:
                                try:
                                    html = crawl_product_detail_with_driver(
                                        drv,
                                        product_url,
                                        save_html=False,
                                        verbose=False,
                                        timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß
                                        use_redis_cache=True,
                                        use_rate_limiting=True,
                                    )
                                finally:
                                    driver_pool.return_driver(drv)
                    except Exception as pooled_err:
                        logger.warning(f"‚ö†Ô∏è  L·ªói khi d√πng pooled driver: {pooled_err}")
                        html = None

                    # Fallback cu·ªëi c√πng: t·∫°o driver ri√™ng qua h√†m s·∫µn c√≥
                    if html is None:
                        html = crawl_product_detail_with_selenium(
                            product_url,
                            verbose=False,
                            max_retries=2,
                            timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß
                            use_redis_cache=True,
                            use_rate_limiting=True,
                        )
                    if html:
                        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
                        hierarchy_map = get_hierarchy_map()
                        detail = extract_product_detail(
                            html, product_url, verbose=False, hierarchy_map=hierarchy_map
                        )

                        # Ki·ªÉm tra n·∫øu extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict
                        if isinstance(detail, str) and detail.strip().startswith("<"):
                            logger.warning(
                                f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict cho product {product_id}, th·ª≠ parse l·∫°i"
                            )
                            # Th·ª≠ parse l·∫°i HTML
                            try:
                                detail = extract_product_detail(
                                    html, product_url, verbose=False, hierarchy_map=hierarchy_map
                                )
                            except Exception as parse_error:
                                logger.warning(f"‚ö†Ô∏è  L·ªói khi parse l·∫°i HTML: {parse_error}")
                                detail = None

                        # ƒê·∫£m b·∫£o detail l√† dict, kh√¥ng ph·∫£i HTML string
                        if not isinstance(detail, dict):
                            logger.warning(
                                f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}"
                            )
                            detail = None
                    else:
                        detail = None

                if detail and isinstance(detail, dict):
                    result["detail"] = detail
                    result["status"] = "success"
                else:
                    result["error"] = "Kh√¥ng th·ªÉ crawl detail ho·∫∑c extract detail kh√¥ng h·ª£p l·ªá"
                    result["status"] = "failed"

            except Exception as e:
                result["error"] = str(e)
                result["status"] = "failed"
                logger.warning(f"‚ö†Ô∏è  L·ªói khi crawl product {product_id}: {e}")

            return result

        # Crawl t·∫•t c·∫£ products trong batch song song v·ªõi async
        # (Event loop ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n)
        # S·ª≠ d·ª•ng asyncio.gather() ƒë·ªÉ crawl parallel
        rate_limit_delay = float(get_variable("TIKI_DETAIL_RATE_LIMIT_DELAY", default="0.1"))

        # T·∫°o semaphore ƒë·ªÉ limit concurrent tasks (t·ªëi ∆∞u throughput)
        max_concurrent = int(get_variable("TIKI_DETAIL_MAX_CONCURRENT_TASKS", default="12"))
        semaphore = asyncio.Semaphore(max_concurrent)

        async def bounded_task(task_coro):
            """
            Wrap task ƒë·ªÉ respect semaphore limit.
            """
            async with semaphore:
                return await task_coro

        # T·∫°o tasks v·ªõi rate limiting: stagger start times
        async def crawl_batch_parallel():
            """
            Crawl batch v·ªõi parallel processing v√† rate limiting.
            """
            # T·∫°o session ngay l·∫≠p t·ª©c trong async context (tr∆∞·ªõc khi t·∫°o tasks)
            # ƒê·∫£m b·∫£o session ƒë∆∞·ª£c t·∫°o trong async context c√≥ event loop
            nonlocal session
            if session is None:
                try:
                    import aiohttp

                    timeout = aiohttp.ClientTimeout(total=30)
                    # T·∫°o connector v·ªõi optimized pooling (s·ª≠ d·ª•ng config)
                    # ƒê·∫£m b·∫£o sys.path ƒë∆∞·ª£c c·∫•u h√¨nh tr∆∞·ªõc khi import
                    src_path = Path("/opt/airflow/src")
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))

                    from pipelines.crawl.config import (
                        HTTP_CONNECTOR_LIMIT,
                        HTTP_CONNECTOR_LIMIT_PER_HOST,
                        HTTP_DNS_CACHE_TTL,
                    )

                    connector = aiohttp.TCPConnector(
                        limit=HTTP_CONNECTOR_LIMIT,  # S·ª≠ d·ª•ng config (150)
                        limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,  # S·ª≠ d·ª•ng config (15)
                        ttl_dns_cache=HTTP_DNS_CACHE_TTL,  # S·ª≠ d·ª•ng config (1800s = 30 min)
                        force_close=False,  # Keep connections alive for reuse
                        enable_cleanup_closed=True,
                    )
                    # T·∫°o session trong async context (c√≥ event loop ƒëang ch·∫°y)
                    # ƒê√¢y l√† async function n√™n event loop ƒë√£ c√≥ s·∫µn
                    session = aiohttp.ClientSession(
                        timeout=timeout,
                        connector=connector,
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        },
                    )
                    logger.info("‚úÖ ƒê√£ t·∫°o aiohttp session trong async context")
                except RuntimeError as e:
                    # L·ªói "no running event loop" - fallback v·ªÅ Selenium
                    logger.warning(
                        f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session (no event loop): {e}, s·∫Ω d√πng Selenium"
                    )
                    session = None
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session: {e}, s·∫Ω d√πng Selenium")
                    session = None

            # Factory function ƒë·ªÉ tr√°nh closure issue
            def create_crawl_task(product_info, delay_value):
                async def crawl_with_delay():
                    if delay_value > 0:
                        await asyncio.sleep(delay_value)
                    return await crawl_single_async(product_info)

                return crawl_with_delay()

            tasks = []
            for i, product in enumerate(product_batch):
                delay = i * rate_limit_delay / len(product_batch)  # Ph√¢n t√°n delay
                task = create_crawl_task(product, delay)
                # Wrap v·ªõi bounded_task ƒë·ªÉ respect semaphore
                bounded = bounded_task(task)
                tasks.append(bounded)

            # Ch·∫°y t·∫•t c·∫£ tasks song song (limited b·ªüi semaphore)
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # X·ª≠ l√Ω exceptions
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    product_info = product_batch[i]
                    processed_results.append(
                        {
                            "product_id": product_info.get("product_id", "unknown"),
                            "url": product_info.get("url", ""),
                            "status": "failed",
                            "error": str(result),
                            "detail": None,
                            "crawled_at": datetime.now().isoformat(),
                        }
                    )
                else:
                    processed_results.append(result)

            return processed_results

        results = loop.run_until_complete(crawl_batch_parallel())

        # ƒê√≥ng session
        if session:
            loop.run_until_complete(session.close())

        # Cleanup driver pool
        driver_pool.cleanup()

        # Th·ªëng k√™
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = len(results) - success_count

        logger.info(f"‚úÖ Batch {batch_index} ho√†n th√†nh:")
        logger.info(f"   - Success: {success_count}/{len(product_batch)}")
        logger.info(f"   - Failed: {failed_count}/{len(product_batch)}")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi crawl batch {batch_index}: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ results v·ªõi status failed cho t·∫•t c·∫£
        if product_batch and isinstance(product_batch, list):
            for product_info in product_batch:
                results.append(
                    {
                        "product_id": product_info.get("product_id", "unknown"),
                        "url": product_info.get("url", ""),
                        "status": "failed",
                        "error": f"Batch error: {str(e)}",
                        "detail": None,
                        "crawled_at": datetime.now().isoformat(),
                    }
                )
        else:
            logger.error("‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o failed results v√¨ product_batch kh√¥ng h·ª£p l·ªá")

    return results


def crawl_single_product_detail(product_info: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task: Crawl detail cho m·ªôt product (Dynamic Task Mapping)

    T·ªëi ∆∞u:
    - S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Rate limiting
    - Error handling: ti·∫øp t·ª•c v·ªõi product kh√°c khi l·ªói
    - Atomic write cache

    Args:
        product_info: Th√¥ng tin product (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi detail v√† metadata
    """
    # Kh·ªüi t·∫°o result m·∫∑c ƒë·ªãnh
    default_result = {
        "product_id": "unknown",
        "url": "",
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    try:
        logger = get_logger(context)
    except Exception as e:
        # N·∫øu kh√¥ng th·ªÉ t·∫°o logger, v·∫´n ti·∫øp t·ª•c v·ªõi default result
        import logging

        logger = logging.getLogger("airflow.task")
        logger.error(f"Kh√¥ng th·ªÉ t·∫°o logger t·ª´ context: {e}")

    # L·∫•y product_info t·ª´ keyword argument ho·∫∑c context
    if not product_info:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_info = op_kwargs.get("product_info")

        if not product_info:
            product_info = context.get("product_info") or context.get("op_kwargs", {}).get(
                "product_info"
            )

    if not product_info:
        logger.error(f"Kh√¥ng t√¨m th·∫•y product_info. Context keys: {list(context.keys())}")
        # Return result v·ªõi status failed thay v√¨ raise exception
        return {
            "product_id": "unknown",
            "url": "",
            "status": "failed",
            "error": "Kh√¥ng t√¨m th·∫•y product_info trong context",
            "detail": None,
            "crawled_at": datetime.now().isoformat(),
        }

    product_id = product_info.get("product_id", "")
    product_url = product_info.get("url", "")
    product_name = product_info.get("name", "Unknown")

    logger.info(f"üîç TASK: Crawl Product '{product_name}' | ID: {product_id} | URL: {product_url}")

    result = {
        "product_id": product_id,
        "url": product_url,
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    # Ki·ªÉm tra cache tr∆∞·ªõc - ∆∞u ti√™n Redis, fallback v·ªÅ file
    # Ki·ªÉm tra xem c√≥ force refresh kh√¥ng (t·ª´ Airflow Variable)
    force_refresh = get_variable("TIKI_FORCE_REFRESH_CACHE", default="false").lower() == "true"

    if force_refresh:
        logger.info(f"üîÑ FORCE REFRESH MODE: B·ªè qua cache cho product {product_id}")
    else:
        # Th·ª≠ Redis cache tr∆∞·ªõc (nhanh h∆°n, distributed)
        logger.info(f"üîç ƒêang ki·ªÉm tra cache cho product {product_id}...")
        redis_cache = None
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_detail = redis_cache.get_cached_product_detail(product_id)
                if cached_detail:
                    has_price = cached_detail.get("price", {}).get("current_price")
                    has_sales_count = cached_detail.get("sales_count") is not None
                    brand_value = cached_detail.get("brand")
                    has_brand = bool(
                        brand_value and (not isinstance(brand_value, str) or brand_value.strip())
                    )
                    seller_info = cached_detail.get("seller", {})
                    seller_name = seller_info.get("name") if isinstance(seller_info, dict) else None
                    has_seller = bool(
                        seller_name and (not isinstance(seller_name, str) or seller_name.strip())
                    )

                    if has_price and has_sales_count and has_brand and has_seller:
                        logger.info("=" * 70)
                        logger.info(f"‚úÖ SKIP CRAWL - Redis Cache Hit cho product {product_id}")
                        logger.info(f"   - C√≥ price: {has_price}")
                        logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                        logger.info(f"   - C√≥ brand: {has_brand}")
                        logger.info(f"   - C√≥ seller: {has_seller}")
                        logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                        logger.info("=" * 70)
                        result["detail"] = cached_detail
                        result["status"] = "cached"
                        return result
                    if has_price or has_sales_count or has_brand or has_seller:
                        logger.info(
                            f"[Redis Cache] ‚ö†Ô∏è  Cache thi·∫øu d·ªØ li·ªáu cho product {product_id}, s·∫Ω crawl l·∫°i"
                        )
                else:
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache kh√¥ng ƒë·∫ßy ƒë·ªß cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
        except Exception:
            # Redis kh√¥ng available, fallback v·ªÅ file cache
            pass

        # Fallback: Ki·ªÉm tra file cache n·∫øu Redis kh√¥ng available ho·∫∑c kh√¥ng c√≥ cache
        if not force_refresh:
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None
                        brand_value = cached_detail.get("brand")
                        has_brand = bool(
                            brand_value
                            and (not isinstance(brand_value, str) or brand_value.strip())
                        )
                        seller_info = cached_detail.get("seller", {})
                        seller_name = (
                            seller_info.get("name") if isinstance(seller_info, dict) else None
                        )
                        has_seller = bool(
                            seller_name
                            and (not isinstance(seller_name, str) or seller_name.strip())
                        )

                        if has_price and has_sales_count and has_brand and has_seller:
                            logger.info("=" * 70)
                            logger.info(f"‚úÖ SKIP CRAWL - File Cache Hit cho product {product_id}")
                            logger.info(f"   - C√≥ price: {has_price}")
                            logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                            logger.info(f"   - C√≥ brand: {has_brand}")
                            logger.info(f"   - C√≥ seller: {has_seller}")
                            logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                            logger.info("=" * 70)
                            result["detail"] = cached_detail
                            result["status"] = "cached"
                            return result
                except Exception:
                    # File cache l·ªói, ti·∫øp t·ª•c crawl
                    pass

    # Ti·∫øp t·ª•c crawl n·∫øu kh√¥ng c√≥ cache ho·∫∑c force refresh
    # (File cache check ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü tr√™n trong else block)

    # B·∫Øt ƒë·∫ßu crawl product detail
    try:
        # Lazily initialize expensive singletons only when the task runs
        ensure_output_dirs()
        tiki_degradation = get_tiki_degradation()
        tiki_circuit_breaker = get_tiki_circuit_breaker()
        tiki_dlq = get_tiki_dlq()

        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            logger.warning("=" * 70)
            logger.warning(f"‚ö†Ô∏è  SKIP CRAWL - Service Degraded cho product {product_id}")
            logger.warning("   - Service ƒëang ·ªü tr·∫°ng th√°i FAILED")
            logger.warning("   - Graceful degradation: skip crawl ƒë·ªÉ tr√°nh l√†m t·ªá h∆°n")
            logger.warning("=" * 70)
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            return result

        # Validate URL
        if not product_url or not product_url.startswith("http"):
            raise ValueError(f"URL kh√¥ng h·ª£p l·ªá: {product_url}")

        # L·∫•y c·∫•u h√¨nh
        rate_limit_delay = float(
            get_variable("TIKI_DETAIL_RATE_LIMIT_DELAY", default="0.1")
        )  # Delay 1.5s cho detail (t·ªëi ∆∞u t·ª´ 2.0s)
        timeout = int(
            get_variable("TIKI_DETAIL_CRAWL_TIMEOUT", default="180")
        )  # 3 ph√∫t m·ªói product (tƒÉng t·ª´ 120s ƒë·ªÉ tr√°nh timeout)

        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        # S·ª≠ d·ª•ng Selenium ƒë·ªÉ crawl detail (c·∫ßn thi·∫øt cho dynamic content)
        html_content = None
        try:
            # Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker
            def _crawl_detail_with_params():
                """
                Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker.
                """
                return crawl_product_detail_with_selenium(
                    product_url,
                    save_html=False,
                    verbose=False,  # Kh√¥ng verbose trong Airflow
                    max_retries=3,  # Retry 3 l·∫ßn (tƒÉng t·ª´ 2)
                    timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ ƒë·ªß th·ªùi gian cho trang load ƒë·∫ßy ƒë·ªß
                    use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                    use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
                )

            try:
                # G·ªçi v·ªõi circuit breaker
                html_content = tiki_circuit_breaker.call(_crawl_detail_with_params)
                tiki_degradation.record_success()
            except CircuitBreakerOpenError as e:
                # Circuit breaker ƒëang m·ªü
                result["error"] = f"Circuit breaker open: {str(e)}"
                result["status"] = "circuit_breaker_open"
                logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho product {product_id}: {e}")
                # Th√™m v√†o DLQ
                try:
                    crawl_error = classify_error(
                        e, context={"product_url": product_url, "product_id": product_id}
                    )
                    tiki_dlq.add(
                        task_id=f"crawl_detail_{product_id}",
                        task_type="crawl_product_detail",
                        error=crawl_error,
                        context={
                            "product_url": product_url,
                            "product_name": product_name,
                            "product_id": product_id,
                        },
                        retry_count=0,
                    )
                    logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
                except Exception as dlq_error:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
                return result
            except Exception:
                # Ghi nh·∫≠n failure
                tiki_degradation.record_failure()
                raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

            if not html_content or len(html_content) < 100:
                raise ValueError(
                    f"HTML content qu√° ng·∫Øn ho·∫∑c r·ªóng: {len(html_content) if html_content else 0} k√Ω t·ª±"
                )

        except Exception as selenium_error:
            # Log l·ªói Selenium chi ti·∫øt
            error_type = type(selenium_error).__name__
            error_msg = str(selenium_error)

            # R√∫t g·ªçn error message n·∫øu qu√° d√†i
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."

            logger.error(f"‚ùå L·ªói Selenium ({error_type}): {error_msg}")

            # Ki·ªÉm tra c√°c l·ªói ph·ªï bi·∫øn v√† ph√¢n lo·∫°i
            error_msg_lower = error_msg.lower()
            if (
                "chrome" in error_msg_lower
                or "driver" in error_msg_lower
                or "webdriver" in error_msg_lower
            ):
                result["error"] = f"Chrome/Driver error: {error_msg}"
                result["status"] = "selenium_error"
            elif (
                "timeout" in error_msg_lower
                or "timed out" in error_msg_lower
                or "time-out" in error_msg_lower
            ):
                result["error"] = f"Timeout: {error_msg}"
                result["status"] = "timeout"
            elif (
                "connection" in error_msg_lower
                or "network" in error_msg_lower
                or "refused" in error_msg_lower
            ):
                result["error"] = f"Network error: {error_msg}"
                result["status"] = "network_error"
            elif "memory" in error_msg_lower or "out of memory" in error_msg_lower:
                result["error"] = f"Memory error: {error_msg}"
                result["status"] = "memory_error"
            else:
                result["error"] = f"Selenium error: {error_msg}"
                result["status"] = "failed"

            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    selenium_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            # Kh√¥ng raise, return result v·ªõi status failed
            return result

        # Extract detail
        try:
            hierarchy_map = get_hierarchy_map()
            detail = extract_product_detail(
                html_content, product_url, verbose=False, hierarchy_map=hierarchy_map
            )

            if not detail:
                raise ValueError("Kh√¥ng extract ƒë∆∞·ª£c detail t·ª´ HTML")

        except Exception as extract_error:
            error_type = type(extract_error).__name__
            error_msg = str(extract_error)
            logger.error(f"‚ùå L·ªói khi extract detail ({error_type}): {error_msg}")
            result["error"] = f"Extract error: {error_msg}"
            result["status"] = "extract_error"
            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    extract_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(
                f"Crawl detail v∆∞·ª£t qu√° timeout {timeout}s (elapsed: {elapsed:.1f}s)"
            )

        result["detail"] = detail
        result["status"] = "success"
        result["elapsed_time"] = elapsed

        # L∆∞u v√†o cache - ∆∞u ti√™n Redis, fallback v·ªÅ file
        # Redis cache (nhanh, distributed) - CRITICAL: Chu·∫©n h√≥a URL tr∆∞·ªõc khi cache
        if redis_cache:
            try:
                # IMPORTANT: S·ª≠ d·ª•ng product_id (kh√¥ng ph·ª• thu·ªôc v√†o URL) ƒë·ªÉ cache
                # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o r·∫±ng c√πng 1 product t·ª´ category kh√°c nhau s·∫Ω hit cache
                redis_cache.cache_product_detail(product_id, detail, ttl=604800)  # 7 ng√†y
                logger.info(
                    f"[Redis Cache] ‚úÖ ƒê√£ cache detail cho product {product_id} (TTL: 7 days)"
                )
            except Exception as e:
                logger.warning(f"[Redis Cache] ‚ö†Ô∏è  L·ªói khi cache v√†o Redis: {e}")

        # File cache (fallback)
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

            temp_file = cache_file.with_suffix(".tmp")
            logger.debug(f"üíæ ƒêang l∆∞u cache v√†o: {cache_file}")

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)

            # Atomic move
            if os.name == "nt":  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))

            # Verify cache file was created
            if cache_file.exists():
                logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {elapsed:.1f}s, ƒë√£ cache v√†o {cache_file}")
                # Log sales_count n·∫øu c√≥
                if detail.get("sales_count") is not None:
                    logger.info(f"   üìä sales_count: {detail.get('sales_count')}")
                else:
                    logger.warning("   ‚ö†Ô∏è  sales_count: None (kh√¥ng t√¨m th·∫•y)")
            else:
                logger.error(f"‚ùå Cache file kh√¥ng ƒë∆∞·ª£c t·∫°o: {cache_file}")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng l∆∞u ƒë∆∞·ª£c cache: {e}", exc_info=True)
            # Kh√¥ng fail task v√¨ ƒë√£ crawl th√†nh c√¥ng, ch·ªâ kh√¥ng l∆∞u ƒë∆∞·ª£c cache

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except ValueError as e:
        result["error"] = str(e)
        result["status"] = "validation_error"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå Validation error: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        error_type = type(e).__name__
        logger.error(f"‚ùå L·ªói khi crawl detail ({error_type}): {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi product kh√°c

    # ƒê·∫£m b·∫£o lu√¥n return result, kh√¥ng bao gi·ªù raise exception
    # Ki·ªÉm tra result c√≥ h·ª£p l·ªá kh√¥ng tr∆∞·ªõc khi return
    if not result or not isinstance(result, dict):
        logger.warning("‚ö†Ô∏è  Result kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng default_result")
        result = default_result.copy()
        result["error"] = "Result kh√¥ng h·ª£p l·ªá"
        result["status"] = "failed"

    # ƒê·∫£m b·∫£o result c√≥ ƒë·∫ßy ƒë·ªß c√°c field c·∫ßn thi·∫øt
    if "product_id" not in result:
        result["product_id"] = product_id if "product_id" in locals() else "unknown"
    if "url" not in result:
        result["url"] = product_url if "product_url" in locals() else ""
    if "status" not in result:
        result["status"] = "failed"
    if "crawled_at" not in result:
        result["crawled_at"] = datetime.now().isoformat()

    try:
        return result
    except Exception as e:
        # N·∫øu c√≥ l·ªói khi return (kh√¥ng th·ªÉ x·∫£y ra nh∆∞ng ƒë·ªÉ an to√†n)
        logger.error(f"‚ùå L·ªói khi return result: {e}", exc_info=True)
        default_result["error"] = f"L·ªói khi return result: {str(e)}"
        default_result["product_id"] = product_id if "product_id" in locals() else "unknown"
        default_result["url"] = product_url if "product_url" in locals() else ""
        return default_result


def merge_product_details(**context) -> dict[str, Any]:
    """
    Task: Merge product details v√†o products list

    Returns:
        Dict: Products v·ªõi detail ƒë√£ merge
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Product Details")
    logger.info("=" * 70)

    try:
        ensure_output_dirs()
        ti = context["ti"]

        # L·∫•y products g·ªëc
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file
            if OUTPUT_FILE.exists():
                import json as json_module  # noqa: F401

                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json_module.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")

        # L·∫•y s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl t·ª´ prepare_products_for_detail
        # ƒê√¢y l√† s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø, kh√¥ng ph·∫£i t·ªïng s·ªë products
        products_to_crawl = None
        try:
            products_to_crawl = ti.xcom_pull(
                task_ids="crawl_product_details.prepare_products_for_detail"
            )
        except Exception:
            try:
                products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
            except Exception:
                pass

        # S·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl
        expected_products_count = len(products_to_crawl) if products_to_crawl else 0
        # V·ªõi batch processing, s·ªë map_index = s·ªë batches, kh√¥ng ph·∫£i s·ªë products
        # L·∫•y batch size t·ª´ config
        try:
            from pipelines.crawl.config import PRODUCT_BATCH_SIZE

            batch_size = PRODUCT_BATCH_SIZE
        except Exception:
            batch_size = 12  # Default fallback
        expected_crawl_count = (
            (expected_products_count + batch_size - 1) // batch_size
            if expected_products_count > 0
            else 0
        )
        logger.info(
            f"üìä S·ªë products: {expected_products_count}, S·ªë batches d·ª± ki·∫øn: {expected_crawl_count}"
        )

        # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø c√≥ s·∫µn b·∫±ng c√°ch th·ª≠ l·∫•y XCom
        # ƒêi·ªÅu n√†y gi√∫p x·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt s·ªë tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong
        actual_crawl_count = expected_crawl_count
        if expected_crawl_count > 0:
            # Th·ª≠ l·∫•y XCom t·ª´ map_index cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø
            # T√¨m map_index cao nh·∫•t c√≥ XCom
            task_id = "crawl_product_details.crawl_product_detail"
            max_found_index = -1

            # Binary search ƒë·ªÉ t√¨m map_index cao nh·∫•t c√≥ XCom (t·ªëi ∆∞u h∆°n linear search)
            # Th·ª≠ m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max index
            logger.info(
                f"üîç ƒêang ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø (d·ª± ki·∫øn: {expected_crawl_count})..."
            )
            test_indices = []
            if expected_crawl_count > 1000:
                # V·ªõi s·ªë l∆∞·ª£ng l·ªõn, test m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max
                step = max(100, expected_crawl_count // 20)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            elif expected_crawl_count > 100:
                # V·ªõi s·ªë l∆∞·ª£ng trung b√¨nh, test nhi·ªÅu ƒëi·ªÉm h∆°n
                step = max(50, expected_crawl_count // 10)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            else:
                # V·ªõi s·ªë l∆∞·ª£ng nh·ªè, test t·∫•t c·∫£
                test_indices = list(range(expected_crawl_count))

            # T√¨m t·ª´ cu·ªëi v·ªÅ ƒë·∫ßu ƒë·ªÉ t√¨m max index nhanh h∆°n
            for test_idx in reversed(test_indices):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[test_idx]
                    )
                    if result:
                        max_found_index = test_idx
                        logger.info(f"‚úÖ T√¨m th·∫•y XCom t·∫°i map_index {test_idx}")
                        break
                except Exception as e:
                    logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {test_idx}: {e}")

            if max_found_index >= 0:
                # T√¨m ch√≠nh x√°c map_index cao nh·∫•t b·∫±ng c√°ch t√¨m t·ª´ max_found_index
                # Ch·ªâ th·ª≠ th√™m t·ªëi ƒëa 200 map_index ti·∫øp theo ƒë·ªÉ tr√°nh qu√° l√¢u
                logger.info(f"üîç ƒêang t√¨m ch√≠nh x√°c max index t·ª´ {max_found_index}...")
                search_range = min(max_found_index + 200, expected_crawl_count)
                for idx in range(max_found_index + 1, search_range):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[idx]
                        )
                        if result:
                            max_found_index = idx
                        else:
                            # N·∫øu kh√¥ng c√≥ result, d·ª´ng l·∫°i (c√≥ th·ªÉ ƒë√£ ƒë·∫øn cu·ªëi)
                            break
                    except Exception as e:
                        # N·∫øu exception, c√≥ th·ªÉ l√† h·∫øt map_index
                        logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {idx}: {e}")
                        break

                actual_crawl_count = max_found_index + 1
                logger.info(
                    f"‚úÖ Ph√°t hi·ªán {actual_crawl_count} map_index th·ª±c t·∫ø c√≥ XCom (d·ª± ki·∫øn: {expected_crawl_count})"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y XCom n√†o, s·ª≠ d·ª•ng expected_crawl_count: {expected_crawl_count}. "
                    f"C√≥ th·ªÉ t·∫•t c·∫£ tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong."
                )
                actual_crawl_count = expected_crawl_count

        if actual_crawl_count == 0:
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  KH√îNG C√ì PRODUCTS N√ÄO ƒê∆Ø·ª¢C CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("üí° Nguy√™n nh√¢n c√≥ th·ªÉ:")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong cache v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl tr∆∞·ªõc ƒë√≥ (t·ª´ progress file)")
            logger.warning("   - Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c prepare ƒë·ªÉ crawl")
            logger.warning("=" * 70)
            logger.warning("üí° ƒê·ªÉ force crawl l·∫°i, ki·ªÉm tra task 'prepare_products_for_detail' log")
            logger.warning("=" * 70)
            # Tr·∫£ v·ªÅ products g·ªëc kh√¥ng c√≥ detail
            return {
                "products": products,
                "stats": {
                    "total_products": len(products),
                    "with_detail": 0,
                    "cached": 0,
                    "failed": 0,
                    "timeout": 0,
                    "crawled_count": 0,
                },
                "merged_at": datetime.now().isoformat(),
            }

        # L·∫•y detail results t·ª´ Dynamic Task Mapping
        task_id = "crawl_product_details.crawl_product_detail"
        all_detail_results = []

        # L·∫•y t·∫•t c·∫£ results b·∫±ng c√°ch l·∫•y t·ª´ng map_index ƒë·ªÉ tr√°nh gi·ªõi h·∫°n XCom
        # CH·ªà l·∫•y t·ª´ map_index 0 ƒë·∫øn actual_crawl_count - 1 (kh√¥ng ph·∫£i len(products))
        # Fetch detail results from crawled products

        # L·∫•y theo batch ƒë·ªÉ t·ªëi ∆∞u
        batch_size = 100
        total_batches = (actual_crawl_count + batch_size - 1) // batch_size
        logger.info(
            f"üì¶ S·∫Ω l·∫•y {actual_crawl_count} results trong {total_batches} batches (m·ªói batch {batch_size})"
        )

        for batch_num, start_idx in enumerate(range(0, actual_crawl_count, batch_size), 1):
            end_idx = min(start_idx + batch_size, actual_crawl_count)
            batch_map_indexes = list(range(start_idx, end_idx))

            # Heartbeat: log m·ªói batch ƒë·ªÉ Airflow bi·∫øt task v·∫´n ƒëang ch·∫°y
            if batch_num % 5 == 0 or batch_num == 1:
                logger.info(
                    f"üíì [Heartbeat] ƒêang x·ª≠ l√Ω batch {batch_num}/{total_batches} (index {start_idx}-{end_idx - 1})..."
                )

            try:
                batch_results = ti.xcom_pull(
                    task_ids=task_id, key="return_value", map_indexes=batch_map_indexes
                )

                if batch_results:
                    if isinstance(batch_results, list):
                        # List results theo th·ª© t·ª± map_indexes
                        # M·ªói result c√≥ th·ªÉ l√† list (t·ª´ batch) ho·∫∑c dict (t·ª´ single)
                        for result in batch_results:
                            if result:
                                if isinstance(result, list):
                                    # Batch result: flatten list of results
                                    all_detail_results.extend([r for r in result if r])
                                elif isinstance(result, dict):
                                    # Single result
                                    all_detail_results.append(result)
                    elif isinstance(batch_results, dict):
                        # Dict v·ªõi key l√† map_index ho·∫∑c string
                        # L·∫•y t·∫•t c·∫£ values, s·∫Øp x·∫øp theo map_index n·∫øu c√≥ th·ªÉ
                        for value in batch_results.values():
                            if value:
                                if isinstance(value, list):
                                    # Batch result: flatten
                                    all_detail_results.extend([r for r in value if r])
                                elif isinstance(value, dict):
                                    # Single result
                                    all_detail_results.append(value)
                    else:
                        # Single result
                        if isinstance(batch_results, list):
                            # Batch result: flatten
                            all_detail_results.extend([r for r in batch_results if r])
                        else:
                            all_detail_results.append(batch_results)

                # Log progress m·ªói 5 batches ho·∫∑c m·ªói 10% progress
                if batch_num % max(5, total_batches // 10) == 0:
                    progress_pct = (
                        (len(all_detail_results) / actual_crawl_count * 100)
                        if actual_crawl_count > 0
                        else 0
                    )
                    logger.info(
                        f"üìä ƒê√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%)..."
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi l·∫•y batch {start_idx}-{end_idx}: {e}")
                logger.warning("   S·∫Ω th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y...")
                # Th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y
                for map_index in batch_map_indexes:
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )
                        if result:
                            if isinstance(result, list):
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                    except Exception as e2:
                        # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e2}")

        logger.info(
            f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(all_detail_results)} detail results qua batch (mong ƒë·ª£i {actual_crawl_count})"
        )

        # N·∫øu kh√¥ng l·∫•y ƒë·ªß ho·∫∑c c√≥ l·ªói khi l·∫•y batch, th·ª≠ l·∫•y t·ª´ng map_index m·ªôt ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu
        # KH√îNG reset all_detail_results, ch·ªâ l·∫•y th√™m nh·ªØng map_index ch∆∞a c√≥
        if len(all_detail_results) < actual_crawl_count * 0.8:  # N·∫øu thi·∫øu h∆°n 20%
            # Log c·∫£nh b√°o n·∫øu thi·∫øu nhi·ªÅu
            missing_pct = (
                ((actual_crawl_count - len(all_detail_results)) / actual_crawl_count * 100)
                if actual_crawl_count > 0
                else 0
            )
            if missing_pct > 30:
                logger.warning(
                    f"‚ö†Ô∏è  Thi·∫øu {missing_pct:.1f}% results ({actual_crawl_count - len(all_detail_results)}/{actual_crawl_count}), "
                    f"c√≥ th·ªÉ do nhi·ªÅu tasks failed ho·∫∑c timeout"
                )
            logger.warning(
                f"‚ö†Ô∏è  Ch·ªâ l·∫•y ƒë∆∞·ª£c {len(all_detail_results)}/{actual_crawl_count} results qua batch, "
                f"th·ª≠ l·∫•y t·ª´ng map_index ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu..."
            )

            # T·∫°o set c√°c product_id ƒë√£ c√≥ ƒë·ªÉ tr√°nh duplicate
            existing_product_ids = set()
            for result in all_detail_results:
                if isinstance(result, dict) and result.get("product_id"):
                    existing_product_ids.add(result.get("product_id"))

            missing_count = actual_crawl_count - len(all_detail_results)
            logger.info(
                f"üìä C·∫ßn l·∫•y th√™m ~{missing_count} results t·ª´ {actual_crawl_count} map_indexes"
            )

            # Heartbeat: log th∆∞·ªùng xuy√™n trong v√≤ng l·∫∑p d√†i
            fetched_count = 0
            for map_index in range(actual_crawl_count):  # CH·ªà l·∫•y t·ª´ 0 ƒë·∫øn actual_crawl_count - 1
                # Heartbeat m·ªói 100 items ƒë·ªÉ tr√°nh timeout
                if map_index % 100 == 0 and map_index > 0:
                    logger.info(
                        f"üíì [Heartbeat] ƒêang l·∫•y t·ª´ng map_index: {map_index}/{actual_crawl_count} "
                        f"(ƒë√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count})..."
                    )

                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[map_index]
                    )
                    if result:
                        # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ (tr√°nh duplicate)
                        product_id_to_check = None
                        if isinstance(result, dict):
                            product_id_to_check = result.get("product_id")
                        elif (
                            isinstance(result, list)
                            and len(result) > 0
                            and isinstance(result[0], dict)
                        ):
                            product_id_to_check = result[0].get("product_id")

                        # Ch·ªâ th√™m n·∫øu product_id ch∆∞a c√≥ trong danh s√°ch
                        if (
                            not product_id_to_check
                            or product_id_to_check not in existing_product_ids
                        ):
                            if isinstance(result, list):
                                for r in result:
                                    if isinstance(r, dict) and r.get("product_id"):
                                        existing_product_ids.add(r.get("product_id"))
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                if product_id_to_check:
                                    existing_product_ids.add(product_id_to_check)
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                            fetched_count += 1

                    # Log progress m·ªói 200 items
                    if (map_index + 1) % 200 == 0:
                        progress_pct = (
                            (len(all_detail_results) / actual_crawl_count * 100)
                            if actual_crawl_count > 0
                            else 0
                        )
                        logger.info(
                            f"üìä ƒê√£ l·∫•y t·ªïng {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%) t·ª´ng map_index..."
                        )
                except Exception as e:
                    # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                    logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e}")

            logger.info(
                f"‚úÖ Sau khi l·∫•y t·ª´ng map_index: t·ªïng {len(all_detail_results)} detail results (l·∫•y th√™m {fetched_count})"
            )

        # T·∫°o dict ƒë·ªÉ lookup nhanh
        detail_dict = {}
        stats = {
            "total_products": len(products),
            "crawled_count": 0,  # S·ªë l∆∞·ª£ng products th·ª±c s·ª± ƒë∆∞·ª£c crawl detail
            "with_detail": 0,
            "cached": 0,
            "failed": 0,
            "timeout": 0,
            "degraded": 0,
            "circuit_breaker_open": 0,
        }

        logger.info(f"üìä ƒêang x·ª≠ l√Ω {len(all_detail_results)} detail results...")

        # Ki·ªÉm tra n·∫øu c√≥ qu√° nhi·ªÅu k·∫øt qu·∫£ None ho·∫∑c invalid
        valid_results = 0
        error_details = {}  # Th·ªëng k√™ chi ti·∫øt c√°c lo·∫°i l·ªói
        failed_products = []  # Danh s√°ch products b·ªã fail ƒë·ªÉ ph√¢n t√≠ch

        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get("product_id")
                if product_id:
                    detail_dict[product_id] = detail_result
                    status = detail_result.get("status", "failed")
                    error = detail_result.get("error")

                    # ƒê·∫øm s·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl (t·∫•t c·∫£ c√°c status tr·ª´ "not_crawled")
                    if status in [
                        "success",
                        "failed",
                        "timeout",
                        "degraded",
                        "circuit_breaker_open",
                        "selenium_error",
                        "network_error",
                        "extract_error",
                        "validation_error",
                        "memory_error",
                    ]:
                        stats["crawled_count"] += 1

                    if status == "success":
                        stats["with_detail"] += 1
                    elif status == "cached":
                        stats["cached"] += 1
                    elif status == "timeout":
                        stats["timeout"] += 1
                        error_details["timeout"] = error_details.get("timeout", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "degraded":
                        stats["degraded"] += 1
                        error_details["degraded"] = error_details.get("degraded", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "circuit_breaker_open":
                        stats["circuit_breaker_open"] += 1
                        error_details["circuit_breaker_open"] = (
                            error_details.get("circuit_breaker_open", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "selenium_error":
                        stats["failed"] += 1
                        error_details["selenium_error"] = error_details.get("selenium_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "extract_error":
                        stats["failed"] += 1
                        error_details["extract_error"] = error_details.get("extract_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "network_error":
                        stats["failed"] += 1
                        error_details["network_error"] = error_details.get("network_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "memory_error":
                        stats["failed"] += 1
                        error_details["memory_error"] = error_details.get("memory_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "validation_error":
                        stats["failed"] += 1
                        error_details["validation_error"] = (
                            error_details.get("validation_error", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    else:
                        stats["failed"] += 1
                        error_type = status if status else "unknown"
                        error_details[error_type] = error_details.get(error_type, 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )

        logger.info(
            f"üìä C√≥ {valid_results} detail results h·ª£p l·ªá t·ª´ {len(all_detail_results)} results"
        )

        if valid_results < len(all_detail_results):
            logger.warning(
                f"‚ö†Ô∏è  C√≥ {len(all_detail_results) - valid_results} results kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu product_id"
            )

        # Log chi ti·∫øt v·ªÅ c√°c l·ªói
        if error_details:
            logger.info("=" * 70)
            logger.info("üìã PH√ÇN T√çCH C√ÅC LO·∫†I L·ªñI")
            logger.info("=" * 70)
            for error_type, count in sorted(
                error_details.items(), key=lambda x: x[1], reverse=True
            ):
                logger.info(f"  ‚ùå {error_type}: {count} products")
            logger.info("=" * 70)

            # Log m·ªôt s·ªë products b·ªã fail ƒë·∫ßu ti√™n ƒë·ªÉ ph√¢n t√≠ch
            if failed_products:
                logger.info(f"üìù M·∫´u {min(10, len(failed_products))} products b·ªã fail ƒë·∫ßu ti√™n:")
                for i, failed in enumerate(failed_products[:10], 1):
                    logger.info(
                        f"  {i}. Product ID: {failed['product_id']}, Status: {failed['status']}, Error: {failed.get('error', 'N/A')[:100]}"
                    )

        # L∆∞u th√¥ng tin l·ªói v√†o stats ƒë·ªÉ ph√¢n t√≠ch sau
        stats["error_details"] = error_details
        stats["failed_products_count"] = len(failed_products)

        # Merge detail v√†o products
        # CH·ªà l∆∞u products c√≥ detail V√Ä status == "success" (kh√¥ng l∆∞u cached ho·∫∑c failed)
        products_with_detail = []
        products_without_detail = 0
        products_cached = 0
        products_failed = 0
        products_no_brand = 0  # ƒê·∫øm s·ªë products b·ªã lo·∫°i b·ªè v√¨ brand null

        for product in products:
            product_id = product.get("product_id")
            detail_result = detail_dict.get(product_id)

            if detail_result and detail_result.get("detail"):
                status = detail_result.get("status", "failed")

                # CH·ªà l∆∞u products c√≥ status == "success" (ƒë√£ crawl th√†nh c√¥ng, kh√¥ng ph·∫£i t·ª´ cache)
                if status == "success":
                    # Merge detail v√†o product
                    detail = detail_result["detail"]

                    # Ki·ªÉm tra n·∫øu detail l√† None ho·∫∑c r·ªóng
                    if detail is None:
                        logger.warning(f"‚ö†Ô∏è  Detail l√† None cho product {product_id}")
                        products_failed += 1
                        continue

                    # Ki·ªÉm tra n·∫øu detail l√† string (JSON), parse n√≥
                    if isinstance(detail, str):
                        # B·ªè qua string r·ªóng
                        if not detail.strip():
                            logger.warning(f"‚ö†Ô∏è  Detail l√† string r·ªóng cho product {product_id}")
                            products_failed += 1
                            continue

                        try:
                            import json

                            detail = json.loads(detail)
                        except (json.JSONDecodeError, TypeError) as e:
                            logger.warning(
                                f"‚ö†Ô∏è  Kh√¥ng th·ªÉ parse detail JSON cho product {product_id}: {e}, detail type: {type(detail)}, detail value: {str(detail)[:100]}"
                            )
                            products_failed += 1
                            continue

                    # Ki·ªÉm tra n·∫øu detail kh√¥ng ph·∫£i l√† dict
                    if not isinstance(detail, dict):
                        logger.warning(
                            f"‚ö†Ô∏è  Detail kh√¥ng ph·∫£i l√† dict cho product {product_id}: {type(detail)}, value: {str(detail)[:100]}"
                        )
                        products_failed += 1
                        continue

                    product_with_detail = {**product}

                    # Update c√°c tr∆∞·ªùng t·ª´ detail
                    if detail.get("price"):
                        product_with_detail["price"] = detail["price"]
                    if detail.get("rating"):
                        product_with_detail["rating"] = detail["rating"]
                    if detail.get("description"):
                        product_with_detail["description"] = detail["description"]
                    if detail.get("specifications"):
                        product_with_detail["specifications"] = detail["specifications"]
                    if detail.get("images"):
                        product_with_detail["images"] = detail["images"]
                    if detail.get("brand"):
                        product_with_detail["brand"] = detail["brand"]
                    if detail.get("seller"):
                        product_with_detail["seller"] = detail["seller"]
                    if detail.get("stock"):
                        product_with_detail["stock"] = detail["stock"]
                    if detail.get("shipping"):
                        product_with_detail["shipping"] = detail["shipping"]
                    # C·∫≠p nh·∫≠t sales_count: ∆∞u ti√™n t·ª´ detail, n·∫øu kh√¥ng c√≥ th√¨ d√πng t·ª´ product g·ªëc
                    # Ch·ªâ c·∫ßn c√≥ trong m·ªôt trong hai l√† ƒë·ªß
                    if detail.get("sales_count") is not None:
                        product_with_detail["sales_count"] = detail["sales_count"]
                    elif product.get("sales_count") is not None:
                        product_with_detail["sales_count"] = product["sales_count"]
                    # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng c√≥, gi·ªØ None (ƒë√£ c√≥ trong product g·ªëc)

                    # Th√™m metadata
                    product_with_detail["detail_crawled_at"] = detail_result.get("crawled_at")
                    product_with_detail["detail_status"] = status
                    detail_metadata = detail.get("_metadata")
                    if detail_metadata:
                        product_with_detail["_metadata"] = detail_metadata
                        product_with_detail["_metadata"]["crawl_status"] = status
                        if detail_result.get("crawled_at"):
                            product_with_detail["_metadata"]["completed_at"] = detail_result.get(
                                "crawled_at"
                            )
                    elif status or detail_result.get("crawled_at"):
                        product_with_detail["_metadata"] = {
                            "crawl_status": status,
                            "completed_at": detail_result.get("crawled_at"),
                        }

                    # CRITICAL: L·ªçc b·ªè products c√≥ brand null/empty
                    # Brand thi·∫øu th∆∞·ªùng d·∫´n ƒë·∫øn nhi·ªÅu tr∆∞·ªùng kh√°c c≈©ng thi·∫øu
                    # Seller c√≥ th·ªÉ l√† "Unknown" - v·∫´n l∆∞u l·∫°i
                    # Nh·ªØng products n√†y s·∫Ω ƒë∆∞·ª£c crawl l·∫°i trong l·∫ßn ch·∫°y ti·∫øp theo
                    brand = product_with_detail.get("brand")

                    # Only skip if BRAND is missing/empty (seller can be "Unknown")
                    if not brand or (isinstance(brand, str) and not brand.strip()):
                        logger.warning(
                            f"‚ö†Ô∏è  Product {product_id} ({product_with_detail.get('name', 'Unknown')[:50]}) "
                            f"c√≥ brand null/empty, s·∫Ω b·ªè qua ƒë·ªÉ crawl l·∫°i l·∫ßn sau"
                        )
                        products_no_brand += 1
                        products_failed += 1
                        continue

                    products_with_detail.append(product_with_detail)
                elif status == "cached":
                    # Kh√¥ng l∆∞u products t·ª´ cache (ch·ªâ l∆∞u products ƒë√£ crawl m·ªõi)
                    products_cached += 1
                else:
                    # Kh√¥ng l∆∞u products b·ªã fail
                    products_failed += 1
            else:
                # Kh√¥ng l∆∞u products kh√¥ng c√≥ detail
                products_without_detail += 1

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä MERGE DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products ban ƒë·∫ßu: {stats['total_products']}")
        logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {stats['crawled_count']}")
        logger.info(f"‚úÖ C√≥ detail (success): {stats['with_detail']}")
        logger.info(f"üì¶ C√≥ detail (cached): {stats['cached']}")
        logger.info(f"‚ö†Ô∏è  Degraded: {stats['degraded']}")
        logger.info(f"‚ö° Circuit breaker open: {stats['circuit_breaker_open']}")
        logger.info(f"‚ùå Failed: {stats['failed']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout']}")

        # T√≠nh t·ªïng c√≥ detail (success + cached)
        total_with_detail = stats["with_detail"] + stats["cached"]

        # T·ª∑ l·ªá th√†nh c√¥ng d·ª±a tr√™n s·ªë l∆∞·ª£ng ƒë∆∞·ª£c crawl (quan tr·ªçng h∆°n)
        if stats["crawled_count"] > 0:
            success_rate = (stats["with_detail"] / stats["crawled_count"]) * 100
            logger.info(
                f"üìà T·ª∑ l·ªá th√†nh c√¥ng (d·ª±a tr√™n crawled): {stats['with_detail']}/{stats['crawled_count']} ({success_rate:.1f}%)"
            )

        # T·ª∑ l·ªá c√≥ detail trong t·ªïng products (ƒë·ªÉ tham kh·∫£o)
        if stats["total_products"] > 0:
            detail_coverage = total_with_detail / stats["total_products"] * 100
            logger.info(
                f"üìä T·ª∑ l·ªá c√≥ detail (trong t·ªïng products): {total_with_detail}/{stats['total_products']} ({detail_coverage:.1f}%)"
            )

        logger.info("=" * 70)
        logger.info(
            f"üíæ Products ƒë∆∞·ª£c l∆∞u v√†o file: {len(products_with_detail)} (ch·ªâ l∆∞u products c√≥ status='success')"
        )
        logger.info(f"üì¶ Products t·ª´ cache (ƒë√£ b·ªè qua): {products_cached}")
        logger.info(f"‚ùå Products b·ªã fail (ƒë√£ b·ªè qua): {products_failed}")
        logger.info(f"üö´ Products kh√¥ng c√≥ brand (ƒë√£ b·ªè qua ƒë·ªÉ crawl l·∫°i): {products_no_brand}")
        logger.info(f"üö´ Products kh√¥ng c√≥ detail (ƒë√£ b·ªè qua): {products_without_detail}")
        logger.info("=" * 70)

        # C·∫£nh b√°o n·∫øu c√≥ nhi·ªÅu products kh√¥ng c√≥ brand (>10% total products)
        if products_no_brand > 0 and stats["total_products"] > 0:
            no_brand_rate = (products_no_brand / stats["total_products"]) * 100
            if no_brand_rate > 10:
                logger.warning("=" * 70)
                logger.warning(
                    f"‚ö†Ô∏è  C·∫¢NH B√ÅO: C√≥ {products_no_brand} products ({no_brand_rate:.1f}%) kh√¥ng c√≥ brand!"
                )
                logger.warning("   Nh·ªØng products n√†y s·∫Ω ƒë∆∞·ª£c crawl l·∫°i trong l·∫ßn ch·∫°y ti·∫øp theo.")
                logger.warning("   Nguy√™n nh√¢n c√≥ th·ªÉ:")
                logger.warning("   - Trang detail kh√¥ng load ƒë·∫ßy ƒë·ªß (network issue, timeout)")
                logger.warning("   - HTML structure thay ƒë·ªïi (c·∫ßn update selector)")
                logger.warning("   - Rate limit qu√° cao (c·∫ßn gi·∫£m TIKI_DETAIL_RATE_LIMIT_DELAY)")
                logger.warning("=" * 70)
            elif no_brand_rate > 0:
                logger.info(
                    f"üí° C√≥ {products_no_brand} products ({no_brand_rate:.1f}%) kh√¥ng c√≥ brand, s·∫Ω crawl l·∫°i l·∫ßn sau"
                )

        # C·∫≠p nh·∫≠t stats ƒë·ªÉ ph·∫£n √°nh s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c l∆∞u
        stats["products_saved"] = len(products_with_detail)
        stats["products_skipped"] = products_without_detail
        stats["products_cached_skipped"] = products_cached
        stats["products_failed_skipped"] = products_failed
        stats["products_no_brand_skipped"] = products_no_brand

        result = {
            "products": products_with_detail,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
            "note": f"Ch·ªâ l∆∞u {len(products_with_detail)} products c√≥ status='success' v√† brand/seller kh√¥ng null (ƒë√£ b·ªè qua {products_cached} cached, {products_failed} failed, {products_no_brand} kh√¥ng c√≥ brand, {products_without_detail} kh√¥ng c√≥ detail)",
        }

        return result

    except ValueError as e:
        logger.error(f"‚ùå Validation error khi merge details: {e}", exc_info=True)
        # N·∫øu l√† validation error (thi·∫øu products), return empty result thay v√¨ raise
        return {
            "products": [],
            "stats": {
                "total_products": 0,
                "crawled_count": 0,  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
                "with_detail": 0,
                "cached": 0,
                "failed": 0,
                "timeout": 0,
            },
            "merged_at": datetime.now().isoformat(),
            "error": str(e),
        }
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge details: {e}", exc_info=True)
        # Log chi ti·∫øt context ƒë·ªÉ debug
        logger.error(f"   Context keys: {list(context.keys()) if context else 'None'}")
        try:
            ti = context.get("ti")
            if ti:
                logger.error(f"   Task ID: {ti.task_id}, DAG ID: {ti.dag_id}, Run ID: {ti.run_id}")
        except Exception:
            pass
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: L∆∞u products v·ªõi detail v√†o file

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products with Detail")
    logger.info("=" * 70)

    try:
        ensure_output_dirs()
        ti = context["ti"]

        # L·∫•y k·∫øt qu·∫£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="crawl_product_details.merge_product_details")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_product_details")
            except Exception:
                pass

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y merge result t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})
        note = merge_result.get("note", "Crawl t·ª´ Airflow DAG v·ªõi product details")

        logger.info(f"üíæ ƒêang l∆∞u {len(products)} products v·ªõi detail...")

        # Log th√¥ng tin v·ªÅ crawl detail
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")

        if stats.get("products_skipped"):
            logger.info(f"üö´ ƒê√£ b·ªè qua {stats.get('products_skipped')} products kh√¥ng c√≥ detail")

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": note,
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} products v·ªõi detail v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products with detail: {e}", exc_info=True)
        raise
