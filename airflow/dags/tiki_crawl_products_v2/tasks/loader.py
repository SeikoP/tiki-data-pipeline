from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from ..bootstrap import (
    CATEGORIES_FILE,
    DATA_DIR,
    Any,
    dag_file_dir,
    get_load_categories_db_func,
    get_variable,
    json,
    os,
    sys,
)
from .common import (
    _fix_sys_path_for_pipelines_import,  # noqa: F401
    get_logger,  # noqa: F401
)


def fix_missing_parent_categories(**context) -> dict[str, Any]:
    """Fix missing parent categories v√† rebuild category_path ƒë·∫ßy ƒë·ªß.

    Logic t·ª´ scripts/imp/fix_missing_parents.py
    """
    logger = get_logger(context)

    try:
        import json
        import re

        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        logger.info("=" * 70)
        logger.info("üîß FIX MISSING PARENT CATEGORIES")
        logger.info("=" * 70)

        # 1. Load file JSON categories
        json_file = str(CATEGORIES_FILE)
        if not os.path.exists(json_file):
            logger.warning(
                f"‚ö†Ô∏è  File categories kh√¥ng t·ªìn t·∫°i: {json_file}, b·ªè qua fix missing parents"
            )
            return {"status": "skipped", "message": "File not found", "fixed_count": 0}

        logger.info(f"üìÇ ƒêang ƒë·ªçc file: {json_file}")
        with open(json_file, encoding="utf-8") as f:
            categories = json.load(f)

        url_to_cat = {cat.get("url"): cat for cat in categories}
        logger.info(f"üìä Loaded {len(categories)} categories t·ª´ file JSON")

        # 2. T√¨m c√°c parent categories c√≤n thi·∫øu trong DB
        storage = PostgresStorage()

        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                # L·∫•y t·∫•t c·∫£ categories trong DB
                cur.execute("SELECT url, parent_url FROM categories")
                db_cats = cur.fetchall()
                db_urls = {cat[0] for cat in db_cats}

                # T√¨m c√°c parent URLs c·∫ßn thi·∫øt
                missing_parents = set()
                for db_cat in db_cats:
                    parent_url = db_cat[1] if len(db_cat) > 1 else None
                    if parent_url and parent_url not in db_urls and parent_url in url_to_cat:
                        missing_parents.add(parent_url)

                if not missing_parents:
                    logger.info("‚úÖ Kh√¥ng c√≥ parent categories n√†o c√≤n thi·∫øu!")
                    return {"status": "success", "fixed_count": 0}

                logger.info(f"üîç T√¨m th·∫•y {len(missing_parents)} parent categories c√≤n thi·∫øu")

                # 3. Load c√°c parent categories c√≤n thi·∫øu
                def normalize_category_id(cat_id):
                    if not cat_id:
                        return None
                    if isinstance(cat_id, int):
                        return f"c{cat_id}"
                    cat_id_str = str(cat_id).strip()
                    if cat_id_str.startswith("c"):
                        return cat_id_str
                    return f"c{cat_id_str}"

                saved_count = 0
                for url in missing_parents:
                    cat = url_to_cat[url]

                    # Extract category_id
                    cat_id = cat.get("category_id")
                    if not cat_id and url:
                        match = re.search(r"c?(\d+)", url)
                        if match:
                            cat_id = match.group(1)
                    cat_id = normalize_category_id(cat_id)

                    # Build parent chain ƒë·ªÉ c√≥ category_path
                    path = []
                    current = cat
                    visited = set()
                    depth = 0
                    while current and depth < 10:
                        if current.get("url") in visited:
                            break
                        visited.add(current.get("url"))
                        name = current.get("name", "")
                        if name:
                            path.insert(0, name)
                        parent_url = current.get("parent_url")
                        if not parent_url:
                            break
                        if parent_url in url_to_cat:
                            current = url_to_cat[parent_url]
                        elif parent_url in db_urls:
                            # Query t·ª´ DB
                            cur.execute(
                                "SELECT name, url, parent_url FROM categories WHERE url = %s",
                                (parent_url,),
                            )
                            row = cur.fetchone()
                            if row:
                                current = {"name": row[0], "url": row[1], "parent_url": row[2]}
                            else:
                                break
                        else:
                            break
                        depth += 1

                    # Insert v√†o DB
                    level_1 = path[0] if len(path) > 0 else None
                    level_2 = path[1] if len(path) > 1 else None
                    level_3 = path[2] if len(path) > 2 else None
                    level_4 = path[3] if len(path) > 3 else None
                    level_5 = path[4] if len(path) > 4 else None
                    calculated_level = len(path) if path else 0
                    root_name = path[0] if path else None

                    # Check if leaf
                    parent_urls_in_db = {c[1] for c in db_cats if len(c) > 1 and c[1]}
                    is_leaf = url not in parent_urls_in_db

                    try:
                        cur.execute(
                            """
                            INSERT INTO categories (
                                category_id, name, url, image_url, parent_url, level,
                                category_path, level_1, level_2, level_3, level_4, level_5,
                                root_category_name, is_leaf
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (url) DO UPDATE SET
                                category_path = EXCLUDED.category_path,
                                level_1 = EXCLUDED.level_1,
                                level_2 = EXCLUDED.level_2,
                                level_3 = EXCLUDED.level_3,
                                level_4 = EXCLUDED.level_4,
                                level_5 = EXCLUDED.level_5,
                                level = EXCLUDED.level,
                                root_category_name = EXCLUDED.root_category_name,
                                updated_at = CURRENT_TIMESTAMP
                        """,
                            (
                                cat_id,
                                cat.get("name"),
                                url,
                                cat.get("image_url"),
                                cat.get("parent_url"),
                                calculated_level,
                                json.dumps(path, ensure_ascii=False),
                                level_1,
                                level_2,
                                level_3,
                                level_4,
                                level_5,
                                root_name,
                                is_leaf,
                            ),
                        )
                        saved_count += 1
                        logger.info(f"   ‚úÖ ƒê√£ load: {cat.get('name')}")
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è  L·ªói khi load {cat.get('name')}: {e}")

                conn.commit()
                logger.info(f"‚úÖ ƒê√£ load {saved_count} parent categories v√†o DB")

        # 4. Rebuild category_path cho t·∫•t c·∫£ categories (sau khi ƒë√≥ng connection)
        if saved_count > 0:
            logger.info("üîß ƒêang rebuild category_path cho t·∫•t c·∫£ categories...")

            # T·∫°o storage m·ªõi ƒë·ªÉ rebuild
            storage_rebuild = PostgresStorage()
            try:
                with storage_rebuild.get_connection() as conn_rebuild:
                    with conn_rebuild.cursor() as cur_rebuild:
                        cur_rebuild.execute("SELECT url FROM categories")
                        all_db_urls = [row[0] for row in cur_rebuild.fetchall()]

                categories_to_rebuild = [
                    url_to_cat[url] for url in all_db_urls if url in url_to_cat
                ]

                if categories_to_rebuild:
                    # Rebuild paths b·∫±ng c√°ch g·ªçi save_categories l·∫°i
                    # (s·∫Ω t·ª± ƒë·ªông rebuild paths v·ªõi logic ƒë√£ ƒë∆∞·ª£c s·ª≠a)
                    rebuild_count = storage_rebuild.save_categories(
                        categories_to_rebuild, only_leaf=False, sync_with_products=False
                    )
                    logger.info(f"‚úÖ ƒê√£ rebuild {rebuild_count} categories")
            finally:
                storage_rebuild.close()

        logger.info("=" * 70)
        return {"status": "success", "fixed_count": saved_count}

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi fix missing parent categories: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "fixed_count": 0}


def load_categories_to_db_wrapper(**context):
    """Task wrapper to load categories from JSON file into PostgreSQL database.

    Sau khi load, t·ª± ƒë·ªông fix missing parent categories v√† rebuild paths.
    """
    logger = get_logger(context)

    try:
        json_file = str(CATEGORIES_FILE)
        if not os.path.exists(json_file):
            logger.error(f"‚ùå File categories kh√¥ng t·ªìn t·∫°i: {json_file}")
            return {"status": "error", "message": "File not found", "count": 0}

        load_categories_db_func = get_load_categories_db_func()
        if not load_categories_db_func:
            logger.error("‚ùå load_categories_db_func not available")
            return {"status": "error", "message": "Import failed"}

        logger.info(f"üöÄ Loading categories to DB from {json_file}")
        load_categories_db_func(json_file)

        # Sau khi load, fix missing parent categories
        logger.info("üîß Fixing missing parent categories...")
        fix_result = fix_missing_parent_categories(**context)

        if fix_result.get("status") == "success":
            fixed_count = fix_result.get("fixed_count", 0)
            if fixed_count > 0:
                logger.info(f"‚úÖ ƒê√£ fix {fixed_count} missing parent categories")
            else:
                logger.info("‚úÖ Kh√¥ng c√≥ parent categories n√†o c·∫ßn fix")
        else:
            logger.warning(
                f"‚ö†Ô∏è  Fix missing parents c√≥ v·∫•n ƒë·ªÅ: {fix_result.get('message', 'Unknown error')}"
            )

        return {"status": "success", "fixed_parents": fix_result.get("fixed_count", 0)}

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories v√†o DB: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def _import_postgres_storage():
    """Helper function ƒë·ªÉ import PostgresStorage v·ªõi fallback logic H·ªó tr·ª£ c·∫£ m√¥i tr∆∞·ªùng Airflow
    (importlib) v√† m√¥i tr∆∞·ªùng b√¨nh th∆∞·ªùng.

    Returns:
        PostgresStorage class ho·∫∑c None n·∫øu kh√¥ng th·ªÉ import
    """
    try:
        # Th·ª≠ import t·ª´ __init__.py c·ªßa storage module
        from pipelines.crawl.storage import PostgresStorage

        return PostgresStorage
    except ImportError:
        try:
            # Th·ª≠ import tr·ª±c ti·∫øp t·ª´ file
            from pipelines.crawl.storage.postgres_storage import PostgresStorage

            return PostgresStorage
        except ImportError:
            try:
                import importlib.util
                from pathlib import Path

                # T√¨m ƒë∆∞·ªùng d·∫´n ƒë·∫øn postgres_storage.py
                possible_paths = [
                    # T·ª´ /opt/airflow/src (Docker default - ∆∞u ti√™n)
                    Path("/opt/airflow/src/pipelines/crawl/storage/postgres_storage.py"),
                    # T·ª´ dag_file_dir
                    Path(dag_file_dir).parent.parent
                    / "src"
                    / "pipelines"
                    / "crawl"
                    / "storage"
                    / "postgres_storage.py",
                    # T·ª´ current working directory
                    Path(os.getcwd())
                    / "src"
                    / "pipelines"
                    / "crawl"
                    / "storage"
                    / "postgres_storage.py",
                    # T·ª´ workspace root
                    Path("/workspace/src/pipelines/crawl/storage/postgres_storage.py"),
                ]

                postgres_storage_path = None
                for path in possible_paths:
                    if path.exists() and path.is_file():
                        postgres_storage_path = path
                        break

                if postgres_storage_path:
                    # S·ª≠ d·ª•ng importlib ƒë·ªÉ load tr·ª±c ti·∫øp t·ª´ file
                    spec = importlib.util.spec_from_file_location(
                        "postgres_storage", postgres_storage_path
                    )
                    if spec and spec.loader:
                        postgres_storage_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(postgres_storage_module)
                        return postgres_storage_module.PostgresStorage

                # N·∫øu kh√¥ng t√¨m th·∫•y file, th·ª≠ th√™m src v√†o path v√† import absolute
                src_paths = [
                    Path("/opt/airflow/src"),
                    Path(dag_file_dir).parent.parent / "src",
                    Path(os.getcwd()) / "src",
                ]

                for src_path in src_paths:
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))
                        try:
                            from pipelines.crawl.storage import PostgresStorage

                            return PostgresStorage
                        except ImportError:
                            try:
                                from pipelines.crawl.storage.postgres_storage import PostgresStorage

                                return PostgresStorage
                            except ImportError:
                                continue

                return None
            except Exception:
                return None


def load_products(**context) -> dict[str, Any]:
    """
    Task: Load d·ªØ li·ªáu ƒë√£ transform v√†o database

    Returns:
        Dict: K·∫øt qu·∫£ load v·ªõi stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Load Products to Database")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y transformed file t·ª´ transform_products task
        transform_result = None
        try:
            transform_result = ti.xcom_pull(task_ids="transform_and_load.transform_products")
        except Exception:
            try:
                transform_result = ti.xcom_pull(task_ids="transform_products")
            except Exception:
                pass

        if not transform_result:
            # Fallback: t√¨m file transformed
            processed_dir = DATA_DIR / "processed"
            transformed_file = processed_dir / "products_transformed.json"
            if transformed_file.exists():
                transform_result = {"transformed_file": str(transformed_file)}
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y transform result t·ª´ XCom ho·∫∑c file")

        transformed_file = transform_result.get("transformed_file")
        if not transformed_file or not os.path.exists(transformed_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file transformed: {transformed_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc transformed file: {transformed_file}")

        # ƒê·ªçc transformed products
        with open(transformed_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products ƒë·ªÉ load: {len(products)}")

        # Import OptimizedDataLoader
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n load module
            # ∆Øu ti√™n load t·ª´ file src/pipelines/load/loader.py
            from pipelines.load.loader import OptimizedDataLoader

            # L·∫•y database config t·ª´ Airflow Variables ho·∫∑c environment variables
            db_host = get_variable("POSTGRES_HOST", default=os.getenv("POSTGRES_HOST", "postgres"))
            db_port = int(get_variable("POSTGRES_PORT", default=os.getenv("POSTGRES_PORT", "5432")))
            db_name = get_variable("POSTGRES_DB", default=os.getenv("POSTGRES_DB", "crawl_data"))
            db_user = get_variable("POSTGRES_USER", default=os.getenv("POSTGRES_USER", "postgres"))
            # trufflehog:ignore
            db_password = get_variable(
                "POSTGRES_PASSWORD", default=os.getenv("POSTGRES_PASSWORD", "postgres")
            )

            # Prepare DB Config for OptimizedDataLoader
            db_config = {
                "host": db_host,
                "port": db_port,
                "database": db_name,
                "user": db_user,
                "password": db_password,
            }

            # Initialize OptimizedDataLoader
            # Note: OptimizedDataLoader uses connection pooling internally
            loader = OptimizedDataLoader(
                batch_size=int(get_variable("TIKI_SAVE_BATCH_SIZE", default=2000)),
                enable_db=True,
                db_config=db_config,
                show_progress=True,
            )

            try:
                # L∆∞u v√†o processed directory
                processed_dir = DATA_DIR / "processed"
                processed_dir.mkdir(parents=True, exist_ok=True)
                final_file = processed_dir / "products_final.json"

                # Kh·ªüi t·∫°o bi·∫øn ƒë·ªÉ l∆∞u s·ªë l∆∞·ª£ng products
                count_before = None
                count_after = None

                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB tr∆∞·ªõc khi load (for stats)
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is not None:
                        storage = PostgresStorage(
                            host=db_host,
                            port=db_port,
                            database=db_name,
                            user=db_user,
                            password=db_password,
                        )
                        with storage.get_connection() as conn:
                            with conn.cursor() as cur:
                                cur.execute("SELECT COUNT(*) FROM products;")
                                count_before = cur.fetchone()[0]
                        storage.close()
                        logger.info(f"üìä S·ªë products trong DB tr∆∞·ªõc khi load: {count_before}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë products trong DB: {e}")
                    count_before = None

                # S·ª≠ d·ª•ng OptimizedDataLoader.load_products
                # Signature: load_products(products, upsert=True, validate_before_load=True, save_to_file=None)
                load_stats = loader.load_products(
                    products, upsert=True, validate_before_load=True, save_to_file=str(final_file)
                )

                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB sau khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is not None:
                        storage = PostgresStorage(
                            host=db_host,
                            port=db_port,
                            database=db_name,
                            user=db_user,
                            password=db_password,
                        )
                        with storage.get_connection() as conn:
                            with conn.cursor() as cur:
                                cur.execute("SELECT COUNT(*) FROM products;")
                                count_after = cur.fetchone()[0]
                        storage.close()
                        logger.info(f"üìä S·ªë products trong DB sau khi load: {count_after}")
                        if count_before is not None:
                            diff = count_after - count_before
                            if diff > 0:
                                logger.info(f"‚úÖ ƒê√£ th√™m {diff} products m·ªõi v√†o DB")
                            elif diff == 0:
                                logger.info(
                                    "‚ÑπÔ∏è  Kh√¥ng c√≥ products m·ªõi (ch·ªâ UPDATE c√°c products ƒë√£ c√≥)"
                                )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë l∆∞·ª£ng products sau khi load: {e}")
                    count_after = None

                logger.info("=" * 70)
                logger.info("üìä LOAD RESULTS (Optimized)")
                logger.info("=" * 70)
                logger.info(f"‚úÖ DB loaded: {load_stats.get('db_loaded', 0)} products")

                inserted = load_stats.get("inserted_count", 0)
                updated = load_stats.get("updated_count", 0)
                if inserted > 0 or updated > 0:
                    logger.info(f"   - INSERT: {inserted}")
                    logger.info(f"   - UPDATE: {updated}")

                logger.info(f"‚úÖ File loaded: {load_stats.get('file_loaded', 0)}")
                logger.info(f"‚ùå Failed: {load_stats.get('failed_count', 0)}")

                if load_stats.get("errors"):
                    logger.warning(
                        f"‚ö†Ô∏è  Errors ({len(load_stats['errors'])}): {load_stats['errors'][:5]}..."
                    )

                return {
                    "final_file": str(final_file),
                    "load_stats": load_stats,
                }

            finally:
                loader.close()

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataLoader: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong load_products task: {e}", exc_info=True)
        raise
