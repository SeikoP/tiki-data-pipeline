from __future__ import annotations

# Import all bootstrap globals (paths, config, dynamic imports, singletons).
# This preserves legacy behavior without renaming any globals referenced by task callables.
from tiki_crawl_products_v2.bootstrap import Any, Path, json, logging, os, shutil, sys


def get_logger(context):
    """
    L·∫•y logger t·ª´ context (Airflow 3.x compatible)
    """
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging

        ti = context.get("task_instance")
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging

        return logging.getLogger("airflow.task")


def _fix_sys_path_for_pipelines_import(logger=None):
    """S·ª≠a sys.path v√† sys.modules ƒë·ªÉ ƒë·∫£m b·∫£o pipelines c√≥ th·ªÉ ƒë∆∞·ª£c import ƒë√∫ng c√°ch.

    X√≥a c√°c ƒë∆∞·ªùng d·∫´n con nh∆∞ /opt/airflow/src/pipelines kh·ªèi sys.path, x√≥a c√°c fake modules kh·ªèi
    sys.modules, v√† ch·ªâ gi·ªØ l·∫°i /opt/airflow/src.
    """

    if logger is None:
        logger = logging.getLogger("airflow.task")

    # X√≥a c√°c fake modules kh·ªèi sys.modules (quan tr·ªçng!)
    # C√°c fake modules n√†y ƒë∆∞·ª£c t·∫°o ·ªü ƒë·∫ßu file v√† g√¢y l·ªói 'pipelines' is not a package
    modules_to_remove = [
        module_name
        for module_name in list(sys.modules.keys())
        if module_name.startswith("pipelines")
    ]

    for module_name in modules_to_remove:
        del sys.modules[module_name]
        if logger:
            logger.info(f"üóëÔ∏è  ƒê√£ x√≥a fake module kh·ªèi sys.modules: {module_name}")

    # X√≥a c√°c ƒë∆∞·ªùng d·∫´n con kh·ªèi sys.path (g√¢y l·ªói 'pipelines' is not a package)
    paths_to_remove = []
    for path in sys.path:
        # X√≥a c√°c ƒë∆∞·ªùng d·∫´n nh∆∞ /opt/airflow/src/pipelines ho·∫∑c /opt/airflow/src/pipelines/crawl
        normalized_path = path.replace("\\", "/")
        if normalized_path.endswith("/pipelines") or normalized_path.endswith("/pipelines/crawl"):
            paths_to_remove.append(path)

    for path in paths_to_remove:
        if path in sys.path:
            sys.path.remove(path)
            if logger:
                logger.info(f"üóëÔ∏è  ƒê√£ x√≥a ƒë∆∞·ªùng d·∫´n sai kh·ªèi sys.path: {path}")

    # ƒê·∫£m b·∫£o /opt/airflow/src c√≥ trong sys.path
    possible_src_paths = [
        "/opt/airflow/src",  # Docker default path
        os.path.abspath(
            os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "..", "src")
        ),  # Local dev
    ]

    for src_path in possible_src_paths:
        if os.path.exists(src_path) and os.path.isdir(src_path):
            if src_path not in sys.path:
                sys.path.insert(0, src_path)
                if logger:
                    logger.info(f"‚úÖ ƒê√£ th√™m v√†o sys.path: {src_path}")
            return src_path

    return None


def atomic_write_file(filepath: str, data: Any, **context):
    """Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt.

    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi v√†o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == "nt":  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))

        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")

    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def get_selenium_driver_pool_class(logger):
    """
    Import SeleniumDriverPool helper to reduce complexity in tasks.
    """
    _SeleniumDriverPool = None

    try:
        # 1. Try standard package import first (preferred)
        _fix_sys_path_for_pipelines_import(logger)
        from pipelines.crawl.utils import SeleniumDriverPool

        _SeleniumDriverPool = SeleniumDriverPool
        logger.info("‚úÖ Imported SeleniumDriverPool from pipelines.crawl.utils")
        return _SeleniumDriverPool
    except ImportError:
        logger.warning("‚ö†Ô∏è Standard import failed, trying file-based import for SeleniumDriverPool")

    try:
        import importlib.util

        src_path = Path("/opt/airflow/src")
        if not src_path.exists():
            # Try local dev path
            src_path = Path(__file__).parent.parent.parent.parent.parent / "src"

        utils_path = src_path / "pipelines" / "crawl" / "utils.py"
        if utils_path.exists():
            spec = importlib.util.spec_from_file_location("crawl_utils_task", str(utils_path))
            if spec and spec.loader:
                utils_mod = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_mod)
                _SeleniumDriverPool = getattr(utils_mod, "SeleniumDriverPool", None)
    except Exception as e:
        logger.error(f"‚ùå Failed to import SeleniumDriverPool: {e}")

    if _SeleniumDriverPool is None:
        raise ImportError("Could not import SeleniumDriverPool from any source")

    return _SeleniumDriverPool
