"""DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn.

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching

Dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator gi·ªØa c√°c tasks.
"""

import json
import logging
import os
import re  # noqa: F401
import shutil  # noqa: F401
import sys
import time  # noqa: F401
import warnings
from datetime import datetime, timedelta
from functools import lru_cache
from pathlib import Path
from threading import Lock
from typing import Any

try:
    import pandas as pd
except ImportError:
    pd = None

try:
    import psycopg2
except ImportError:
    psycopg2 = None

try:
    import aiohttp
except ImportError:
    aiohttp = None

try:
    from airflow import DAG
    from airflow.models import Pool

    # Airflow 3.0+ compatible imports
    try:
        from airflow.sdk import TaskGroup
    except ImportError:
        from airflow.utils.task_group import TaskGroup

    try:
        from airflow.providers.standard.operators.python import PythonOperator
    except ImportError:
        from airflow.operators.python import PythonOperator
except ImportError:
    # Mock for local development without airflow installed
    class DAG:
        def __init__(self, *args, **kwargs):
            pass

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            pass

    class Pool:
        def __init__(self, *args, **kwargs):
            pass

    class PythonOperator:
        def __init__(self, *args, **kwargs):
            self.output = []

        def expand(self, **kwargs):
            return self

        @classmethod
        def partial(cls, *args, **kwargs):
            return cls()

        def __rshift__(self, other):
            return other

        def __lshift__(self, other):
            return self

        def __rrshift__(self, other):
            return self

        def __rlshift__(self, other):
            return self

    class TaskGroup:
        def __init__(self, *args, **kwargs):
            pass

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def __rshift__(self, other):
            return other

        def __lshift__(self, other):
            return self

        def __rrshift__(self, other):
            return self

        def __rlshift__(self, other):
            return self


try:
    from airflow.exceptions import AirflowSkipException
except ImportError:

    class AirflowSkipException(Exception):
        pass


def load_env_file():
    """
    Dummy load_env_file for bootstrap.
    """


# Setup imports path
src_path = Path("/opt/airflow/src")
if src_path.exists() and str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# Fallback for local development
if not src_path.exists():
    local_src = Path(__file__).resolve().parent.parent.parent.parent / "src"
    if local_src.exists() and str(local_src) not in sys.path:
        sys.path.insert(0, str(local_src))

# Import Custom Utilities
try:
    from common.airflow_utils import (
        TaskGroup,
        get_int_variable,
        get_variable,
        load_env_file,
        safe_import_attr,
    )
    from pipelines.crawl.hierarchy import get_hierarchy_map
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Could not import common utilities: {e}")

    # Minimal fallback to allow DAG parse (though it will likely fail run)
    def get_variable(key, default=None):
        return os.getenv(key, default)

    def get_int_variable(key, default=0):
        return int(os.getenv(key, default))

    def load_env_file():
        pass

    class TaskGroup:
        def __init__(self, *args, **kwargs):
            pass

        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def __rshift__(self, other):
            return other

        def __lshift__(self, other):
            return self

        def __rrshift__(self, other):
            return self

        def __rlshift__(self, other):
            return self

    def safe_import_attr(*args, **kwargs):
        return None

    def get_hierarchy_map(force_reload=False):
        return {}


# NOTE: Avoid heavy work at DAG parse time.
# We defer loading .env and other expensive initialization until first use in tasks.
_ENV_LOADED = False


def ensure_env_loaded() -> None:
    global _ENV_LOADED
    if _ENV_LOADED:
        return
    try:
        load_env_file()
    finally:
        _ENV_LOADED = True


# Try to import redis_cache for caching
redis_cache = None
try:
    # try:
    #     from pipelines.crawl.storage.redis_cache import get_redis_cache  # type: ignore
    #     # redis_cache = get_redis_cache("redis://redis:6379/1")  # type: ignore
    # except Exception as import_err:
    #     pass
    pass
except Exception as e:
    warnings.warn(f"Redis cache initialization skipped: {e}", stacklevel=2)
    redis_cache = None


# ƒê∆∞·ªùng d·∫´n c∆° s·ªü c·ªßa file DAG
dag_file_dir = os.path.dirname(__file__)

# C√°c ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ ch·ª©a module crawl
possible_paths = [
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "..", "src", "pipelines", "crawl")),
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")),
    "/opt/airflow/src/pipelines/crawl",
]


def _fix_sys_path_for_pipelines_import(logger=None):
    """S·ª≠a sys.path v√† sys.modules ƒë·ªÉ ƒë·∫£m b·∫£o import ƒë∆∞·ª£c pipelines.

    X·ª≠ l√Ω tr∆∞·ªùng h·ª£p pipelines b·ªã nh·∫≠n nh·∫ßm l√† namespace package ho·∫∑c module kh√¥ng t·ªìn t·∫°i.
    """
    if logger:
        logger.info("üîß Fixing sys.path for pipelines import...")

    # 1. Th√™m src v√†o sys.path n·∫øu ch∆∞a c√≥
    src_path_str = str(src_path)
    if src_path_str not in sys.path:
        sys.path.insert(0, src_path_str)
        if logger:
            logger.info(f"   Added {src_path_str} to sys.path")

    # 2. X√≥a c√°c entries gi·∫£ trong sys.modules (quan tr·ªçng cho reload)
    modules_to_remove = [
        m for m in sys.modules.keys() if m.startswith("pipelines") or m.startswith("common")
    ]
    # Ch·ªâ x√≥a n·∫øu th·ª±c s·ª± c·∫ßn thi·∫øt (optional strategy) - ·ªü ƒë√¢y ta gi·ªØ nguy√™n logic c≈© l√† x√≥a
    # However, aggressive cleaning might break things if imports are partial.
    # Let's trust the adding of src_path to sys.path is enough for now,
    # OR replicate the full logic if we are sure.

    # Full logic from previous viewing:
    # X√≥a c√°c fake modules kh·ªèi sys.modules
    count = 0
    for module_name in modules_to_remove:
        # Check if attribute is None (namespace package without init)
        if sys.modules[module_name] is None:
            del sys.modules[module_name]
            count += 1

    if logger and count > 0:
        logger.info(f"   Cleaned {count} None modules starting with pipelines/common")


def get_logger(context=None):
    """
    Get Airflow task logger or standard logger.
    """
    # In Airflow 3.0+, context["ti"] is a RuntimeTaskInstance which may not have .log
    # Standard way to get the task logger across versions:
    return logging.getLogger("airflow.task")


def atomic_write_file(filepath: str, data: Any, **context):
    """Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt.

    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi v√†o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Rename atomic
        # os.replace is atomic on POSIX and Windows (Python 3.3+)
        os.replace(temp_file, filepath)
        # logger.info(f"‚úÖ ƒê√£ ghi file: {filepath}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ghi file {filepath}: {e}")
        if temp_file.exists():
            try:
                os.remove(temp_file)
            except OSError:
                pass
        raise


# Import pipelines modules
try:
    from pipelines.transform.transformer import DataTransformer
except ImportError:
    DataTransformer = None

try:
    from pipelines.load.optimized_loader import OptimizedDataLoader
except ImportError:
    OptimizedDataLoader = None

try:
    from pipelines.transform.merger import combine_products
except ImportError:
    combine_products = None


# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products_detail.py")
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_products_detail", crawl_products_detail_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_detail_path}")
        crawl_products_detail_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_detail_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_product_detail_with_selenium = (
            crawl_products_detail_module.crawl_product_detail_with_selenium
        )
        extract_product_detail = crawl_products_detail_module.extract_product_detail
        crawl_product_detail_async = crawl_products_detail_module.crawl_product_detail_async
        # Optional pooled-driver variant (present in optimized module)
        crawl_product_detail_with_driver = getattr(
            crawl_products_detail_module, "crawl_product_detail_with_driver", None
        )
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products_detail module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_product_detail_with_selenium(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        def crawl_product_detail_with_driver(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        extract_product_detail = crawl_product_detail_with_selenium

        async def crawl_product_detail_async(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        # Fallback SeleniumDriverPool
        SeleniumDriverPool = None

else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng
    try:
        from crawl_products_detail import (
            crawl_product_detail_with_selenium,
        )

        SeleniumDriverPool = None  # Kh√¥ng c√≥ trong crawl_products_detail, s·∫Ω import t·ª´ utils
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"L·ªói g·ªëc: {e}"
        ) from e

# Import crawl_category_products t·ª´ crawl_products
crawl_products_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_products_path = test_path
        break

if crawl_products_path and os.path.exists(crawl_products_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
        if spec and spec.loader:
            crawl_products_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(crawl_products_module)
            crawl_category_products = getattr(
                crawl_products_module, "crawl_category_products", None
            )
        else:
            crawl_category_products = None
    except Exception as e:
        err_msg = str(e)
        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products module: {err_msg}", stacklevel=2)

        def crawl_category_products(*args, **kwargs):
            raise ImportError(f"Module crawl_products ch∆∞a ƒë∆∞·ª£c import: {err_msg}")
else:
    try:
        from pipelines.crawl.crawl_products import crawl_category_products
    except ImportError:

        def crawl_category_products(*args, **kwargs):
            raise ImportError("Kh√¥ng t√¨m th·∫•y module crawl_products")


# Import SeleniumDriverPool t·ª´ utils ·ªü module level
try:
    # utils l√† file (.py), kh√¥ng ph·∫£i package, n√™n import tr·ª±c ti·∫øp
    import importlib.util
    import sys

    src_path = Path("/opt/airflow/src")
    if src_path.exists() and str(src_path) not in sys.path:
        sys.path.insert(0, str(src_path))

    # Th·ª≠ import t·ª´ pipelines.crawl.utils
    try:
        from pipelines.crawl.utils import SeleniumDriverPool
    except Exception:
        # Fallback: direct import t·ª´ file
        utils_path = src_path / "pipelines" / "crawl" / "utils.py"
        if utils_path.exists():
            spec = importlib.util.spec_from_file_location("crawl_utils", str(utils_path))
            if spec and spec.loader:
                crawl_utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(crawl_utils_module)
                SeleniumDriverPool = getattr(crawl_utils_module, "SeleniumDriverPool", None)
            else:
                SeleniumDriverPool = None
        else:
            SeleniumDriverPool = None
except Exception:
    SeleniumDriverPool = None  # Fallback, s·∫Ω import l·∫°i trong task n·∫øu c·∫ßn

# Import module crawl_categories_batch (for category batch processing)
crawl_categories_batch_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_categories_batch.py")
    if os.path.exists(test_path):
        crawl_categories_batch_path = test_path
        break

if crawl_categories_batch_path and os.path.exists(crawl_categories_batch_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_categories_batch", crawl_categories_batch_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_categories_batch_path}")
        crawl_categories_batch_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_categories_batch_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_category_batch = crawl_categories_batch_module.crawl_category_batch
    except Exception as e:
        # N·∫øu import l·ªói, log v√† t·∫°o fallback (s·∫Ω s·ª≠ d·ª•ng crawl_single_category)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_categories_batch module: {e}", stacklevel=2)

        # T·∫°o dummy function ƒë·ªÉ tr√°nh NameError
        crawl_category_batch = None
else:
    # Module ch∆∞a t·ªìn t·∫°i, s·∫Ω fallback v·ªÅ crawl_single_category
    crawl_category_batch = None

# Import resilience patterns
# Import tr·ª±c ti·∫øp t·ª´ng module con ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ relative imports
resilience_module_path = None
for path in possible_paths:
    test_path = os.path.join(path, "resilience", "__init__.py")
    if os.path.exists(test_path):
        resilience_module_path = os.path.join(path, "resilience")
        break

if resilience_module_path and os.path.exists(resilience_module_path):
    try:
        import importlib.util
        import sys

        # Th√™m parent path (pipelines/crawl) v√†o sys.path
        parent_path = os.path.dirname(resilience_module_path)  # .../crawl
        if parent_path not in sys.path:
            sys.path.insert(0, parent_path)

        # Th√™m grandparent path (pipelines) v√†o sys.path
        grandparent_path = os.path.dirname(parent_path)  # .../pipelines
        if grandparent_path not in sys.path:
            sys.path.insert(0, grandparent_path)

        # Import tr·ª±c ti·∫øp t·ª´ng module con v·ªõi t√™n module ƒë·∫ßy ƒë·ªß
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c module c√≥ th·ªÉ import l·∫´n nhau n·∫øu c·∫ßn

        # T·∫°o package structure trong sys.modules
        import types

        if "pipelines" not in sys.modules:
            sys.modules["pipelines"] = types.ModuleType("pipelines")
        if "pipelines.crawl" not in sys.modules:
            sys.modules["pipelines.crawl"] = types.ModuleType("pipelines.crawl")
        if "pipelines.crawl.resilience" not in sys.modules:
            sys.modules["pipelines.crawl.resilience"] = types.ModuleType(
                "pipelines.crawl.resilience"
            )

        # ƒê·∫£m b·∫£o utils module ƒë√£ ƒë∆∞·ª£c import (c·∫ßn thi·∫øt cho dead_letter_queue)
        # utils module ƒë√£ ƒë∆∞·ª£c import ·ªü tr√™n (d√≤ng 137-156)
        # N·∫øu ch∆∞a c√≥, t·∫°o fake module
        if "pipelines.crawl.utils" not in sys.modules and "crawl_utils" in sys.modules:
            sys.modules["pipelines.crawl.utils"] = sys.modules["crawl_utils"]

        # 1. Import exceptions tr∆∞·ªõc (kh√¥ng c√≥ dependency)
        exceptions_path = os.path.join(resilience_module_path, "exceptions.py")
        if os.path.exists(exceptions_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.exceptions", exceptions_path
            )
            if spec and spec.loader:
                exceptions_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.exceptions"] = exceptions_module
                spec.loader.exec_module(exceptions_module)
                CrawlError = exceptions_module.CrawlError
                classify_error = exceptions_module.classify_error
            else:
                raise ImportError(f"Kh√¥ng th·ªÉ load exceptions module t·ª´ {exceptions_path}")
        else:
            raise ImportError(f"Kh√¥ng t√¨m th·∫•y exceptions.py t·∫°i {exceptions_path}")

        # 2. Import circuit_breaker (kh√¥ng c√≥ dependency)
        circuit_breaker_path = os.path.join(resilience_module_path, "circuit_breaker.py")
        if os.path.exists(circuit_breaker_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.circuit_breaker", circuit_breaker_path
            )
            if spec and spec.loader:
                circuit_breaker_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.circuit_breaker"] = circuit_breaker_module
                spec.loader.exec_module(circuit_breaker_module)
                CircuitBreaker = circuit_breaker_module.CircuitBreaker
                CircuitBreakerOpenError = circuit_breaker_module.CircuitBreakerOpenError
            else:
                raise ImportError("Kh√¥ng th·ªÉ load circuit_breaker module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y circuit_breaker.py")

        # 3. Import dead_letter_queue (c√≥ th·ªÉ import t·ª´ utils)
        dlq_path = os.path.join(resilience_module_path, "dead_letter_queue.py")
        if os.path.exists(dlq_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.dead_letter_queue", dlq_path
            )
            if spec and spec.loader:
                dlq_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.dead_letter_queue"] = dlq_module
                spec.loader.exec_module(dlq_module)
                DeadLetterQueue = dlq_module.DeadLetterQueue
                get_dlq = dlq_module.get_dlq
            else:
                raise ImportError("Kh√¥ng th·ªÉ load dead_letter_queue module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y dead_letter_queue.py")

        # 4. Import graceful_degradation (kh√¥ng c√≥ dependency)
        degradation_path = os.path.join(resilience_module_path, "graceful_degradation.py")
        if os.path.exists(degradation_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.graceful_degradation", degradation_path
            )
            if spec and spec.loader:
                degradation_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.graceful_degradation"] = degradation_module
                spec.loader.exec_module(degradation_module)
                GracefulDegradation = degradation_module.GracefulDegradation
                DegradationLevel = degradation_module.DegradationLevel
                get_service_health = degradation_module.get_service_health
            else:
                raise ImportError("Kh√¥ng th·ªÉ load graceful_degradation module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y graceful_degradation.py")

    except Exception as e:
        # N·∫øu import l·ªói, t·∫°o dummy classes ƒë·ªÉ tr√°nh NameError
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import resilience module: {e}", stacklevel=2)

        # T·∫°o dummy classes
        class CircuitBreaker:
            def __init__(self, *args, **kwargs):
                pass

            def call(self, func, *args, **kwargs):
                return func(*args, **kwargs)

        class CircuitBreakerOpenError(Exception):
            pass

        class DeadLetterQueue:
            def add(self, *args, **kwargs):
                pass

        def get_dlq(*args, **kwargs):
            return DeadLetterQueue()

        class GracefulDegradation:
            def should_skip(self):
                return False

            def record_success(self):
                pass

            def record_failure(self):
                pass

        class DegradationLevel:
            FULL = "full"
            REDUCED = "reduced"
            MINIMAL = "minimal"
            FAILED = "failed"

        def get_service_health():
            return type(
                "ServiceHealth",
                (),
                {"register_service": lambda *args, **kwargs: GracefulDegradation()},
            )()

        def classify_error(error, **kwargs):
            return error

        class CrawlError(Exception):
            pass

else:
    # Fallback: t·∫°o dummy classes
    class CircuitBreaker:
        def __init__(self, *args, **kwargs):
            pass

        def call(self, func, *args, **kwargs):
            return func(*args, **kwargs)

    class CircuitBreakerOpenError(Exception):
        pass

    class DeadLetterQueue:
        def add(self, *args, **kwargs):
            pass

    def get_dlq(*args, **kwargs):
        return DeadLetterQueue()

    class GracefulDegradation:
        def should_skip(self):
            return False

        def record_success(self):
            pass

        def record_failure(self):
            pass

    class DegradationLevel:
        FULL = "full"
        REDUCED = "reduced"
        MINIMAL = "minimal"
        FAILED = "failed"

    def get_service_health():
        return type(
            "ServiceHealth", (), {"register_service": lambda *args, **kwargs: GracefulDegradation()}
        )()

    def classify_error(error, **kwargs):
        return error

    class CrawlError(Exception):
        pass


# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    "owner": "data-team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,  # Disable retries (was 3) - SDK API issues with retries
    "retry_delay": timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    "retry_exponential_backoff": True,  # Exponential backoff
    "max_retry_delay": timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG - C√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa t·ª± ƒë·ªông v√† th·ªß c√¥ng qua Variable
# ƒê·ªçc c·∫•u h√¨nh t·ª´ Airflow Variable/Env
schedule_mode = get_variable("TIKI_DAG_SCHEDULE_MODE", default="manual")
schedule_hours = get_int_variable("TIKI_DAG_SCHEDULE_HOURS", default=1)

# X√°c ƒë·ªãnh schedule d·ª±a tr√™n mode
if schedule_mode == "scheduled":
    # S·ª≠ d·ª•ng timedelta ƒë·ªÉ ƒë·∫£m b·∫£o kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn ch·∫°y
    dag_schedule = timedelta(hours=schedule_hours)
    dag_description = (
        f"Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping (T·ª± ƒë·ªông ch·∫°y: m·ªói {schedule_hours} gi·ªù)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "scheduled"]
else:
    dag_schedule = None  # Ch·ªâ ch·∫°y khi trigger th·ªß c√¥ng
    dag_description = "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping (Ch·∫°y th·ªß c√¥ng - Test mode)"
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "manual"]

# C·∫•u h√¨nh DAG schedule
dag_schedule_config = dag_schedule

# Documentation ƒë∆°n gi·∫£n cho DAG
dag_doc_md = "Crawl s·∫£n ph·∫©m t·ª´ Tiki.vn v·ªõi Dynamic Task Mapping v√† Selenium"

DAG_CONFIG = {
    "dag_id": "tiki_crawl_products_v2",
    "description": dag_description,
    "doc_md": dag_doc_md,
    "default_args": DEFAULT_ARGS,
    "schedule": dag_schedule_config,
    "start_date": datetime(2025, 11, 1),  # Ng√†y c·ªë ƒë·ªãnh trong qu√° kh·ª©
    "catchup": False,  # Kh√¥ng ch·∫°y l·∫°i c√°c task ƒë√£ b·ªè l·ª°
    "tags": dag_tags,
    "max_active_runs": 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    "max_active_tasks": 4,  # T·ªëi ∆∞u cho Local: 4 tasks * 4 drivers = 16 drivers (v·ª´a ƒë·ªß 16GB RAM)
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path("/opt/airflow/data"),  # Docker mount
    Path(__file__).parent.parent.parent / "data",  # Local development
    Path(os.getcwd()) / "data",  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ho·∫∑c AIRFLOW_HOME
    if os.getenv("AIRFLOW_HOME"):
        DATA_DIR = Path(os.getenv("AIRFLOW_HOME")) / "data"
    else:
        DATA_DIR = Path(__file__).parent.parent.parent / "data"

CATEGORIES_FILE = DATA_DIR / "raw" / "categories_recursive_optimized.json"
CATEGORIES_TREE_FILE = DATA_DIR / "raw" / "categories_tree.json"
OUTPUT_DIR = DATA_DIR / "raw" / "products"
CACHE_DIR = OUTPUT_DIR / "cache"
DETAIL_CACHE_DIR = OUTPUT_DIR / "detail" / "cache"
OUTPUT_FILE = OUTPUT_DIR / "products.json"
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / "products_with_detail.json"
# Progress tracking cho multi-day crawling
PROGRESS_FILE = OUTPUT_DIR / "crawl_progress.json"

# Asset/Dataset ƒë√£ ƒë∆∞·ª£c x√≥a - dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator


@lru_cache(maxsize=1)
def ensure_output_dirs() -> None:
    """Ensure output/cache directories exist.

    Previously executed at import time; now lazily executed on first write-related task call.
    """
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)


# Thread-safe lock cho atomic writes
write_lock = Lock()


@lru_cache(maxsize=1)
def get_tiki_circuit_breaker():
    """
    Lazy circuit breaker init to speed up DAG parse.
    """
    ensure_env_loaded()
    return CircuitBreaker(
        failure_threshold=get_int_variable("TIKI_CIRCUIT_BREAKER_FAILURE_THRESHOLD", default=5),
        recovery_timeout=get_int_variable("TIKI_CIRCUIT_BREAKER_RECOVERY_TIMEOUT", default=60),
        expected_exception=Exception,
        name="tiki_api",
    )


@lru_cache(maxsize=1)
def get_tiki_dlq():
    """
    Lazy DLQ init (redis preferred, file fallback).
    """
    ensure_env_loaded()
    try:
        redis_url = get_variable("REDIS_URL", default="redis://redis:6379/3")
        return get_dlq(storage_type="redis", redis_url=redis_url)
    except Exception:
        try:
            dlq_path = DATA_DIR / "dlq"
            return get_dlq(storage_type="file", storage_path=str(dlq_path))
        except Exception:
            return get_dlq()


@lru_cache(maxsize=1)
def get_tiki_degradation():
    """
    Lazy graceful degradation registration.
    """
    ensure_env_loaded()
    service_health = get_service_health()
    return service_health.register_service(
        name="tiki",
        failure_threshold=get_int_variable("TIKI_DEGRADATION_FAILURE_THRESHOLD", default=3),
        recovery_threshold=get_int_variable("TIKI_DEGRADATION_RECOVERY_THRESHOLD", default=5),
    )


# Import modules cho AI summarization v√† Discord notification
# T√¨m c√°c th∆∞ m·ª•c: analytics/, ai/, notifications/ ·ªü common/ (sau src/)
analytics_path = None
ai_path = None
notifications_path = None
config_path = None

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ cho c√°c modules ·ªü common/
common_base_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    "/opt/airflow/src/common",
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "common")),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "common")),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    "/workspace/src/common",
    # T·ª´ current working directory
    os.path.join(os.getcwd(), "src", "common"),
]

for common_base in common_base_paths:
    test_analytics = os.path.join(common_base, "analytics", "aggregator.py")
    test_ai = os.path.join(common_base, "ai", "summarizer.py")
    test_notifications = os.path.join(common_base, "notifications", "discord.py")
    test_config = os.path.join(common_base, "config.py")

    if os.path.exists(test_analytics):
        analytics_path = test_analytics
    if os.path.exists(test_ai):
        ai_path = test_ai
    if os.path.exists(test_notifications):
        notifications_path = test_notifications
    if os.path.exists(test_config):
        config_path = test_config

    if analytics_path and ai_path and notifications_path and config_path:
        break

# Fallback for common_base_paths in local dev
if not (analytics_path and ai_path and notifications_path and config_path):
    project_root = Path(__file__).resolve().parent.parent.parent.parent
    common_base = project_root / "src" / "common"
    if common_base.exists():
        test_analytics = common_base / "analytics" / "aggregator.py"
        test_ai = common_base / "ai" / "summarizer.py"
        test_notifications = common_base / "notifications" / "discord.py"
        test_config = common_base / "config.py"

        if test_analytics.exists():
            analytics_path = str(test_analytics)
        if test_ai.exists():
            ai_path = str(test_ai)
        if test_notifications.exists():
            notifications_path = str(test_notifications)
        if test_config.exists():
            config_path = str(test_config)


def _lazy_import_from_file(module_name: str, file_path: str, attr: str):
    """
    Import attribute from a .py file path lazily.
    """
    import importlib.util

    spec = importlib.util.spec_from_file_location(module_name, file_path)
    if spec is None or spec.loader is None:
        return None
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)
    return getattr(mod, attr, None)


@lru_cache(maxsize=1)
def get_DataAggregator():
    ensure_env_loaded()
    if analytics_path and os.path.exists(analytics_path):
        try:
            return _lazy_import_from_file(
                "common.analytics.aggregator", analytics_path, "DataAggregator"
            )
        except Exception as e:
            warnings.warn(f"Kh√¥ng th·ªÉ import common.analytics.aggregator module: {e}", stacklevel=2)
    return None


@lru_cache(maxsize=1)
def get_load_categories_db_func():
    ensure_env_loaded()
    func = safe_import_attr(
        "pipelines.load.load_categories_to_db",
        "load_categories",
        fallback_paths=[Path(p).parent.parent for p in possible_paths],
    )
    if not func:
        warnings.warn("load_categories_to_db not available; DB load will be skipped", stacklevel=2)
    return func


# Global debug flags controlled via Airflow Variables
DEBUG_LOAD_CATEGORIES = (
    get_variable("TIKI_DEBUG_LOAD_CATEGORIES", default="false").lower() == "true"
)
DEBUG_ENRICH_CATEGORY_PATH = (
    get_variable("TIKI_DEBUG_ENRICH_CATEGORY_PATH", default="false").lower() == "true"
)


@lru_cache(maxsize=1)
def get_AISummarizer():
    ensure_env_loaded()
    if ai_path and os.path.exists(ai_path):
        try:
            return _lazy_import_from_file("common.ai.summarizer", ai_path, "AISummarizer")
        except Exception as e:
            warnings.warn(f"Kh√¥ng th·ªÉ import common.ai.summarizer module: {e}", stacklevel=2)
    return None


@lru_cache(maxsize=1)
def get_DiscordNotifier():
    ensure_env_loaded()
    if notifications_path and os.path.exists(notifications_path):
        try:
            return _lazy_import_from_file(
                "common.notifications.discord", notifications_path, "DiscordNotifier"
            )
        except Exception as e:
            warnings.warn(
                f"Kh√¥ng th·ªÉ import common.notifications.discord module: {e}", stacklevel=2
            )
    return None
