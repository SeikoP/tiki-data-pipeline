"""
DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching

Dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator gi·ªØa c√°c tasks.
"""

import json
import os
import shutil
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator

# Asset/Dataset ƒë√£ ƒë∆∞·ª£c x√≥a v√¨ kh√¥ng c·∫ßn thi·∫øt cho single DAG v√† g√¢y l·ªói v·ªõi PythonOperator

# Import Variable v√† TaskGroup v·ªõi suppress warning
try:
    # Th·ª≠ import t·ª´ airflow.sdk (Airflow 3.x)
    from airflow.sdk import TaskGroup, Variable

    _Variable = Variable  # Alias ƒë·ªÉ d√πng wrapper
except ImportError:
    # Fallback: d√πng airflow.models v√† airflow.utils.task_group (Airflow 2.x)
    try:
        from airflow.utils.task_group import TaskGroup
    except ImportError:
        # N·∫øu kh√¥ng c√≥ TaskGroup, t·∫°o dummy class
        class TaskGroup:
            def __init__(self, *args, **kwargs):
                pass

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

    from airflow.models import Variable as _Variable


# Wrapper function ƒë·ªÉ suppress deprecation warning khi g·ªçi Variable.get()
def get_variable(key, default_var=None):
    """Wrapper cho Variable.get() ƒë·ªÉ suppress deprecation warning"""
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore", category=DeprecationWarning, module="airflow.models.variable"
        )
        return _Variable.get(key, default=default_var)


# Alias Variable ƒë·ªÉ code c≈© v·∫´n ho·∫°t ƒë·ªông, nh∆∞ng d√πng wrapper
class VariableWrapper:
    """Wrapper cho Variable ƒë·ªÉ suppress warnings"""

    @staticmethod
    def get(key, default_var=None):
        return get_variable(key, default_var)

    @staticmethod
    def set(key, value):
        return _Variable.set(key, value)


Variable = VariableWrapper

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o sys.path
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa DAG file
dag_file_dir = os.path.dirname(os.path.abspath(__file__))

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
# Trong Docker, src ƒë∆∞·ª£c mount v√†o /opt/airflow/src
possible_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    "/opt/airflow/src/pipelines/crawl",
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "pipelines", "crawl")),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    "/workspace/src/pipelines/crawl",
    # T·ª´ current working directory
    os.path.join(os.getcwd(), "src", "pipelines", "crawl"),
]

# T√¨m ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá
crawl_module_path = None
crawl_products_path = None

for path in possible_paths:
    test_path = os.path.join(path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = path
        crawl_products_path = test_path
        break

if not crawl_module_path:
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ DAG file
    relative_path = os.path.abspath(
        os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")
    )
    test_path = os.path.join(relative_path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = relative_path
        crawl_products_path = test_path

# Import module utils TR∆Ø·ªöC (c·∫ßn thi·∫øt cho crawl_products v√† crawl_products_detail)
# Kh·ªüi t·∫°o SeleniumDriverPool = None ƒë·ªÉ tr√°nh NameError
SeleniumDriverPool = None

utils_path = None
if crawl_module_path:
    utils_path = os.path.join(crawl_module_path, "utils.py")
    if not os.path.exists(utils_path):
        utils_path = None

if not utils_path:
    # Th·ª≠ t√¨m trong c√°c possible paths
    for path in possible_paths:
        test_path = os.path.join(path, "utils.py")
        if os.path.exists(test_path):
            utils_path = test_path
            break

if utils_path and os.path.exists(utils_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
        if spec and spec.loader:
            utils_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(utils_module)
            # L∆∞u v√†o sys.modules ƒë·ªÉ c√°c module kh√°c c√≥ th·ªÉ import
            sys.modules["crawl_utils"] = utils_module
            # T·∫°o fake package structure ƒë·ªÉ relative import ho·∫°t ƒë·ªông
            if "pipelines.crawl.utils" not in sys.modules:
                sys.modules["pipelines"] = type(sys)("pipelines")
                sys.modules["pipelines.crawl"] = type(sys)("pipelines.crawl")
                sys.modules["pipelines.crawl.utils"] = utils_module
            # Extract SeleniumDriverPool ƒë·ªÉ s·ª≠ d·ª•ng tr·ª±c ti·∫øp
            SeleniumDriverPool = getattr(utils_module, "SeleniumDriverPool", None)
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import utils module: {e}", stacklevel=2)
        SeleniumDriverPool = None

# Import module crawl_products
if crawl_products_path and os.path.exists(crawl_products_path):
    try:
        # S·ª≠ d·ª•ng importlib ƒë·ªÉ import tr·ª±c ti·∫øp t·ª´ file (c√°ch ƒë√°ng tin c·∫≠y nh·∫•t)
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_path}")
        crawl_products_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_category_products = crawl_products_module.crawl_category_products
        get_page_with_requests = crawl_products_module.get_page_with_requests
        parse_products_from_html = crawl_products_module.parse_products_from_html
        get_total_pages = crawl_products_module.get_total_pages
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_category_products(*args, **kwargs):
            raise ImportError(f"Module crawl_products ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        get_page_with_requests = crawl_category_products
        parse_products_from_html = crawl_category_products
        get_total_pages = crawl_category_products
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng n·∫øu ƒë√£ th√™m v√†o sys.path
    if crawl_module_path and crawl_module_path not in sys.path:
        sys.path.insert(0, crawl_module_path)

    try:
        from crawl_products import crawl_category_products
    except ImportError as e:
        # Debug: ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        debug_info = {
            "dag_file_dir": dag_file_dir,
            "cwd": os.getcwd(),
            "possible_paths": possible_paths,
            "crawl_module_path": crawl_module_path,
            "crawl_products_path": crawl_products_path,
            "sys_path": sys.path[:5],  # Ch·ªâ l·∫•y 5 ƒë·∫ßu ti√™n
        }

        # Ki·ªÉm tra xem /opt/airflow/src c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists("/opt/airflow/src"):
            try:
                debug_info["opt_airflow_src_contents"] = os.listdir("/opt/airflow/src")
            except Exception:
                pass

        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products.\n" f"Debug info: {debug_info}\n" f"L·ªói g·ªëc: {e}"
        ) from e

# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products_detail.py")
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_products_detail", crawl_products_detail_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_detail_path}")
        crawl_products_detail_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_detail_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_product_detail_with_selenium = (
            crawl_products_detail_module.crawl_product_detail_with_selenium
        )
        extract_product_detail = crawl_products_detail_module.extract_product_detail
        crawl_product_detail_async = crawl_products_detail_module.crawl_product_detail_async
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products_detail module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_product_detail_with_selenium(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        extract_product_detail = crawl_product_detail_with_selenium
        
        async def crawl_product_detail_async(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng
    try:
        from crawl_products_detail import (
            crawl_product_detail_with_selenium,
            extract_product_detail,
            crawl_product_detail_async,
        )
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"L·ªói g·ªëc: {e}"
        ) from e

# Import resilience patterns
# Import tr·ª±c ti·∫øp t·ª´ng module con ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ relative imports
resilience_module_path = None
for path in possible_paths:
    test_path = os.path.join(path, "resilience", "__init__.py")
    if os.path.exists(test_path):
        resilience_module_path = os.path.join(path, "resilience")
        break

if resilience_module_path and os.path.exists(resilience_module_path):
    try:
        import importlib.util
        import sys

        # Th√™m parent path (pipelines/crawl) v√†o sys.path
        parent_path = os.path.dirname(resilience_module_path)  # .../crawl
        if parent_path not in sys.path:
            sys.path.insert(0, parent_path)

        # Th√™m grandparent path (pipelines) v√†o sys.path
        grandparent_path = os.path.dirname(parent_path)  # .../pipelines
        if grandparent_path not in sys.path:
            sys.path.insert(0, grandparent_path)

        # Import tr·ª±c ti·∫øp t·ª´ng module con v·ªõi t√™n module ƒë·∫ßy ƒë·ªß
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c module c√≥ th·ªÉ import l·∫´n nhau n·∫øu c·∫ßn

        # T·∫°o package structure trong sys.modules
        import types

        if "pipelines" not in sys.modules:
            sys.modules["pipelines"] = types.ModuleType("pipelines")
        if "pipelines.crawl" not in sys.modules:
            sys.modules["pipelines.crawl"] = types.ModuleType("pipelines.crawl")
        if "pipelines.crawl.resilience" not in sys.modules:
            sys.modules["pipelines.crawl.resilience"] = types.ModuleType(
                "pipelines.crawl.resilience"
            )

        # ƒê·∫£m b·∫£o utils module ƒë√£ ƒë∆∞·ª£c import (c·∫ßn thi·∫øt cho dead_letter_queue)
        # utils module ƒë√£ ƒë∆∞·ª£c import ·ªü tr√™n (d√≤ng 137-156)
        # N·∫øu ch∆∞a c√≥, t·∫°o fake module
        if "pipelines.crawl.utils" not in sys.modules and "crawl_utils" in sys.modules:
            sys.modules["pipelines.crawl.utils"] = sys.modules["crawl_utils"]

        # 1. Import exceptions tr∆∞·ªõc (kh√¥ng c√≥ dependency)
        exceptions_path = os.path.join(resilience_module_path, "exceptions.py")
        if os.path.exists(exceptions_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.exceptions", exceptions_path
            )
            if spec and spec.loader:
                exceptions_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.exceptions"] = exceptions_module
                spec.loader.exec_module(exceptions_module)
                CrawlError = exceptions_module.CrawlError
                classify_error = exceptions_module.classify_error
            else:
                raise ImportError(f"Kh√¥ng th·ªÉ load exceptions module t·ª´ {exceptions_path}")
        else:
            raise ImportError(f"Kh√¥ng t√¨m th·∫•y exceptions.py t·∫°i {exceptions_path}")

        # 2. Import circuit_breaker (kh√¥ng c√≥ dependency)
        circuit_breaker_path = os.path.join(resilience_module_path, "circuit_breaker.py")
        if os.path.exists(circuit_breaker_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.circuit_breaker", circuit_breaker_path
            )
            if spec and spec.loader:
                circuit_breaker_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.circuit_breaker"] = circuit_breaker_module
                spec.loader.exec_module(circuit_breaker_module)
                CircuitBreaker = circuit_breaker_module.CircuitBreaker
                CircuitBreakerOpenError = circuit_breaker_module.CircuitBreakerOpenError
            else:
                raise ImportError("Kh√¥ng th·ªÉ load circuit_breaker module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y circuit_breaker.py")

        # 3. Import dead_letter_queue (c√≥ th·ªÉ import t·ª´ utils)
        dlq_path = os.path.join(resilience_module_path, "dead_letter_queue.py")
        if os.path.exists(dlq_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.dead_letter_queue", dlq_path
            )
            if spec and spec.loader:
                dlq_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.dead_letter_queue"] = dlq_module
                spec.loader.exec_module(dlq_module)
                DeadLetterQueue = dlq_module.DeadLetterQueue
                get_dlq = dlq_module.get_dlq
            else:
                raise ImportError("Kh√¥ng th·ªÉ load dead_letter_queue module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y dead_letter_queue.py")

        # 4. Import graceful_degradation (kh√¥ng c√≥ dependency)
        degradation_path = os.path.join(resilience_module_path, "graceful_degradation.py")
        if os.path.exists(degradation_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.graceful_degradation", degradation_path
            )
            if spec and spec.loader:
                degradation_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.graceful_degradation"] = degradation_module
                spec.loader.exec_module(degradation_module)
                GracefulDegradation = degradation_module.GracefulDegradation
                DegradationLevel = degradation_module.DegradationLevel
                get_service_health = degradation_module.get_service_health
            else:
                raise ImportError("Kh√¥ng th·ªÉ load graceful_degradation module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y graceful_degradation.py")

    except Exception as e:
        # N·∫øu import l·ªói, t·∫°o dummy classes ƒë·ªÉ tr√°nh NameError
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import resilience module: {e}", stacklevel=2)

        # T·∫°o dummy classes
        class CircuitBreaker:
            def __init__(self, *args, **kwargs):
                pass

            def call(self, func, *args, **kwargs):
                return func(*args, **kwargs)

        class CircuitBreakerOpenError(Exception):
            pass

        class DeadLetterQueue:
            def add(self, *args, **kwargs):
                pass

        def get_dlq(*args, **kwargs):
            return DeadLetterQueue()

        class GracefulDegradation:
            def should_skip(self):
                return False

            def record_success(self):
                pass

            def record_failure(self):
                pass

        class DegradationLevel:
            FULL = "full"
            REDUCED = "reduced"
            MINIMAL = "minimal"
            FAILED = "failed"

        def get_service_health():
            return type(
                "ServiceHealth",
                (),
                {"register_service": lambda *args, **kwargs: GracefulDegradation()},
            )()

        def classify_error(error, **kwargs):
            return error

        class CrawlError(Exception):
            pass

else:
    # Fallback: t·∫°o dummy classes
    class CircuitBreaker:
        def __init__(self, *args, **kwargs):
            pass

        def call(self, func, *args, **kwargs):
            return func(*args, **kwargs)

    class CircuitBreakerOpenError(Exception):
        pass

    class DeadLetterQueue:
        def add(self, *args, **kwargs):
            pass

    def get_dlq(*args, **kwargs):
        return DeadLetterQueue()

    class GracefulDegradation:
        def should_skip(self):
            return False

        def record_success(self):
            pass

        def record_failure(self):
            pass

    class DegradationLevel:
        FULL = "full"
        REDUCED = "reduced"
        MINIMAL = "minimal"
        FAILED = "failed"

    def get_service_health():
        return type(
            "ServiceHealth", (), {"register_service": lambda *args, **kwargs: GracefulDegradation()}
        )()

    def classify_error(error, **kwargs):
        return error

    class CrawlError(Exception):
        pass


# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    "owner": "data-team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,  # Retry 3 l·∫ßn
    "retry_delay": timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    "retry_exponential_backoff": True,  # Exponential backoff
    "max_retry_delay": timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG - C√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa t·ª± ƒë·ªông v√† th·ªß c√¥ng qua Variable
# ƒê·ªçc schedule mode t·ª´ Airflow Variable (m·∫∑c ƒë·ªãnh: 'manual' ƒë·ªÉ test)
# C√≥ th·ªÉ set Variable 'TIKI_DAG_SCHEDULE_MODE' = 'scheduled' ƒë·ªÉ ch·∫°y t·ª± ƒë·ªông
try:
    schedule_mode = Variable.get("TIKI_DAG_SCHEDULE_MODE", default_var="manual")
except Exception:
    schedule_mode = "manual"  # M·∫∑c ƒë·ªãnh l√† manual ƒë·ªÉ test

# X√°c ƒë·ªãnh schedule d·ª±a tr√™n mode
if schedule_mode == "scheduled":
    dag_schedule = timedelta(days=1)  # Ch·∫°y t·ª± ƒë·ªông h√†ng ng√†y
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (T·ª± ƒë·ªông ch·∫°y h√†ng ng√†y)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "scheduled"]
else:
    dag_schedule = None  # Ch·ªâ ch·∫°y khi trigger th·ªß c√¥ng
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (Ch·∫°y th·ªß c√¥ng - Test mode)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "manual"]

# C·∫•u h√¨nh DAG schedule
dag_schedule_config = dag_schedule

# Documentation ƒë∆°n gi·∫£n cho DAG
dag_doc_md = "Crawl s·∫£n ph·∫©m t·ª´ Tiki.vn v·ªõi Dynamic Task Mapping v√† Selenium"

DAG_CONFIG = {
    "dag_id": "tiki_crawl_products",
    "description": dag_description,
    "doc_md": dag_doc_md,
    "default_args": DEFAULT_ARGS,
    "schedule": dag_schedule_config,
    "start_date": datetime(2025, 11, 1),  # Ng√†y c·ªë ƒë·ªãnh trong qu√° kh·ª©
    "catchup": False,  # Kh√¥ng ch·∫°y l·∫°i c√°c task ƒë√£ b·ªè l·ª°
    "tags": dag_tags,
    "max_active_runs": 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    "max_active_tasks": 10,  # Gi·∫£m xu·ªëng 10 tasks song song ƒë·ªÉ tr√°nh qu√° t·∫£i khi t·∫°o Selenium driver
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path("/opt/airflow/data"),  # Docker mount
    Path(__file__).parent.parent.parent / "data",  # Local development
    Path(os.getcwd()) / "data",  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    DATA_DIR = Path(__file__).parent.parent.parent / "data"

CATEGORIES_FILE = DATA_DIR / "raw" / "categories_recursive_optimized.json"
CATEGORIES_TREE_FILE = DATA_DIR / "raw" / "categories_tree.json"
OUTPUT_DIR = DATA_DIR / "raw" / "products"
CACHE_DIR = OUTPUT_DIR / "cache"
DETAIL_CACHE_DIR = OUTPUT_DIR / "detail" / "cache"
OUTPUT_FILE = OUTPUT_DIR / "products.json"
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / "products_with_detail.json"
# Progress tracking cho multi-day crawling
PROGRESS_FILE = OUTPUT_DIR / "crawl_progress.json"

# Asset/Dataset ƒë√£ ƒë∆∞·ª£c x√≥a - dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()

# Kh·ªüi t·∫°o resilience patterns
# Circuit breaker cho Tiki API
tiki_circuit_breaker = CircuitBreaker(
    failure_threshold=int(Variable.get("TIKI_CIRCUIT_BREAKER_FAILURE_THRESHOLD", default_var="5")),
    recovery_timeout=int(Variable.get("TIKI_CIRCUIT_BREAKER_RECOVERY_TIMEOUT", default_var="60")),
    expected_exception=Exception,
    name="tiki_api",
)

# Dead Letter Queue
try:
    # Th·ª≠ d√πng Redis n·∫øu c√≥
    redis_url = Variable.get("REDIS_URL", default_var="redis://redis:6379/3")
    tiki_dlq = get_dlq(storage_type="redis", redis_url=redis_url)
except Exception:
    # Fallback v·ªÅ file-based
    try:
        dlq_path = DATA_DIR / "dlq"
        tiki_dlq = get_dlq(storage_type="file", storage_path=str(dlq_path))
    except Exception:
        # N·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c, d√πng default
        tiki_dlq = get_dlq()

# Graceful Degradation cho Tiki service
service_health = get_service_health()
tiki_degradation = service_health.register_service(
    name="tiki",
    failure_threshold=int(Variable.get("TIKI_DEGRADATION_FAILURE_THRESHOLD", default_var="3")),
    recovery_threshold=int(Variable.get("TIKI_DEGRADATION_RECOVERY_THRESHOLD", default_var="5")),
)

# Import modules cho AI summarization v√† Discord notification
# T√¨m c√°c th∆∞ m·ª•c: analytics/, ai/, notifications/ ·ªü common/ (sau src/)
analytics_path = None
ai_path = None
notifications_path = None
config_path = None

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ cho c√°c modules ·ªü common/
common_base_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    "/opt/airflow/src/common",
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "common")),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "common")),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    "/workspace/src/common",
    # T·ª´ current working directory
    os.path.join(os.getcwd(), "src", "common"),
]

for common_base in common_base_paths:
    test_analytics = os.path.join(common_base, "analytics", "aggregator.py")
    test_ai = os.path.join(common_base, "ai", "summarizer.py")
    test_notifications = os.path.join(common_base, "notifications", "discord.py")
    test_config = os.path.join(common_base, "config.py")

    if os.path.exists(test_analytics):
        analytics_path = test_analytics
    if os.path.exists(test_ai):
        ai_path = test_ai
    if os.path.exists(test_notifications):
        notifications_path = test_notifications
    if os.path.exists(test_config):
        config_path = test_config

    if analytics_path and ai_path and notifications_path and config_path:
        break

# IMPORTANT: Load config.py TR∆Ø·ªöC ƒë·ªÉ ƒë·∫£m b·∫£o .env ƒë∆∞·ª£c load
# ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env ƒë∆∞·ª£c set tr∆∞·ªõc khi c√°c module kh√°c import
if config_path and os.path.exists(config_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.config", config_path)
        if spec is not None and spec.loader is not None:
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            # Config module s·∫Ω t·ª± ƒë·ªông load .env khi ƒë∆∞·ª£c import
            import warnings

            warnings.warn(
                f"‚úÖ ƒê√£ load common.config t·ª´ {config_path}, .env s·∫Ω ƒë∆∞·ª£c load t·ª± ƒë·ªông",
                stacklevel=2,
            )
    except Exception as e:
        import warnings

        warnings.warn(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load common.config: {e}", stacklevel=2)

# Import DataAggregator t·ª´ common/analytics/
if analytics_path and os.path.exists(analytics_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.analytics.aggregator", analytics_path)
        if spec is not None and spec.loader is not None:
            aggregator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(aggregator_module)
            DataAggregator = aggregator_module.DataAggregator
        else:
            DataAggregator = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.analytics.aggregator module: {e}", stacklevel=2)
        DataAggregator = None
else:
    DataAggregator = None

# Import AISummarizer t·ª´ common/ai/
if ai_path and os.path.exists(ai_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.ai.summarizer", ai_path)
        if spec is not None and spec.loader is not None:
            ai_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(ai_module)
            AISummarizer = ai_module.AISummarizer
        else:
            AISummarizer = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.ai.summarizer module: {e}", stacklevel=2)
        AISummarizer = None
else:
    AISummarizer = None

# Import DiscordNotifier t·ª´ common/notifications/
if notifications_path and os.path.exists(notifications_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "common.notifications.discord", notifications_path
        )
        if spec is not None and spec.loader is not None:
            notifications_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(notifications_module)
            DiscordNotifier = notifications_module.DiscordNotifier
        else:
            DiscordNotifier = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.notifications.discord module: {e}", stacklevel=2)
        DiscordNotifier = None
else:
    DiscordNotifier = None


def get_logger(context):
    """L·∫•y logger t·ª´ context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging

        ti = context.get("task_instance")
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging

        return logging.getLogger("airflow.task")


def _fix_sys_path_for_pipelines_import(logger=None):
    """
    S·ª≠a sys.path v√† sys.modules ƒë·ªÉ ƒë·∫£m b·∫£o pipelines c√≥ th·ªÉ ƒë∆∞·ª£c import ƒë√∫ng c√°ch.
    X√≥a c√°c ƒë∆∞·ªùng d·∫´n con nh∆∞ /opt/airflow/src/pipelines kh·ªèi sys.path,
    x√≥a c√°c fake modules kh·ªèi sys.modules, v√† ch·ªâ gi·ªØ l·∫°i /opt/airflow/src.
    """
    import logging
    if logger is None:
        logger = logging.getLogger("airflow.task")
    
    # X√≥a c√°c fake modules kh·ªèi sys.modules (quan tr·ªçng!)
    # C√°c fake modules n√†y ƒë∆∞·ª£c t·∫°o ·ªü ƒë·∫ßu file v√† g√¢y l·ªói 'pipelines' is not a package
    modules_to_remove = []
    for module_name in list(sys.modules.keys()):
        if module_name.startswith('pipelines'):
            modules_to_remove.append(module_name)
    
    for module_name in modules_to_remove:
        del sys.modules[module_name]
        if logger:
            logger.info(f"üóëÔ∏è  ƒê√£ x√≥a fake module kh·ªèi sys.modules: {module_name}")
    
    # X√≥a c√°c ƒë∆∞·ªùng d·∫´n con kh·ªèi sys.path (g√¢y l·ªói 'pipelines' is not a package)
    paths_to_remove = []
    for path in sys.path:
        # X√≥a c√°c ƒë∆∞·ªùng d·∫´n nh∆∞ /opt/airflow/src/pipelines ho·∫∑c /opt/airflow/src/pipelines/crawl
        normalized_path = path.replace('\\', '/')
        if normalized_path.endswith('/pipelines') or normalized_path.endswith('/pipelines/crawl'):
            paths_to_remove.append(path)
    
    for path in paths_to_remove:
        if path in sys.path:
            sys.path.remove(path)
            if logger:
                logger.info(f"üóëÔ∏è  ƒê√£ x√≥a ƒë∆∞·ªùng d·∫´n sai kh·ªèi sys.path: {path}")
    
    # ƒê·∫£m b·∫£o /opt/airflow/src c√≥ trong sys.path
    possible_src_paths = [
        "/opt/airflow/src",  # Docker default path
        os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "src")),  # Local dev
    ]
    
    for src_path in possible_src_paths:
        if os.path.exists(src_path) and os.path.isdir(src_path):
            if src_path not in sys.path:
                sys.path.insert(0, src_path)
                if logger:
                    logger.info(f"‚úÖ ƒê√£ th√™m v√†o sys.path: {src_path}")
            return src_path
    
    return None


def extract_and_load_categories_to_db(**context) -> dict[str, Any]:
    """
    Task 0: Extract categories t·ª´ categories_tree.json v√† load v√†o database

    Returns:
        Dict: Stats v·ªÅ vi·ªác load categories
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìÅ TASK: Extract & Load Categories to Database")
    logger.info("=" * 70)

    try:
        # Import extract v√† load modules
        try:
            # Th·ª≠ import t·ª´ ƒë∆∞·ªùng d·∫´n trong Docker/Airflow
            import sys
            import importlib.util
            from pathlib import Path

            # T√¨m ƒë∆∞·ªùng d·∫´n ƒë·∫øn extract_categories.py
            possible_paths = [
                "/opt/airflow/src/pipelines/extract/extract_categories.py",
                os.path.join(os.path.dirname(__file__), "..", "..", "src", "pipelines", "extract", "extract_categories.py"),
                os.path.join(os.getcwd(), "src", "pipelines", "extract", "extract_categories.py"),
            ]

            extract_module_path = None
            for path in possible_paths:
                test_path = Path(path)
                if test_path.exists():
                    extract_module_path = test_path
                    break

            if extract_module_path:
                spec = importlib.util.spec_from_file_location("extract_categories", extract_module_path)
                if spec and spec.loader:
                    extract_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(extract_module)
                    extract_categories_from_tree_file = extract_module.extract_categories_from_tree_file
                else:
                    raise ImportError("Kh√¥ng th·ªÉ load extract_categories module")
            else:
                raise ImportError("Kh√¥ng t√¨m th·∫•y extract_categories.py")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import extract module: {e}")
            logger.info("Th·ª≠ import tr·ª±c ti·∫øp...")
            # Fallback: th·ª≠ import tr·ª±c ti·∫øp
            try:
                from pipelines.extract.extract_categories import extract_categories_from_tree_file
            except ImportError:
                # S·ª≠a sys.path v√† th·ª≠ l·∫°i
                _fix_sys_path_for_pipelines_import(logger)
                try:
                    from pipelines.extract.extract_categories import extract_categories_from_tree_file
                except ImportError as e:
                    logger.error(f"‚ùå Kh√¥ng th·ªÉ import extract_categories: {e}")
                    logger.error(f"   sys.path: {sys.path}")
                    raise

        # Import DataLoader
        try:
            from pipelines.load.loader import DataLoader
            logger.info("‚úÖ ƒê√£ import DataLoader th√†nh c√¥ng")
        except ImportError:
            # S·ª≠a sys.path v√† th·ª≠ l·∫°i
            _fix_sys_path_for_pipelines_import(logger)
            try:
                from pipelines.load.loader import DataLoader
                logger.info("‚úÖ ƒê√£ import DataLoader th√†nh c√¥ng")
            except ImportError as e:
                logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataLoader: {e}")
                logger.error(f"   sys.path: {sys.path}")
                raise

        # 1. Extract categories t·ª´ tree file
        tree_file = str(CATEGORIES_TREE_FILE)
        logger.info(f"üìñ ƒêang extract categories t·ª´: {tree_file}")

        if not os.path.exists(tree_file):
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file: {tree_file}")
            logger.info("B·ªè qua task n√†y, categories c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c load tr∆∞·ªõc ƒë√≥")
            return {
                "total_loaded": 0,
                "db_loaded": 0,
                "success_count": 0,
                "failed_count": 0,
                "skipped": True,
            }

        categories = extract_categories_from_tree_file(tree_file)
        logger.info(f"‚úÖ ƒê√£ extract {len(categories)} categories")

        # 2. Load v√†o database
        logger.info("üíæ ƒêang load categories v√†o database...")

        # L·∫•y credentials t·ª´ environment variables
        loader = DataLoader(
            database=os.getenv("POSTGRES_DB", "crawl_data"),
            host=os.getenv("POSTGRES_HOST", "postgres"),
            port=int(os.getenv("POSTGRES_PORT", "5432")),
            user=os.getenv("POSTGRES_USER", "airflow_user"),
            password=os.getenv("POSTGRES_PASSWORD", ""),
            batch_size=100,
            enable_db=True,
        )

        try:
            stats = loader.load_categories(
                categories,
                save_to_file=None,  # Kh√¥ng l∆∞u file, ch·ªâ load v√†o DB
                upsert=True,
                validate_before_load=True,
            )

            logger.info(f"‚úÖ ƒê√£ load {stats['db_loaded']} categories v√†o database")
            logger.info(f"   - T·ªïng s·ªë: {stats['total_loaded']}")
            logger.info(f"   - Th√†nh c√¥ng: {stats['success_count']}")
            logger.info(f"   - Th·∫•t b·∫°i: {stats['failed_count']}")

            if stats.get("errors"):
                logger.warning(f"‚ö†Ô∏è  C√≥ {len(stats['errors'])} l·ªói (hi·ªÉn th·ªã 5 ƒë·∫ßu ti√™n):")
                for error in stats["errors"][:5]:
                    logger.warning(f"   - {error}")

            loader.close()
            return stats

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load v√†o database: {e}", exc_info=True)
            loader.close()
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong extract_and_load_categories_to_db: {e}", exc_info=True)
        raise


def load_categories(**context) -> list[dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file

    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìñ TASK: Load Categories")
    logger.info("=" * 70)

    try:
        categories_file = str(CATEGORIES_FILE)
        logger.info(f"ƒêang ƒë·ªçc file: {categories_file}")

        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")

        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)

        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")

        # L·ªçc danh m·ª•c n·∫øu c·∫ßn (v√≠ d·ª•: ch·ªâ l·∫•y level 2-4)
        # C√≥ th·ªÉ c·∫•u h√¨nh qua Airflow Variable
        try:
            min_level = int(Variable.get("TIKI_MIN_CATEGORY_LEVEL", default_var="2"))
            max_level = int(Variable.get("TIKI_MAX_CATEGORY_LEVEL", default_var="4"))
            categories = [
                cat for cat in categories if min_level <= cat.get("level", 0) <= max_level
            ]
            logger.info(f"‚úì Sau khi l·ªçc level {min_level}-{max_level}: {len(categories)} danh m·ª•c")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l·ªçc theo level: {e}")

        # Gi·ªõi h·∫°n s·ªë danh m·ª•c n·∫øu c·∫ßn (ƒë·ªÉ test)
        try:
            max_categories = int(Variable.get("TIKI_MAX_CATEGORIES", default_var="0"))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"‚úì Gi·ªõi h·∫°n: {max_categories} danh m·ª•c")
        except Exception:
            pass

        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)

    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl

    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)

    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get("ti")
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                category = op_kwargs.get("category")

        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get("category") or context.get("op_kwargs", {}).get("category")

    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get("ti")
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")

    category_url = category.get("url", "")
    category_name = category.get("name", "Unknown")
    category_id = category.get("id", "")

    logger.info("=" * 70)
    logger.info(f"üõçÔ∏è  TASK: Crawl Category - {category_name}")
    logger.info(f"üîó URL: {category_url}")
    logger.info("=" * 70)

    result = {
        "category_id": category_id,
        "category_name": category_name,
        "category_url": category_url,
        "products": [],
        "status": "failed",
        "error": None,
        "crawled_at": datetime.now().isoformat(),
        "pages_crawled": 0,
        "products_count": 0,
    }

    try:
        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            logger.warning(f"‚ö†Ô∏è  Service degraded, skip category {category_name}")
            return result

        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(
            Variable.get("TIKI_MAX_PAGES_PER_CATEGORY", default_var="20")
        )  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = Variable.get("TIKI_USE_SELENIUM", default_var="false").lower() == "true"
        timeout = int(Variable.get("TIKI_CRAWL_TIMEOUT", default_var="300"))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(
            Variable.get("TIKI_RATE_LIMIT_DELAY", default_var="1.0")
        )  # Delay 1s gi·ªØa c√°c request

        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        def _crawl_with_params():
            """Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker"""
            return crawl_category_products(
                category_url,
                max_pages=max_pages if max_pages > 0 else None,
                use_selenium=use_selenium,
                cache_dir=str(CACHE_DIR),
                use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
            )

        try:
            # G·ªçi v·ªõi circuit breaker
            products = tiki_circuit_breaker.call(_crawl_with_params)
            tiki_degradation.record_success()
        except CircuitBreakerOpenError as e:
            # Circuit breaker ƒëang m·ªü
            result["error"] = f"Circuit breaker open: {str(e)}"
            result["status"] = "circuit_breaker_open"
            logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho category {category_name}: {e}")
            # Th√™m v√†o DLQ
            try:
                crawl_error = classify_error(
                    e, context={"category_url": category_url, "category_id": category_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_category_{category_id}",
                    task_type="crawl_category",
                    error=crawl_error,
                    context={
                        "category_url": category_url,
                        "category_name": category_name,
                        "category_id": category_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result
        except Exception:
            # Ghi nh·∫≠n failure
            tiki_degradation.record_failure()
            raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")

        result["products"] = products
        result["status"] = "success"
        result["products_count"] = len(products)
        result["elapsed_time"] = elapsed

        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    return result


def merge_products(**context) -> dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c

    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Products")
    logger.info("=" * 70)

    try:

        ti = context["ti"]

        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
            logger.info(
                f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
            )
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")

        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            "total_categories": len(categories),
            "success_categories": 0,
            "failed_categories": 0,
            "timeout_categories": 0,
            "total_products": 0,
            "unique_products": 0,
        }

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = "crawl_categories.crawl_category"

        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(task_ids=task_id, key="return_value")

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                for result in all_results:
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get("status") == "success":
                    stats["success_categories"] += 1
                    products = all_results.get("products", [])
                    all_products.extend(products)
                    stats["total_products"] += len(products)
                elif all_results.get("status") == "timeout":
                    stats["timeout_categories"] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats["failed_categories"] += 1
                    logger.warning(
                        f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}"
                    )

            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                logger.info("Th·ª≠ l·∫•y t·ª´ng map_index...")
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )

                        if result and isinstance(result, dict):
                            if result.get("status") == "success":
                                stats["success_categories"] += 1
                                products = result.get("products", [])
                                all_products.extend(products)
                                stats["total_products"] += len(products)
                            elif result.get("status") == "timeout":
                                stats["timeout_categories"] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats["failed_categories"] += 1
                                logger.warning(
                                    f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                                )
                    except Exception as e:
                        stats["failed_categories"] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")

        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats["failed_categories"] = len(categories)

        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        products_with_sales_count = 0
        for product in all_products:
            product_id = product.get("product_id")
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                # ƒê·∫£m b·∫£o sales_count lu√¥n c√≥ trong product (k·ªÉ c·∫£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                elif product.get("sales_count") is not None:
                    products_with_sales_count += 1
                unique_products.append(product)

        # Log th·ªëng k√™ sales_count
        logger.info(
            f"üìä Products c√≥ sales_count: {products_with_sales_count}/{len(unique_products)} ({products_with_sales_count/len(unique_products)*100:.1f}%)"
            if unique_products
            else "üìä Products c√≥ sales_count: 0/0"
        )

        stats["unique_products"] = len(unique_products)

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("=" * 70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("=" * 70)

        result = {
            "products": unique_products,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt

    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi v√†o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == "nt":  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))

        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")

    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)

    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products")
    logger.info("=" * 70)

    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context["ti"]
        merge_result = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
            logger.info("L·∫•y merge_result t·ª´ 'process_and_save.merge_products'")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
                logger.info("L·∫•y merge_result t·ª´ 'merge_products'")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")

        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(Variable.get("TIKI_SAVE_BATCH_SIZE", default_var="10000"))

        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                batch_file = OUTPUT_DIR / f"products_batch_{i // batch_size}.json"
                batch_data = {
                    "batch_index": i // batch_size,
                    "total_batches": (len(products) + batch_size - 1) // batch_size,
                    "products": batch,
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def prepare_products_for_detail(**context) -> list[dict[str, Any]]:
    """
    Task: Chu·∫©n b·ªã danh s√°ch products ƒë·ªÉ crawl detail

    T·ªëi ∆∞u cho multi-day crawling:
    - Ch·ªâ crawl products ch∆∞a c√≥ detail
    - Chia th√†nh batches theo ng√†y (c√≥ th·ªÉ crawl trong nhi·ªÅu ng√†y)
    - Ki·ªÉm tra cache v√† progress ƒë·ªÉ tr√°nh crawl l·∫°i
    - Track progress ƒë·ªÉ resume t·ª´ ƒëi·ªÉm d·ª´ng

    Returns:
        List[Dict]: List c√°c dict ch·ª©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìã TASK: Prepare Products for Detail Crawling (Multi-Day Support)")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products t·ª´ task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products: {len(products)}")

        # ƒê·ªçc progress file ƒë·ªÉ bi·∫øt ƒë√£ crawl ƒë·∫øn ƒë√¢u
        progress = {
            "crawled_product_ids": set(),
            "last_crawled_index": 0,
            "total_crawled": 0,
            "last_updated": None,
        }

        if PROGRESS_FILE.exists():
            try:
                with open(PROGRESS_FILE, encoding="utf-8") as f:
                    saved_progress = json.load(f)
                    progress["crawled_product_ids"] = set(
                        saved_progress.get("crawled_product_ids", [])
                    )
                    progress["last_crawled_index"] = saved_progress.get("last_crawled_index", 0)
                    progress["total_crawled"] = saved_progress.get("total_crawled", 0)
                    progress["last_updated"] = saved_progress.get("last_updated")
                    logger.info(
                        f"üìÇ ƒê√£ load progress: {len(progress['crawled_product_ids'])} products ƒë√£ crawl"
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c progress file: {e}")

        # L·ªçc products c·∫ßn crawl detail
        products_to_crawl = []
        cache_hits = 0
        already_crawled = 0
        db_hits = 0  # Products ƒë√£ c√≥ trong DB

        # L·∫•y c·∫•u h√¨nh cho multi-day crawling
        # T√≠nh to√°n: 500 products ~ 52.75 ph√∫t -> 280 products ~ 30 ph√∫t
        products_per_day = int(
            Variable.get("TIKI_PRODUCTS_PER_DAY", default_var="120")
        )  # M·∫∑c ƒë·ªãnh 280 products/ng√†y (~30 ph√∫t)
        max_products = int(
            Variable.get("TIKI_MAX_PRODUCTS_FOR_DETAIL", default_var="0")
        )  # 0 = kh√¥ng gi·ªõi h·∫°n

        logger.info(
            f"‚öôÔ∏è  C·∫•u h√¨nh: {products_per_day} products/ng√†y, max: {max_products if max_products > 0 else 'kh√¥ng gi·ªõi h·∫°n'}"
        )

        # Ki·ªÉm tra products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß (ƒë·ªÉ tr√°nh crawl l·∫°i)
        # Ch·ªâ skip products c√≥ price v√† sales_count (detail ƒë·∫ßy ƒë·ªß)
        existing_product_ids_in_db = set()
        try:
            PostgresStorage = _import_postgres_storage()
            if PostgresStorage is None:
                logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ import PostgresStorage, b·ªè qua ki·ªÉm tra database")
            else:
                # L·∫•y database config
                db_host = Variable.get("POSTGRES_HOST", default_var=os.getenv("POSTGRES_HOST", "postgres"))
                db_port = int(Variable.get("POSTGRES_PORT", default_var=os.getenv("POSTGRES_PORT", "5432")))
                db_name = Variable.get("POSTGRES_DB", default_var=os.getenv("POSTGRES_DB", "crawl_data"))
                db_user = Variable.get("POSTGRES_USER", default_var=os.getenv("POSTGRES_USER", "postgres"))
                db_password = Variable.get("POSTGRES_PASSWORD", default_var=os.getenv("POSTGRES_PASSWORD", "postgres"))
                
                storage = PostgresStorage(
                    host=db_host,
                    port=db_port,
                    database=db_name,
                    user=db_user,
                    password=db_password,
                )
                
                # L·∫•y danh s√°ch product_ids t·ª´ products list
                product_ids_to_check = [p.get("product_id") for p in products if p.get("product_id")]
                
                if product_ids_to_check:
                    logger.info(f"üîç ƒêang ki·ªÉm tra {len(product_ids_to_check)} products trong database...")
                    logger.info("   (ch·ªâ skip products c√≥ price v√† sales_count - detail ƒë·∫ßy ƒë·ªß)")
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            # Chia nh·ªè query n·∫øu c√≥ qu√° nhi·ªÅu product_ids
                            # Ch·ªâ l·∫•y products c√≥ price v√† sales_count (detail ƒë·∫ßy ƒë·ªß)
                            for i in range(0, len(product_ids_to_check), 1000):
                                batch_ids = product_ids_to_check[i : i + 1000]
                                placeholders = ",".join(["%s"] * len(batch_ids))
                                cur.execute(
                                    f"""
                                    SELECT product_id 
                                    FROM products 
                                    WHERE product_id IN ({placeholders})
                                      AND price IS NOT NULL 
                                      AND sales_count IS NOT NULL
                                    """,
                                    batch_ids,
                                )
                                existing_product_ids_in_db.update(row[0] for row in cur.fetchall())
                    
                    logger.info(f"‚úÖ T√¨m th·∫•y {len(existing_product_ids_in_db)} products ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß trong database")
                    logger.info("   (c√≥ price v√† sales_count - s·∫Ω skip crawl l·∫°i)")
                    storage.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra database: {e}")
            logger.info("   S·∫Ω ti·∫øp t·ª•c v·ªõi cache v√† progress file")

        # B·∫Øt ƒë·∫ßu t·ª´ index ƒë√£ crawl
        start_index = progress["last_crawled_index"]
        
        # Ki·ªÉm tra n·∫øu start_index v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng products hi·ªán t·∫°i
        # (c√≥ th·ªÉ do test mode gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products)
        if start_index >= len(products):
            logger.warning("=" * 70)
            logger.warning(f"‚ö†Ô∏è  RESET PROGRESS INDEX!")
            logger.warning(f"   - Progress index: {start_index}")
            logger.warning(f"   - S·ªë products hi·ªán t·∫°i: {len(products)}")
            logger.warning(f"   - Index v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng products")
            logger.warning("   - C√≥ th·ªÉ do test mode gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products")
            logger.warning("   - Reset v·ªÅ index 0 ƒë·ªÉ crawl l·∫°i t·ª´ ƒë·∫ßu")
            logger.warning("=" * 70)
            start_index = 0
            # Reset progress ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
            progress["last_crawled_index"] = 0
            progress["total_crawled"] = 0
            # Gi·ªØ l·∫°i crawled_product_ids ƒë·ªÉ tr√°nh crawl l·∫°i products ƒë√£ c√≥
        
        products_to_check = products[start_index:]

        logger.info(
            f"üîÑ B·∫Øt ƒë·∫ßu t·ª´ index {start_index} (ƒë√£ crawl {progress['total_crawled']} products, t·ªïng products: {len(products)})"
        )

        # T·ªëi ∆∞u: Duy·ªát t·∫•t c·∫£ products ƒë·ªÉ t√¨m products ch∆∞a c√≥ trong DB
        # Thay v√¨ d·ª´ng khi ƒë·∫°t max_products nh∆∞ng to√†n b·ªô l√† skip
        skipped_count = 0
        max_skipped_before_stop = 100  # D·ª´ng n·∫øu skip li√™n ti·∫øp 100 products
        
        for idx, product in enumerate(products_to_check):
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not product_id or not product_url:
                continue

            # Ki·ªÉm tra xem ƒë√£ crawl ch∆∞a (t·ª´ progress)
            if product_id in progress["crawled_product_ids"]:
                already_crawled += 1
                skipped_count += 1
                continue

            # Ki·ªÉm tra xem ƒë√£ c√≥ trong database ch∆∞a (v·ªõi detail ƒë·∫ßy ƒë·ªß)
            # existing_product_ids_in_db ch·ªâ ch·ª©a products c√≥ price v√† sales_count
            if product_id in existing_product_ids_in_db:
                # ƒê√£ c√≥ trong DB v·ªõi detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count)
                # ‚Üí Skip crawl l·∫°i
                db_hits += 1
                progress["crawled_product_ids"].add(product_id)
                already_crawled += 1
                skipped_count += 1
                continue

            # Ki·ªÉm tra cache
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            has_valid_cache = False
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None

                        # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), ƒë√°nh d·∫•u ƒë√£ crawl
                        if has_price and has_sales_count:
                            cache_hits += 1
                            progress["crawled_product_ids"].add(product_id)
                            already_crawled += 1
                            has_valid_cache = True
                            skipped_count += 1
                        # N·∫øu cache thi·∫øu sales_count, v·∫´n c·∫ßn crawl l·∫°i
                except Exception:
                    pass

            # N·∫øu ch∆∞a c√≥ cache h·ª£p l·ªá, th√™m v√†o danh s√°ch crawl
            if not has_valid_cache:
                products_to_crawl.append(
                    {
                        "product_id": product_id,
                        "url": product_url,
                        "name": product.get("name", ""),
                        "product": product,  # Gi·ªØ nguy√™n product data
                        "index": start_index + idx,  # L∆∞u index ƒë·ªÉ track progress
                    }
                )
                skipped_count = 0  # Reset counter khi t√¨m th·∫•y product m·ªõi

            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products crawl trong ng√†y n√†y
            if len(products_to_crawl) >= products_per_day:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {products_per_day} products cho ng√†y h√¥m nay")
                break

            # Gi·ªõi h·∫°n t·ªïng s·ªë (n·∫øu c√≥)
            if max_products > 0 and len(products_to_crawl) >= max_products:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n t·ªïng {max_products} products")
                break
            
            # D·ª´ng n·∫øu skip qu√° nhi·ªÅu products li√™n ti·∫øp (c√≥ th·ªÉ ƒë√£ h·∫øt products m·ªõi)
            if skipped_count >= max_skipped_before_stop:
                logger.info(f"‚ö†Ô∏è  ƒê√£ skip {skipped_count} products li√™n ti·∫øp, c√≥ th·ªÉ ƒë√£ h·∫øt products m·ªõi")
                logger.info(f"   - ƒê√£ t√¨m ƒë∆∞·ª£c {len(products_to_crawl)} products ƒë·ªÉ crawl")
                break

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä PREPARE PRODUCTS FOR DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products ƒë·∫ßu v√†o: {len(products)}")
        logger.info(f"‚úÖ Products c·∫ßn crawl h√¥m nay: {len(products_to_crawl)}")
        logger.info(f"üì¶ Cache hits (c√≥ cache h·ª£p l·ªá): {cache_hits}")
        logger.info(f"üíæ DB hits (ƒë√£ c√≥ trong DB v·ªõi detail ƒë·∫ßy ƒë·ªß): {db_hits}")
        logger.info(f"‚úì ƒê√£ crawl tr∆∞·ªõc ƒë√≥ (t·ª´ progress): {already_crawled - db_hits - cache_hits}")
        logger.info(f"üìà T·ªïng ƒë√£ crawl: {progress['total_crawled'] + already_crawled}")
        logger.info(
            f"üìâ C√≤n l·∫°i: {len(products) - (progress['total_crawled'] + already_crawled + len(products_to_crawl))}"
        )
        logger.info("=" * 70)
        
        if len(products_to_crawl) == 0:
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  KH√îNG C√ì PRODUCTS N√ÄO C·∫¶N CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("üí° L√Ω do:")
            if already_crawled > 0:
                logger.warning(f"   - ƒê√£ c√≥ trong progress: {already_crawled - db_hits - cache_hits} products")
            if cache_hits > 0:
                logger.warning(f"   - ƒê√£ c√≥ trong cache (c√≥ price v√† sales_count): {cache_hits} products")
            if db_hits > 0:
                logger.warning(f"   - ƒê√£ c√≥ trong database (c√≥ price v√† sales_count): {db_hits} products")
            logger.warning("=" * 70)
            logger.warning("üí° ƒê·ªÉ force crawl l·∫°i, b·∫°n c√≥ th·ªÉ:")
            logger.warning("   1. X√≥a progress file: data/processed/detail_crawl_progress.json")
            logger.warning("   2. X√≥a cache files trong: data/raw/products/detail/cache/")
            logger.warning("   3. X√≥a products trong database (n·∫øu mu·ªën crawl l·∫°i)")
            logger.warning("=" * 70)

        # L∆∞u progress (s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau khi crawl xong)
        if products_to_crawl:
            # L∆∞u index c·ªßa product cu·ªëi c√πng s·∫Ω ƒë∆∞·ª£c crawl
            last_index = products_to_crawl[-1]["index"]
            progress["last_crawled_index"] = last_index + 1
            progress["last_updated"] = datetime.now().isoformat()

            # L∆∞u progress v√†o file
            try:
                with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "crawled_product_ids": list(progress["crawled_product_ids"]),
                            "last_crawled_index": progress["last_crawled_index"],
                            "total_crawled": progress["total_crawled"] + already_crawled,
                            "last_updated": progress["last_updated"],
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
                logger.info(f"üíæ ƒê√£ l∆∞u progress: index {progress['last_crawled_index']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng l∆∞u ƒë∆∞·ª£c progress: {e}")

        # Debug: Log m·ªôt v√†i products ƒë·∫ßu ti√™n
        if products_to_crawl:
            logger.info("üìã Sample products (first 3):")
            for i, p in enumerate(products_to_crawl[:3]):
                logger.info(
                    f"  {i+1}. Product ID: {p.get('product_id')}, URL: {p.get('url')[:80]}..."
                )
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o c·∫ßn crawl detail h√¥m nay!")
            logger.info("üí° T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl ho·∫∑c c√≥ cache h·ª£p l·ªá")

        logger.info(f"üî¢ Tr·∫£ v·ªÅ {len(products_to_crawl)} products cho Dynamic Task Mapping")

        return products_to_crawl

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi prepare products: {e}", exc_info=True)
        raise


def crawl_product_batch(product_batch: list[dict[str, Any]] = None, batch_index: int = -1, **context) -> list[dict[str, Any]]:
    """
    Task: Crawl detail cho m·ªôt batch products (Batch Processing v·ªõi Driver Pooling v√† Async)
    
    T·ªëi ∆∞u:
    - Batch processing: 10 products/batch
    - Driver pooling: Reuse Selenium drivers trong batch
    - Async/aiohttp: Crawl parallel trong batch
    - Fallback Selenium: N·∫øu aiohttp thi·∫øu sales_count
    
    Args:
        product_batch: List products trong batch (t·ª´ expand_kwargs)
        batch_index: Index c·ªßa batch
        context: Airflow context
    
    Returns:
        List[Dict]: List k·∫øt qu·∫£ crawl cho batch
    """
    try:
        logger = get_logger(context)
    except Exception:
        import logging
        logger = logging.getLogger("airflow.task")
    
    # L·∫•y product_batch t·ª´ op_kwargs n·∫øu ch∆∞a c√≥
    if not product_batch:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_batch = op_kwargs.get("product_batch")
                batch_index = op_kwargs.get("batch_index", -1)
        
        if not product_batch:
            product_batch = context.get("product_batch") or context.get("op_kwargs", {}).get("product_batch")
            batch_index = context.get("batch_index", -1)
    
    if not product_batch:
        logger.error("=" * 70)
        logger.error("‚ùå KH√îNG T√åM TH·∫§Y PRODUCT_BATCH TRONG CONTEXT!")
        logger.error("=" * 70)
        logger.error("üí° Debug info:")
        logger.error(f"   - Context keys: {list(context.keys())}")
        if ti:
            logger.error(f"   - ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        logger.error("=" * 70)
        return []
    
    # Validate product_batch
    if not isinstance(product_batch, list):
        logger.error("=" * 70)
        logger.error(f"‚ùå PRODUCT_BATCH KH√îNG PH·∫¢I LIST: {type(product_batch)}")
        logger.error(f"   - Value: {product_batch}")
        logger.error("=" * 70)
        return []
    
    if len(product_batch) == 0:
        logger.warning("=" * 70)
        logger.warning(f"‚ö†Ô∏è  BATCH {batch_index} R·ªñNG - Kh√¥ng c√≥ products n√†o")
        logger.warning("=" * 70)
        return []
    
    logger.info("=" * 70)
    logger.info(f"üì¶ BATCH {batch_index}: Crawl {len(product_batch)} products")
    logger.info(f"   - Product IDs: {[p.get('product_id', 'unknown') for p in product_batch[:5]]}")
    if len(product_batch) > 5:
        logger.info(f"   - ... v√† {len(product_batch) - 5} products n·ªØa")
    logger.info("=" * 70)
    
    results = []
    
    try:
        import asyncio
        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        # crawl_product_detail_async v√† SeleniumDriverPool ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        if SeleniumDriverPool is None:
            raise ImportError("SeleniumDriverPool ch∆∞a ƒë∆∞·ª£c import t·ª´ utils module")
        
        # T·∫°o driver pool cho batch
        driver_pool = SeleniumDriverPool(pool_size=5, headless=True, timeout=60)
        
        # T·∫°o event loop tr∆∞·ªõc
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # Session s·∫Ω ƒë∆∞·ª£c t·∫°o b√™n trong async function (c·∫ßn async context)
        session = None
        
        async def crawl_single_async(product_info: dict) -> dict[str, Any]:
            """Crawl m·ªôt product v·ªõi async"""
            product_id = product_info.get("product_id", "unknown")
            product_url = product_info.get("url", "")
            
            result = {
                "product_id": product_id,
                "url": product_url,
                "status": "failed",
                "error": None,
                "detail": None,
                "crawled_at": datetime.now().isoformat(),
            }
            
            try:
                # Th·ª≠ async crawl tr∆∞·ªõc
                if session:
                    detail = await crawl_product_detail_async(
                        product_url,
                        session=session,
                        use_selenium_fallback=True,
                        verbose=False
                    )
                    
                    # Ki·ªÉm tra n·∫øu crawl_product_detail_async tr·∫£ v·ªÅ HTML string (do fallback v·ªÅ Selenium)
                    if isinstance(detail, str) and detail.strip().startswith("<"):
                        logger.info(f"‚ÑπÔ∏è  crawl_product_detail_async tr·∫£ v·ªÅ HTML (fallback Selenium) cho product {product_id}, ƒëang parse HTML")
                        # Parse HTML th√†nh dict
                        try:
                            detail = extract_product_detail(detail, product_url, verbose=False)
                            if detail and isinstance(detail, dict):
                                logger.info(f"‚úÖ ƒê√£ parse HTML th√†nh c√¥ng cho product {product_id}")
                        except Exception as parse_error:
                            logger.warning(f"‚ö†Ô∏è  L·ªói khi parse HTML t·ª´ crawl_product_detail_async: {parse_error}")
                            detail = None
                    
                    # ƒê·∫£m b·∫£o detail l√† dict
                    if detail and not isinstance(detail, dict):
                        logger.warning(f"‚ö†Ô∏è  crawl_product_detail_async tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}")
                        detail = None
                else:
                    # Fallback v·ªÅ Selenium n·∫øu kh√¥ng c√≥ aiohttp
                    # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
                    html = crawl_product_detail_with_selenium(
                        product_url,
                        verbose=False,
                        max_retries=2,
                        timeout=60,
                        use_redis_cache=True,
                        use_rate_limiting=True
                    )
                    if html:
                        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
                        detail = extract_product_detail(html, product_url, verbose=False)
                        
                        # Ki·ªÉm tra n·∫øu extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict
                        if isinstance(detail, str) and detail.strip().startswith("<"):
                            logger.warning(f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict cho product {product_id}, th·ª≠ parse l·∫°i")
                            # Th·ª≠ parse l·∫°i HTML
                            try:
                                detail = extract_product_detail(html, product_url, verbose=False)
                            except Exception as parse_error:
                                logger.warning(f"‚ö†Ô∏è  L·ªói khi parse l·∫°i HTML: {parse_error}")
                                detail = None
                        
                        # ƒê·∫£m b·∫£o detail l√† dict, kh√¥ng ph·∫£i HTML string
                        if not isinstance(detail, dict):
                            logger.warning(f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}")
                            detail = None
                    else:
                        detail = None
                
                if detail and isinstance(detail, dict):
                    result["detail"] = detail
                    result["status"] = "success"
                else:
                    result["error"] = "Kh√¥ng th·ªÉ crawl detail ho·∫∑c extract detail kh√¥ng h·ª£p l·ªá"
                    result["status"] = "failed"
                    
            except Exception as e:
                result["error"] = str(e)
                result["status"] = "failed"
                logger.warning(f"‚ö†Ô∏è  L·ªói khi crawl product {product_id}: {e}")
            
            return result
        
        # Crawl t·∫•t c·∫£ products trong batch song song v·ªõi async
        # (Event loop ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n)
        # S·ª≠ d·ª•ng asyncio.gather() ƒë·ªÉ crawl parallel
        rate_limit_delay = float(
            Variable.get("TIKI_DETAIL_RATE_LIMIT_DELAY", default_var="1.5")
        )
        
        # T·∫°o tasks v·ªõi rate limiting: stagger start times
        async def crawl_batch_parallel():
            """Crawl batch v·ªõi parallel processing v√† rate limiting"""
            # T·∫°o session ngay l·∫≠p t·ª©c trong async context (tr∆∞·ªõc khi t·∫°o tasks)
            # ƒê·∫£m b·∫£o session ƒë∆∞·ª£c t·∫°o trong async context c√≥ event loop
            nonlocal session
            if session is None:
                try:
                    import aiohttp
                    timeout = aiohttp.ClientTimeout(total=30)
                    # T·∫°o session trong async context (c√≥ event loop ƒëang ch·∫°y)
                    # ƒê√¢y l√† async function n√™n event loop ƒë√£ c√≥ s·∫µn
                    session = aiohttp.ClientSession(
                        timeout=timeout,
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        }
                    )
                    logger.info("‚úÖ ƒê√£ t·∫°o aiohttp session trong async context")
                except RuntimeError as e:
                    # L·ªói "no running event loop" - fallback v·ªÅ Selenium
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session (no event loop): {e}, s·∫Ω d√πng Selenium")
                    session = None
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session: {e}, s·∫Ω d√πng Selenium")
                    session = None
            
            # Factory function ƒë·ªÉ tr√°nh closure issue
            def create_crawl_task(product_info, delay_value):
                async def crawl_with_delay():
                    if delay_value > 0:
                        await asyncio.sleep(delay_value)
                    return await crawl_single_async(product_info)
                return crawl_with_delay()
            
            tasks = []
            for i, product in enumerate(product_batch):
                delay = i * rate_limit_delay / len(product_batch)  # Ph√¢n t√°n delay
                task = create_crawl_task(product, delay)
                tasks.append(task)
            
            # Ch·∫°y t·∫•t c·∫£ tasks song song
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # X·ª≠ l√Ω exceptions
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    product_info = product_batch[i]
                    processed_results.append({
                        "product_id": product_info.get("product_id", "unknown"),
                        "url": product_info.get("url", ""),
                        "status": "failed",
                        "error": str(result),
                        "detail": None,
                        "crawled_at": datetime.now().isoformat(),
                    })
                else:
                    processed_results.append(result)
            
            return processed_results
        
        results = loop.run_until_complete(crawl_batch_parallel())
        
        # ƒê√≥ng session
        if session:
            loop.run_until_complete(session.close())
        
        # Cleanup driver pool
        driver_pool.cleanup()
        
        # Th·ªëng k√™
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = len(results) - success_count
        
        logger.info(f"‚úÖ Batch {batch_index} ho√†n th√†nh:")
        logger.info(f"   - Success: {success_count}/{len(product_batch)}")
        logger.info(f"   - Failed: {failed_count}/{len(product_batch)}")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi crawl batch {batch_index}: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ results v·ªõi status failed cho t·∫•t c·∫£
        if product_batch and isinstance(product_batch, list):
            for product_info in product_batch:
                results.append({
                    "product_id": product_info.get("product_id", "unknown"),
                    "url": product_info.get("url", ""),
                    "status": "failed",
                    "error": f"Batch error: {str(e)}",
                    "detail": None,
                    "crawled_at": datetime.now().isoformat(),
                })
        else:
            logger.error("‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o failed results v√¨ product_batch kh√¥ng h·ª£p l·ªá")
    
    return results


def crawl_single_product_detail(product_info: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task: Crawl detail cho m·ªôt product (Dynamic Task Mapping)

    T·ªëi ∆∞u:
    - S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Rate limiting
    - Error handling: ti·∫øp t·ª•c v·ªõi product kh√°c khi l·ªói
    - Atomic write cache

    Args:
        product_info: Th√¥ng tin product (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi detail v√† metadata
    """
    # Kh·ªüi t·∫°o result m·∫∑c ƒë·ªãnh
    default_result = {
        "product_id": "unknown",
        "url": "",
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    try:
        logger = get_logger(context)
    except Exception as e:
        # N·∫øu kh√¥ng th·ªÉ t·∫°o logger, v·∫´n ti·∫øp t·ª•c v·ªõi default result
        import logging

        logger = logging.getLogger("airflow.task")
        logger.error(f"Kh√¥ng th·ªÉ t·∫°o logger t·ª´ context: {e}")

    # L·∫•y product_info t·ª´ keyword argument ho·∫∑c context
    if not product_info:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_info = op_kwargs.get("product_info")

        if not product_info:
            product_info = context.get("product_info") or context.get("op_kwargs", {}).get(
                "product_info"
            )

    if not product_info:
        logger.error(f"Kh√¥ng t√¨m th·∫•y product_info. Context keys: {list(context.keys())}")
        # Return result v·ªõi status failed thay v√¨ raise exception
        return {
            "product_id": "unknown",
            "url": "",
            "status": "failed",
            "error": "Kh√¥ng t√¨m th·∫•y product_info trong context",
            "detail": None,
            "crawled_at": datetime.now().isoformat(),
        }

    product_id = product_info.get("product_id", "")
    product_url = product_info.get("url", "")
    product_name = product_info.get("name", "Unknown")

    logger.info("=" * 70)
    logger.info(f"üîç TASK: Crawl Product Detail - {product_name}")
    logger.info(f"üÜî Product ID: {product_id}")
    logger.info(f"üîó URL: {product_url}")
    logger.info("=" * 70)

    result = {
        "product_id": product_id,
        "url": product_url,
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    # Ki·ªÉm tra cache tr∆∞·ªõc - ∆∞u ti√™n Redis, fallback v·ªÅ file
    # Ki·ªÉm tra xem c√≥ force refresh kh√¥ng (t·ª´ Airflow Variable)
    force_refresh = Variable.get("TIKI_FORCE_REFRESH_CACHE", default_var="false").lower() == "true"
    
    if force_refresh:
        logger.info(f"üîÑ FORCE REFRESH MODE: B·ªè qua cache cho product {product_id}")
    else:
        # Th·ª≠ Redis cache tr∆∞·ªõc (nhanh h∆°n, distributed)
        logger.info(f"üîç ƒêang ki·ªÉm tra cache cho product {product_id}...")
        redis_cache = None
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_detail = redis_cache.get_cached_product_detail(product_id)
                if cached_detail:
                    # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                    has_price = cached_detail.get("price", {}).get("current_price")
                    has_sales_count = cached_detail.get("sales_count") is not None

                    # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), d√πng cache
                    if has_price and has_sales_count:
                        logger.info("=" * 70)
                        logger.info(f"‚úÖ SKIP CRAWL - Redis Cache Hit cho product {product_id}")
                        logger.info(f"   - C√≥ price: {has_price}")
                        logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                        logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                        logger.info("=" * 70)
                        result["detail"] = cached_detail
                        result["status"] = "cached"
                        return result
                elif has_price:
                    # Cache c√≥ price nh∆∞ng thi·∫øu sales_count ‚Üí crawl l·∫°i ƒë·ªÉ l·∫•y sales_count
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache thi·∫øu sales_count cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
                else:
                    # Cache kh√¥ng ƒë·∫ßy ƒë·ªß ‚Üí crawl l·∫°i
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache kh√¥ng ƒë·∫ßy ƒë·ªß cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
        except Exception:
            # Redis kh√¥ng available, fallback v·ªÅ file cache
            pass
        
        # Fallback: Ki·ªÉm tra file cache n·∫øu Redis kh√¥ng available ho·∫∑c kh√¥ng c√≥ cache
        if not force_refresh:
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None

                        # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), d√πng cache
                        if has_price and has_sales_count:
                            logger.info("=" * 70)
                            logger.info(f"‚úÖ SKIP CRAWL - File Cache Hit cho product {product_id}")
                            logger.info(f"   - C√≥ price: {has_price}")
                            logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                            logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                            logger.info("=" * 70)
                            result["detail"] = cached_detail
                            result["status"] = "cached"
                            return result
                except Exception:
                    # File cache l·ªói, ti·∫øp t·ª•c crawl
                    pass

    # Ti·∫øp t·ª•c crawl n·∫øu kh√¥ng c√≥ cache ho·∫∑c force refresh
    # (File cache check ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü tr√™n trong else block)
    
    # B·∫Øt ƒë·∫ßu crawl product detail
    try:
        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            logger.warning("=" * 70)
            logger.warning(f"‚ö†Ô∏è  SKIP CRAWL - Service Degraded cho product {product_id}")
            logger.warning("   - Service ƒëang ·ªü tr·∫°ng th√°i FAILED")
            logger.warning("   - Graceful degradation: skip crawl ƒë·ªÉ tr√°nh l√†m t·ªá h∆°n")
            logger.warning("=" * 70)
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            return result

        # Validate URL
        if not product_url or not product_url.startswith("http"):
            raise ValueError(f"URL kh√¥ng h·ª£p l·ªá: {product_url}")

        # L·∫•y c·∫•u h√¨nh
        rate_limit_delay = float(
            Variable.get("TIKI_DETAIL_RATE_LIMIT_DELAY", default_var="1.5")
        )  # Delay 1.5s cho detail (t·ªëi ∆∞u t·ª´ 2.0s)
        timeout = int(
            Variable.get("TIKI_DETAIL_CRAWL_TIMEOUT", default_var="180")
        )  # 3 ph√∫t m·ªói product (tƒÉng t·ª´ 120s ƒë·ªÉ tr√°nh timeout)

        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        # S·ª≠ d·ª•ng Selenium ƒë·ªÉ crawl detail (c·∫ßn thi·∫øt cho dynamic content)
        html_content = None
        try:
            # Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker
            def _crawl_detail_with_params():
                """Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker"""
                return crawl_product_detail_with_selenium(
                    product_url,
                    save_html=False,
                    verbose=False,  # Kh√¥ng verbose trong Airflow
                    max_retries=3,  # Retry 3 l·∫ßn (tƒÉng t·ª´ 2)
                    timeout=60,  # Timeout 60s (tƒÉng t·ª´ 25s ƒë·ªÉ ƒë·ªß th·ªùi gian cho Selenium)
                    use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                    use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
                )

            try:
                # G·ªçi v·ªõi circuit breaker
                html_content = tiki_circuit_breaker.call(_crawl_detail_with_params)
                tiki_degradation.record_success()
            except CircuitBreakerOpenError as e:
                # Circuit breaker ƒëang m·ªü
                result["error"] = f"Circuit breaker open: {str(e)}"
                result["status"] = "circuit_breaker_open"
                logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho product {product_id}: {e}")
                # Th√™m v√†o DLQ
                try:
                    crawl_error = classify_error(
                        e, context={"product_url": product_url, "product_id": product_id}
                    )
                    tiki_dlq.add(
                        task_id=f"crawl_detail_{product_id}",
                        task_type="crawl_product_detail",
                        error=crawl_error,
                        context={
                            "product_url": product_url,
                            "product_name": product_name,
                            "product_id": product_id,
                        },
                        retry_count=0,
                    )
                    logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
                except Exception as dlq_error:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
                return result
            except Exception:
                # Ghi nh·∫≠n failure
                tiki_degradation.record_failure()
                raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

            if not html_content or len(html_content) < 100:
                raise ValueError(
                    f"HTML content qu√° ng·∫Øn ho·∫∑c r·ªóng: {len(html_content) if html_content else 0} k√Ω t·ª±"
                )

        except Exception as selenium_error:
            # Log l·ªói Selenium chi ti·∫øt
            error_type = type(selenium_error).__name__
            error_msg = str(selenium_error)

            # R√∫t g·ªçn error message n·∫øu qu√° d√†i
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."

            logger.error(f"‚ùå L·ªói Selenium ({error_type}): {error_msg}")

            # Ki·ªÉm tra c√°c l·ªói ph·ªï bi·∫øn v√† ph√¢n lo·∫°i
            error_msg_lower = error_msg.lower()
            if (
                "chrome" in error_msg_lower
                or "driver" in error_msg_lower
                or "webdriver" in error_msg_lower
            ):
                result["error"] = f"Chrome/Driver error: {error_msg}"
                result["status"] = "selenium_error"
            elif (
                "timeout" in error_msg_lower
                or "timed out" in error_msg_lower
                or "time-out" in error_msg_lower
            ):
                result["error"] = f"Timeout: {error_msg}"
                result["status"] = "timeout"
            elif (
                "connection" in error_msg_lower
                or "network" in error_msg_lower
                or "refused" in error_msg_lower
            ):
                result["error"] = f"Network error: {error_msg}"
                result["status"] = "network_error"
            elif "memory" in error_msg_lower or "out of memory" in error_msg_lower:
                result["error"] = f"Memory error: {error_msg}"
                result["status"] = "memory_error"
            else:
                result["error"] = f"Selenium error: {error_msg}"
                result["status"] = "failed"

            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    selenium_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            # Kh√¥ng raise, return result v·ªõi status failed
            return result

        # Extract detail
        try:
            detail = extract_product_detail(html_content, product_url, verbose=False)

            if not detail:
                raise ValueError("Kh√¥ng extract ƒë∆∞·ª£c detail t·ª´ HTML")

        except Exception as extract_error:
            error_type = type(extract_error).__name__
            error_msg = str(extract_error)
            logger.error(f"‚ùå L·ªói khi extract detail ({error_type}): {error_msg}")
            result["error"] = f"Extract error: {error_msg}"
            result["status"] = "extract_error"
            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    extract_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(
                f"Crawl detail v∆∞·ª£t qu√° timeout {timeout}s (elapsed: {elapsed:.1f}s)"
            )

        result["detail"] = detail
        result["status"] = "success"
        result["elapsed_time"] = elapsed

        # L∆∞u v√†o cache - ∆∞u ti√™n Redis, fallback v·ªÅ file
        # Redis cache (nhanh, distributed)
        if redis_cache:
            try:
                redis_cache.cache_product_detail(product_id, detail, ttl=604800)  # 7 ng√†y
                logger.info(f"[Redis Cache] ‚úÖ ƒê√£ cache detail cho product {product_id}")
            except Exception as e:
                logger.warning(f"[Redis Cache] ‚ö†Ô∏è  L·ªói khi cache v√†o Redis: {e}")

        # File cache (fallback)
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

            temp_file = cache_file.with_suffix(".tmp")
            logger.debug(f"üíæ ƒêang l∆∞u cache v√†o: {cache_file}")

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)

            # Atomic move
            if os.name == "nt":  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))

            # Verify cache file was created
            if cache_file.exists():
                logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {elapsed:.1f}s, ƒë√£ cache v√†o {cache_file}")
                # Log sales_count n·∫øu c√≥
                if detail.get("sales_count") is not None:
                    logger.info(f"   üìä sales_count: {detail.get('sales_count')}")
                else:
                    logger.warning("   ‚ö†Ô∏è  sales_count: None (kh√¥ng t√¨m th·∫•y)")
            else:
                logger.error(f"‚ùå Cache file kh√¥ng ƒë∆∞·ª£c t·∫°o: {cache_file}")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng l∆∞u ƒë∆∞·ª£c cache: {e}", exc_info=True)
            # Kh√¥ng fail task v√¨ ƒë√£ crawl th√†nh c√¥ng, ch·ªâ kh√¥ng l∆∞u ƒë∆∞·ª£c cache

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except ValueError as e:
        result["error"] = str(e)
        result["status"] = "validation_error"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå Validation error: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        error_type = type(e).__name__
        logger.error(f"‚ùå L·ªói khi crawl detail ({error_type}): {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi product kh√°c

    # ƒê·∫£m b·∫£o lu√¥n return result, kh√¥ng bao gi·ªù raise exception
    # Ki·ªÉm tra result c√≥ h·ª£p l·ªá kh√¥ng tr∆∞·ªõc khi return
    if not result or not isinstance(result, dict):
        logger.warning(f"‚ö†Ô∏è  Result kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng default_result")
        result = default_result.copy()
        result["error"] = "Result kh√¥ng h·ª£p l·ªá"
        result["status"] = "failed"
    
    # ƒê·∫£m b·∫£o result c√≥ ƒë·∫ßy ƒë·ªß c√°c field c·∫ßn thi·∫øt
    if "product_id" not in result:
        result["product_id"] = product_id if "product_id" in locals() else "unknown"
    if "url" not in result:
        result["url"] = product_url if "product_url" in locals() else ""
    if "status" not in result:
        result["status"] = "failed"
    if "crawled_at" not in result:
        result["crawled_at"] = datetime.now().isoformat()
    
    try:
        return result
    except Exception as e:
        # N·∫øu c√≥ l·ªói khi return (kh√¥ng th·ªÉ x·∫£y ra nh∆∞ng ƒë·ªÉ an to√†n)
        logger.error(f"‚ùå L·ªói khi return result: {e}", exc_info=True)
        default_result["error"] = f"L·ªói khi return result: {str(e)}"
        default_result["product_id"] = product_id if "product_id" in locals() else "unknown"
        default_result["url"] = product_url if "product_url" in locals() else ""
        return default_result


def merge_product_details(**context) -> dict[str, Any]:
    """
    Task: Merge product details v√†o products list

    Returns:
        Dict: Products v·ªõi detail ƒë√£ merge
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Product Details")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products g·ªëc
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")

        # L·∫•y s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl t·ª´ prepare_products_for_detail
        # ƒê√¢y l√† s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø, kh√¥ng ph·∫£i t·ªïng s·ªë products
        products_to_crawl = None
        try:
            products_to_crawl = ti.xcom_pull(
                task_ids="crawl_product_details.prepare_products_for_detail"
            )
        except Exception:
            try:
                products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
            except Exception:
                pass

        # S·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl
        expected_products_count = len(products_to_crawl) if products_to_crawl else 0
        # V·ªõi batch processing, s·ªë map_index = s·ªë batches, kh√¥ng ph·∫£i s·ªë products
        batch_size = 10
        expected_crawl_count = (expected_products_count + batch_size - 1) // batch_size if expected_products_count > 0 else 0
        logger.info(f"üìä S·ªë products: {expected_products_count}, S·ªë batches d·ª± ki·∫øn: {expected_crawl_count}")

        # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø c√≥ s·∫µn b·∫±ng c√°ch th·ª≠ l·∫•y XCom
        # ƒêi·ªÅu n√†y gi√∫p x·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt s·ªë tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong
        actual_crawl_count = expected_crawl_count
        if expected_crawl_count > 0:
            # Th·ª≠ l·∫•y XCom t·ª´ map_index cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø
            # T√¨m map_index cao nh·∫•t c√≥ XCom
            task_id = "crawl_product_details.crawl_product_detail"
            max_found_index = -1

            # Binary search ƒë·ªÉ t√¨m map_index cao nh·∫•t c√≥ XCom (t·ªëi ∆∞u h∆°n linear search)
            # Th·ª≠ m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max index
            logger.info(
                f"üîç ƒêang ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø (d·ª± ki·∫øn: {expected_crawl_count})..."
            )
            test_indices = []
            if expected_crawl_count > 1000:
                # V·ªõi s·ªë l∆∞·ª£ng l·ªõn, test m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max
                step = max(100, expected_crawl_count // 20)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            elif expected_crawl_count > 100:
                # V·ªõi s·ªë l∆∞·ª£ng trung b√¨nh, test nhi·ªÅu ƒëi·ªÉm h∆°n
                step = max(50, expected_crawl_count // 10)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            else:
                # V·ªõi s·ªë l∆∞·ª£ng nh·ªè, test t·∫•t c·∫£
                test_indices = list(range(expected_crawl_count))

            # T√¨m t·ª´ cu·ªëi v·ªÅ ƒë·∫ßu ƒë·ªÉ t√¨m max index nhanh h∆°n
            for test_idx in reversed(test_indices):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[test_idx]
                    )
                    if result:
                        max_found_index = test_idx
                        logger.info(f"‚úÖ T√¨m th·∫•y XCom t·∫°i map_index {test_idx}")
                        break
                except Exception as e:
                    logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {test_idx}: {e}")
                    pass

            if max_found_index >= 0:
                # T√¨m ch√≠nh x√°c map_index cao nh·∫•t b·∫±ng c√°ch t√¨m t·ª´ max_found_index
                # Ch·ªâ th·ª≠ th√™m t·ªëi ƒëa 200 map_index ti·∫øp theo ƒë·ªÉ tr√°nh qu√° l√¢u
                logger.info(f"üîç ƒêang t√¨m ch√≠nh x√°c max index t·ª´ {max_found_index}...")
                search_range = min(max_found_index + 200, expected_crawl_count)
                for idx in range(max_found_index + 1, search_range):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[idx]
                        )
                        if result:
                            max_found_index = idx
                        else:
                            # N·∫øu kh√¥ng c√≥ result, d·ª´ng l·∫°i (c√≥ th·ªÉ ƒë√£ ƒë·∫øn cu·ªëi)
                            break
                    except Exception as e:
                        # N·∫øu exception, c√≥ th·ªÉ l√† h·∫øt map_index
                        logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {idx}: {e}")
                        break

                actual_crawl_count = max_found_index + 1
                logger.info(
                    f"‚úÖ Ph√°t hi·ªán {actual_crawl_count} map_index th·ª±c t·∫ø c√≥ XCom (d·ª± ki·∫øn: {expected_crawl_count})"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y XCom n√†o, s·ª≠ d·ª•ng expected_crawl_count: {expected_crawl_count}. "
                    f"C√≥ th·ªÉ t·∫•t c·∫£ tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong."
                )
                actual_crawl_count = expected_crawl_count

        if actual_crawl_count == 0:
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  KH√îNG C√ì PRODUCTS N√ÄO ƒê∆Ø·ª¢C CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("üí° Nguy√™n nh√¢n c√≥ th·ªÉ:")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong cache v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl tr∆∞·ªõc ƒë√≥ (t·ª´ progress file)")
            logger.warning("   - Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c prepare ƒë·ªÉ crawl")
            logger.warning("=" * 70)
            logger.warning("üí° ƒê·ªÉ force crawl l·∫°i, ki·ªÉm tra task 'prepare_products_for_detail' log")
            logger.warning("=" * 70)
            # Tr·∫£ v·ªÅ products g·ªëc kh√¥ng c√≥ detail
            return {
                "products": products,
                "stats": {
                    "total_products": len(products),
                    "with_detail": 0,
                    "cached": 0,
                    "failed": 0,
                    "timeout": 0,
                    "crawled_count": 0,
                },
                "merged_at": datetime.now().isoformat(),
            }

        # L·∫•y detail results t·ª´ Dynamic Task Mapping
        task_id = "crawl_product_details.crawl_product_detail"
        all_detail_results = []

        # L·∫•y t·∫•t c·∫£ results b·∫±ng c√°ch l·∫•y t·ª´ng map_index ƒë·ªÉ tr√°nh gi·ªõi h·∫°n XCom
        # CH·ªà l·∫•y t·ª´ map_index 0 ƒë·∫øn actual_crawl_count - 1 (kh√¥ng ph·∫£i len(products))
        logger.info(f"B·∫Øt ƒë·∫ßu l·∫•y detail results t·ª´ {actual_crawl_count} crawled products...")

        # L·∫•y theo batch ƒë·ªÉ t·ªëi ∆∞u
        batch_size = 100
        total_batches = (actual_crawl_count + batch_size - 1) // batch_size
        logger.info(
            f"üì¶ S·∫Ω l·∫•y {actual_crawl_count} results trong {total_batches} batches (m·ªói batch {batch_size})"
        )

        for batch_num, start_idx in enumerate(range(0, actual_crawl_count, batch_size), 1):
            end_idx = min(start_idx + batch_size, actual_crawl_count)
            batch_map_indexes = list(range(start_idx, end_idx))

            # Heartbeat: log m·ªói batch ƒë·ªÉ Airflow bi·∫øt task v·∫´n ƒëang ch·∫°y
            if batch_num % 5 == 0 or batch_num == 1:
                logger.info(
                    f"üíì [Heartbeat] ƒêang x·ª≠ l√Ω batch {batch_num}/{total_batches} (index {start_idx}-{end_idx-1})..."
                )

            try:
                batch_results = ti.xcom_pull(
                    task_ids=task_id, key="return_value", map_indexes=batch_map_indexes
                )

                if batch_results:
                    if isinstance(batch_results, list):
                        # List results theo th·ª© t·ª± map_indexes
                        # M·ªói result c√≥ th·ªÉ l√† list (t·ª´ batch) ho·∫∑c dict (t·ª´ single)
                        for result in batch_results:
                            if result:
                                if isinstance(result, list):
                                    # Batch result: flatten list of results
                                    all_detail_results.extend([r for r in result if r])
                                elif isinstance(result, dict):
                                    # Single result
                                    all_detail_results.append(result)
                    elif isinstance(batch_results, dict):
                        # Dict v·ªõi key l√† map_index ho·∫∑c string
                        # L·∫•y t·∫•t c·∫£ values, s·∫Øp x·∫øp theo map_index n·∫øu c√≥ th·ªÉ
                        for value in batch_results.values():
                            if value:
                                if isinstance(value, list):
                                    # Batch result: flatten
                                    all_detail_results.extend([r for r in value if r])
                                elif isinstance(value, dict):
                                    # Single result
                                    all_detail_results.append(value)
                    else:
                        # Single result
                        if isinstance(batch_results, list):
                            # Batch result: flatten
                            all_detail_results.extend([r for r in batch_results if r])
                        else:
                            all_detail_results.append(batch_results)

                # Log progress m·ªói 5 batches ho·∫∑c m·ªói 10% progress
                if batch_num % max(5, total_batches // 10) == 0:
                    progress_pct = (
                        (len(all_detail_results) / actual_crawl_count * 100)
                        if actual_crawl_count > 0
                        else 0
                    )
                    logger.info(
                        f"üìä ƒê√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%)..."
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi l·∫•y batch {start_idx}-{end_idx}: {e}")
                logger.warning("   S·∫Ω th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y...")
                # Th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y
                for map_index in batch_map_indexes:
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )
                        if result:
                            if isinstance(result, list):
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                    except Exception as e2:
                        # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e2}")
                        pass

        logger.info(
            f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(all_detail_results)} detail results qua batch (mong ƒë·ª£i {actual_crawl_count})"
        )

        # N·∫øu kh√¥ng l·∫•y ƒë·ªß ho·∫∑c c√≥ l·ªói khi l·∫•y batch, th·ª≠ l·∫•y t·ª´ng map_index m·ªôt ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu
        # KH√îNG reset all_detail_results, ch·ªâ l·∫•y th√™m nh·ªØng map_index ch∆∞a c√≥
        if len(all_detail_results) < actual_crawl_count * 0.8:  # N·∫øu thi·∫øu h∆°n 20%
            # Log c·∫£nh b√°o n·∫øu thi·∫øu nhi·ªÅu
            missing_pct = (
                ((actual_crawl_count - len(all_detail_results)) / actual_crawl_count * 100)
                if actual_crawl_count > 0
                else 0
            )
            if missing_pct > 30:
                logger.warning(
                    f"‚ö†Ô∏è  Thi·∫øu {missing_pct:.1f}% results ({actual_crawl_count - len(all_detail_results)}/{actual_crawl_count}), "
                    f"c√≥ th·ªÉ do nhi·ªÅu tasks failed ho·∫∑c timeout"
                )
            logger.warning(
                f"‚ö†Ô∏è  Ch·ªâ l·∫•y ƒë∆∞·ª£c {len(all_detail_results)}/{actual_crawl_count} results qua batch, "
                f"th·ª≠ l·∫•y t·ª´ng map_index ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu..."
            )

            # T·∫°o set c√°c product_id ƒë√£ c√≥ ƒë·ªÉ tr√°nh duplicate
            existing_product_ids = set()
            for result in all_detail_results:
                if isinstance(result, dict) and result.get("product_id"):
                    existing_product_ids.add(result.get("product_id"))

            missing_count = actual_crawl_count - len(all_detail_results)
            logger.info(
                f"üìä C·∫ßn l·∫•y th√™m ~{missing_count} results t·ª´ {actual_crawl_count} map_indexes"
            )

            # Heartbeat: log th∆∞·ªùng xuy√™n trong v√≤ng l·∫∑p d√†i
            fetched_count = 0
            for map_index in range(actual_crawl_count):  # CH·ªà l·∫•y t·ª´ 0 ƒë·∫øn actual_crawl_count - 1
                # Heartbeat m·ªói 100 items ƒë·ªÉ tr√°nh timeout
                if map_index % 100 == 0 and map_index > 0:
                    logger.info(
                        f"üíì [Heartbeat] ƒêang l·∫•y t·ª´ng map_index: {map_index}/{actual_crawl_count} "
                        f"(ƒë√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count})..."
                    )

                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[map_index]
                    )
                    if result:
                        # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ (tr√°nh duplicate)
                        product_id_to_check = None
                        if isinstance(result, dict):
                            product_id_to_check = result.get("product_id")
                        elif (
                            isinstance(result, list)
                            and len(result) > 0
                            and isinstance(result[0], dict)
                        ):
                            product_id_to_check = result[0].get("product_id")

                        # Ch·ªâ th√™m n·∫øu product_id ch∆∞a c√≥ trong danh s√°ch
                        if (
                            not product_id_to_check
                            or product_id_to_check not in existing_product_ids
                        ):
                            if isinstance(result, list):
                                for r in result:
                                    if isinstance(r, dict) and r.get("product_id"):
                                        existing_product_ids.add(r.get("product_id"))
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                if product_id_to_check:
                                    existing_product_ids.add(product_id_to_check)
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                            fetched_count += 1

                    # Log progress m·ªói 200 items
                    if (map_index + 1) % 200 == 0:
                        progress_pct = (
                            (len(all_detail_results) / actual_crawl_count * 100)
                            if actual_crawl_count > 0
                            else 0
                        )
                        logger.info(
                            f"üìä ƒê√£ l·∫•y t·ªïng {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%) t·ª´ng map_index..."
                        )
                except Exception as e:
                    # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                    logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e}")
                    pass

            logger.info(
                f"‚úÖ Sau khi l·∫•y t·ª´ng map_index: t·ªïng {len(all_detail_results)} detail results (l·∫•y th√™m {fetched_count})"
            )

        # T·∫°o dict ƒë·ªÉ lookup nhanh
        detail_dict = {}
        stats = {
            "total_products": len(products),
            "crawled_count": 0,  # S·ªë l∆∞·ª£ng products th·ª±c s·ª± ƒë∆∞·ª£c crawl detail
            "with_detail": 0,
            "cached": 0,
            "failed": 0,
            "timeout": 0,
            "degraded": 0,
            "circuit_breaker_open": 0,
        }

        logger.info(f"üìä ƒêang x·ª≠ l√Ω {len(all_detail_results)} detail results...")

        # Ki·ªÉm tra n·∫øu c√≥ qu√° nhi·ªÅu k·∫øt qu·∫£ None ho·∫∑c invalid
        valid_results = 0
        error_details = {}  # Th·ªëng k√™ chi ti·∫øt c√°c lo·∫°i l·ªói
        failed_products = []  # Danh s√°ch products b·ªã fail ƒë·ªÉ ph√¢n t√≠ch

        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get("product_id")
                if product_id:
                    detail_dict[product_id] = detail_result
                    valid_results += 1
                    status = detail_result.get("status", "failed")
                    error = detail_result.get("error")

                    # ƒê·∫øm s·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl (t·∫•t c·∫£ c√°c status tr·ª´ "not_crawled")
                    if status in ["success", "cached", "failed", "timeout", "degraded", "circuit_breaker_open", "selenium_error", "network_error", "extract_error", "validation_error", "memory_error"]:
                        stats["crawled_count"] += 1
                    
                    if status == "success":
                        stats["with_detail"] += 1
                    elif status == "cached":
                        stats["cached"] += 1
                    elif status == "timeout":
                        stats["timeout"] += 1
                        error_details["timeout"] = error_details.get("timeout", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "degraded":
                        stats["degraded"] += 1
                        error_details["degraded"] = error_details.get("degraded", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "circuit_breaker_open":
                        stats["circuit_breaker_open"] += 1
                        error_details["circuit_breaker_open"] = (
                            error_details.get("circuit_breaker_open", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "selenium_error":
                        stats["failed"] += 1
                        error_details["selenium_error"] = error_details.get("selenium_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "extract_error":
                        stats["failed"] += 1
                        error_details["extract_error"] = error_details.get("extract_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "network_error":
                        stats["failed"] += 1
                        error_details["network_error"] = error_details.get("network_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "memory_error":
                        stats["failed"] += 1
                        error_details["memory_error"] = error_details.get("memory_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "validation_error":
                        stats["failed"] += 1
                        error_details["validation_error"] = (
                            error_details.get("validation_error", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    else:
                        stats["failed"] += 1
                        error_type = status if status else "unknown"
                        error_details[error_type] = error_details.get(error_type, 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )

        logger.info(
            f"üìä C√≥ {valid_results} detail results h·ª£p l·ªá t·ª´ {len(all_detail_results)} results"
        )

        if valid_results < len(all_detail_results):
            logger.warning(
                f"‚ö†Ô∏è  C√≥ {len(all_detail_results) - valid_results} results kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu product_id"
            )

        # Log chi ti·∫øt v·ªÅ c√°c l·ªói
        if error_details:
            logger.info("=" * 70)
            logger.info("üìã PH√ÇN T√çCH C√ÅC LO·∫†I L·ªñI")
            logger.info("=" * 70)
            for error_type, count in sorted(
                error_details.items(), key=lambda x: x[1], reverse=True
            ):
                logger.info(f"  ‚ùå {error_type}: {count} products")
            logger.info("=" * 70)

            # Log m·ªôt s·ªë products b·ªã fail ƒë·∫ßu ti√™n ƒë·ªÉ ph√¢n t√≠ch
            if failed_products:
                logger.info(f"üìù M·∫´u {min(10, len(failed_products))} products b·ªã fail ƒë·∫ßu ti√™n:")
                for i, failed in enumerate(failed_products[:10], 1):
                    logger.info(
                        f"  {i}. Product ID: {failed['product_id']}, Status: {failed['status']}, Error: {failed.get('error', 'N/A')[:100]}"
                    )

        # L∆∞u th√¥ng tin l·ªói v√†o stats ƒë·ªÉ ph√¢n t√≠ch sau
        stats["error_details"] = error_details
        stats["failed_products_count"] = len(failed_products)

        # Merge detail v√†o products
        # CH·ªà l∆∞u products c√≥ detail V√Ä status == "success" (kh√¥ng l∆∞u cached ho·∫∑c failed)
        products_with_detail = []
        products_without_detail = 0
        products_cached = 0
        products_failed = 0

        for product in products:
            product_id = product.get("product_id")
            detail_result = detail_dict.get(product_id)

            if detail_result and detail_result.get("detail"):
                status = detail_result.get("status", "failed")

                # CH·ªà l∆∞u products c√≥ status == "success" (ƒë√£ crawl th√†nh c√¥ng, kh√¥ng ph·∫£i t·ª´ cache)
                if status == "success":
                    # Merge detail v√†o product
                    detail = detail_result["detail"]
                    
                    # Ki·ªÉm tra n·∫øu detail l√† None ho·∫∑c r·ªóng
                    if detail is None:
                        logger.warning(f"‚ö†Ô∏è  Detail l√† None cho product {product_id}")
                        products_failed += 1
                        continue
                    
                    # Ki·ªÉm tra n·∫øu detail l√† string (JSON), parse n√≥
                    if isinstance(detail, str):
                        # B·ªè qua string r·ªóng
                        if not detail.strip():
                            logger.warning(f"‚ö†Ô∏è  Detail l√† string r·ªóng cho product {product_id}")
                            products_failed += 1
                            continue
                        
                        try:
                            import json
                            detail = json.loads(detail)
                        except (json.JSONDecodeError, TypeError) as e:
                            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ parse detail JSON cho product {product_id}: {e}, detail type: {type(detail)}, detail value: {str(detail)[:100]}")
                            products_failed += 1
                            continue
                    
                    # Ki·ªÉm tra n·∫øu detail kh√¥ng ph·∫£i l√† dict
                    if not isinstance(detail, dict):
                        logger.warning(f"‚ö†Ô∏è  Detail kh√¥ng ph·∫£i l√† dict cho product {product_id}: {type(detail)}, value: {str(detail)[:100]}")
                        products_failed += 1
                        continue
                    
                    product_with_detail = {**product}

                    # Update c√°c tr∆∞·ªùng t·ª´ detail
                    if detail.get("price"):
                        product_with_detail["price"] = detail["price"]
                    if detail.get("rating"):
                        product_with_detail["rating"] = detail["rating"]
                    if detail.get("description"):
                        product_with_detail["description"] = detail["description"]
                    if detail.get("specifications"):
                        product_with_detail["specifications"] = detail["specifications"]
                    if detail.get("images"):
                        product_with_detail["images"] = detail["images"]
                    if detail.get("brand"):
                        product_with_detail["brand"] = detail["brand"]
                    if detail.get("seller"):
                        product_with_detail["seller"] = detail["seller"]
                    if detail.get("stock"):
                        product_with_detail["stock"] = detail["stock"]
                    if detail.get("shipping"):
                        product_with_detail["shipping"] = detail["shipping"]
                    # C·∫≠p nh·∫≠t sales_count: ∆∞u ti√™n t·ª´ detail, n·∫øu kh√¥ng c√≥ th√¨ d√πng t·ª´ product g·ªëc
                    # Ch·ªâ c·∫ßn c√≥ trong m·ªôt trong hai l√† ƒë·ªß
                    if detail.get("sales_count") is not None:
                        product_with_detail["sales_count"] = detail["sales_count"]
                    elif product.get("sales_count") is not None:
                        product_with_detail["sales_count"] = product["sales_count"]
                    # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng c√≥, gi·ªØ None (ƒë√£ c√≥ trong product g·ªëc)

                    # Th√™m metadata
                    product_with_detail["detail_crawled_at"] = detail_result.get("crawled_at")
                    product_with_detail["detail_status"] = status

                    products_with_detail.append(product_with_detail)
                elif status == "cached":
                    # Kh√¥ng l∆∞u products t·ª´ cache (ch·ªâ l∆∞u products ƒë√£ crawl m·ªõi)
                    products_cached += 1
                else:
                    # Kh√¥ng l∆∞u products b·ªã fail
                    products_failed += 1
            else:
                # Kh√¥ng l∆∞u products kh√¥ng c√≥ detail
                products_without_detail += 1

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä MERGE DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products ban ƒë·∫ßu: {stats['total_products']}")
        logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {stats['crawled_count']}")
        logger.info(f"‚úÖ C√≥ detail (success): {stats['with_detail']}")
        logger.info(f"üì¶ C√≥ detail (cached): {stats['cached']}")
        logger.info(f"‚ö†Ô∏è  Degraded: {stats['degraded']}")
        logger.info(f"‚ö° Circuit breaker open: {stats['circuit_breaker_open']}")
        logger.info(f"‚ùå Failed: {stats['failed']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout']}")

        # T√≠nh t·ªïng c√≥ detail (success + cached)
        total_with_detail = stats["with_detail"] + stats["cached"]
        
        # T·ª∑ l·ªá th√†nh c√¥ng d·ª±a tr√™n s·ªë l∆∞·ª£ng ƒë∆∞·ª£c crawl (quan tr·ªçng h∆°n)
        if stats["crawled_count"] > 0:
            success_rate = (stats["with_detail"] / stats["crawled_count"]) * 100
            logger.info(
                f"üìà T·ª∑ l·ªá th√†nh c√¥ng (d·ª±a tr√™n crawled): {stats['with_detail']}/{stats['crawled_count']} ({success_rate:.1f}%)"
            )
        
        # T·ª∑ l·ªá c√≥ detail trong t·ªïng products (ƒë·ªÉ tham kh·∫£o)
        if stats["total_products"] > 0:
            detail_coverage = total_with_detail / stats["total_products"] * 100
            logger.info(
                f"üìä T·ª∑ l·ªá c√≥ detail (trong t·ªïng products): {total_with_detail}/{stats['total_products']} ({detail_coverage:.1f}%)"
            )

        logger.info("=" * 70)
        logger.info(
            f"üíæ Products ƒë∆∞·ª£c l∆∞u v√†o file: {len(products_with_detail)} (ch·ªâ l∆∞u products c√≥ status='success')"
        )
        logger.info(f"üì¶ Products t·ª´ cache (ƒë√£ b·ªè qua): {products_cached}")
        logger.info(f"‚ùå Products b·ªã fail (ƒë√£ b·ªè qua): {products_failed}")
        logger.info(f"üö´ Products kh√¥ng c√≥ detail (ƒë√£ b·ªè qua): {products_without_detail}")
        logger.info("=" * 70)

        # C·∫≠p nh·∫≠t stats ƒë·ªÉ ph·∫£n √°nh s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c l∆∞u
        stats["products_saved"] = len(products_with_detail)
        stats["products_skipped"] = products_without_detail
        stats["products_cached_skipped"] = products_cached
        stats["products_failed_skipped"] = products_failed

        result = {
            "products": products_with_detail,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
            "note": f"Ch·ªâ l∆∞u {len(products_with_detail)} products c√≥ status='success' (ƒë√£ b·ªè qua {products_cached} cached, {products_failed} failed, {products_without_detail} kh√¥ng c√≥ detail)",
        }

        return result

    except ValueError as e:
        logger.error(f"‚ùå Validation error khi merge details: {e}", exc_info=True)
        # N·∫øu l√† validation error (thi·∫øu products), return empty result thay v√¨ raise
        return {
            "products": [],
            "stats": {
                "total_products": 0,
                "crawled_count": 0,  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
                "with_detail": 0,
                "cached": 0,
                "failed": 0,
                "timeout": 0,
            },
            "merged_at": datetime.now().isoformat(),
            "error": str(e),
        }
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge details: {e}", exc_info=True)
        # Log chi ti·∫øt context ƒë·ªÉ debug
        logger.error(f"   Context keys: {list(context.keys()) if context else 'None'}")
        try:
            ti = context.get("ti")
            if ti:
                logger.error(f"   Task ID: {ti.task_id}, DAG ID: {ti.dag_id}, Run ID: {ti.run_id}")
        except Exception:
            pass
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: L∆∞u products v·ªõi detail v√†o file

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products with Detail")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y k·∫øt qu·∫£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="crawl_product_details.merge_product_details")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_product_details")
            except Exception:
                pass

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y merge result t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})
        note = merge_result.get("note", "Crawl t·ª´ Airflow DAG v·ªõi product details")

        logger.info(f"üíæ ƒêang l∆∞u {len(products)} products v·ªõi detail...")
        
        # Log th√¥ng tin v·ªÅ crawl detail
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")
        
        if stats.get("products_skipped"):
            logger.info(f"üö´ ƒê√£ b·ªè qua {stats.get('products_skipped')} products kh√¥ng c√≥ detail")

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": note,
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} products v·ªõi detail v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products with detail: {e}", exc_info=True)
        raise


def transform_products(**context) -> dict[str, Any]:
    """
    Task: Transform d·ªØ li·ªáu s·∫£n ph·∫©m (normalize, validate, compute fields)

    Returns:
        Dict: K·∫øt qu·∫£ transform v·ªõi transformed products v√† stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Transform Products")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y file t·ª´ save_products_with_detail
        output_file = None
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
        except Exception:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
            except Exception:
                pass

        if not output_file:
            output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {output_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc file: {output_file}")

        # ƒê·ªçc products t·ª´ file
        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})
        logger.info(f"üìä T·ªïng s·ªë products trong file: {len(products)}")
        
        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")

        # B·ªï sung category_url v√† category_id tr∆∞·ªõc khi transform
        logger.info("üîó ƒêang b·ªï sung category_url v√† category_id...")
        
        # B∆∞·ªõc 1: Load category_url mapping t·ª´ products.json (n·∫øu c√≥)
        category_url_mapping = {}  # product_id -> category_url
        products_file = OUTPUT_DIR / "products.json"
        if products_file.exists():
            try:
                logger.info(f"üìñ ƒêang ƒë·ªçc category_url mapping t·ª´: {products_file}")
                with open(products_file, encoding="utf-8") as f:
                    products_data = json.load(f)
                
                products_list = []
                if isinstance(products_data, list):
                    products_list = products_data
                elif isinstance(products_data, dict):
                    if "products" in products_data:
                        products_list = products_data["products"]
                    elif "data" in products_data and isinstance(products_data["data"], dict):
                        products_list = products_data["data"].get("products", [])
                
                for product in products_list:
                    product_id = product.get("product_id")
                    category_url = product.get("category_url")
                    if product_id and category_url:
                        category_url_mapping[product_id] = category_url
                
                logger.info(f"‚úÖ ƒê√£ load {len(category_url_mapping)} category_url mappings t·ª´ products.json")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc products.json: {e}")
        
        # B∆∞·ªõc 2: Import utility ƒë·ªÉ extract category_id
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n utils module
            utils_paths = [
                "/opt/airflow/src/pipelines/crawl/utils.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl", "utils.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "crawl", "utils.py"),
            ]
            
            utils_path = None
            for path in utils_paths:
                if os.path.exists(path):
                    utils_path = path
                    break
            
            if utils_path:
                import importlib.util
                spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                extract_category_id_from_url = utils_module.extract_category_id_from_url
            else:
                # Fallback: ƒë·ªãnh nghƒ©a h√†m ƒë∆°n gi·∫£n
                import re
                def extract_category_id_from_url(url: str) -> str | None:
                    if not url:
                        return None
                    match = re.search(r"/c(\d+)", url)
                    if match:
                        return f"c{match.group(1)}"
                    return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import extract_category_id_from_url: {e}")
            import re
            def extract_category_id_from_url(url: str) -> str | None:
                if not url:
                    return None
                match = re.search(r"/c(\d+)", url)
                if match:
                    return f"c{match.group(1)}"
                return None
        
        # B∆∞·ªõc 3: B·ªï sung category_url, category_id v√† ƒë·∫£m b·∫£o category_path cho products
        updated_count = 0
        category_id_added = 0
        category_path_count = 0
        
        for product in products:
            product_id = product.get("product_id")
            
            # B·ªï sung category_url n·∫øu ch∆∞a c√≥
            if not product.get("category_url") and product_id in category_url_mapping:
                product["category_url"] = category_url_mapping[product_id]
                updated_count += 1
            
            # Extract category_id t·ª´ category_url n·∫øu c√≥
            category_url = product.get("category_url")
            if category_url and not product.get("category_id"):
                category_id = extract_category_id_from_url(category_url)
                if category_id:
                    product["category_id"] = category_id
                    category_id_added += 1
            
            # ƒê·∫£m b·∫£o category_path ƒë∆∞·ª£c gi·ªØ l·∫°i (ƒë√£ c√≥ t·ª´ cache, kh√¥ng c·∫ßn x·ª≠ l√Ω)
            if product.get("category_path"):
                category_path_count += 1
        
        if updated_count > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_url cho {updated_count} products")
        if category_id_added > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_id cho {category_id_added} products")
        if category_path_count > 0:
            logger.info(f"‚úÖ C√≥ {category_path_count} products c√≥ category_path (breadcrumb)")
        
        # Import DataTransformer
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n transform module
            transform_paths = [
                "/opt/airflow/src/pipelines/transform/transformer.py",
                os.path.abspath(
                    os.path.join(
                        dag_file_dir, "..", "..", "src", "pipelines", "transform", "transformer.py"
                    )
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "transform", "transformer.py"),
            ]

            transformer_path = None
            for path in transform_paths:
                if os.path.exists(path):
                    transformer_path = path
                    break

            if not transformer_path:
                raise ImportError("Kh√¥ng t√¨m th·∫•y transformer.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("transformer", transformer_path)
            transformer_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(transformer_module)
            DataTransformer = transformer_module.DataTransformer

            # Transform products
            transformer = DataTransformer(
                strict_validation=False, remove_invalid=True, normalize_fields=True
            )

            transformed_products, transform_stats = transformer.transform_products(
                products, validate=True
            )

            logger.info("=" * 70)
            logger.info("üìä TRANSFORM RESULTS")
            logger.info("=" * 70)
            logger.info(f"‚úÖ Valid products: {transform_stats['valid_products']}")
            logger.info(f"‚ùå Invalid products: {transform_stats['invalid_products']}")
            logger.info(f"üîÑ Duplicates removed: {transform_stats['duplicates_removed']}")
            logger.info("=" * 70)

            # L∆∞u transformed products v√†o file
            processed_dir = DATA_DIR / "processed"
            processed_dir.mkdir(parents=True, exist_ok=True)
            transformed_file = processed_dir / "products_transformed.json"

            output_data = {
                "transformed_at": datetime.now().isoformat(),
                "source_file": output_file,
                "total_products": len(products),
                "transform_stats": transform_stats,
                "products": transformed_products,
            }

            atomic_write_file(str(transformed_file), output_data, **context)
            logger.info(
                f"‚úÖ ƒê√£ l∆∞u {len(transformed_products)} transformed products v√†o: {transformed_file}"
            )

            return {
                "transformed_file": str(transformed_file),
                "transformed_count": len(transformed_products),
                "transform_stats": transform_stats,
            }

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataTransformer: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi transform products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong transform_products task: {e}", exc_info=True)
        raise


def _import_postgres_storage():
    """
    Helper function ƒë·ªÉ import PostgresStorage v·ªõi fallback logic
    H·ªó tr·ª£ c·∫£ m√¥i tr∆∞·ªùng Airflow (importlib) v√† m√¥i tr∆∞·ªùng b√¨nh th∆∞·ªùng
    
    Returns:
        PostgresStorage class ho·∫∑c None n·∫øu kh√¥ng th·ªÉ import
    """
    try:
        # Th·ª≠ import t·ª´ __init__.py c·ªßa storage module
        from pipelines.crawl.storage import PostgresStorage
        return PostgresStorage
    except ImportError:
        try:
            # Th·ª≠ import tr·ª±c ti·∫øp t·ª´ file
            from pipelines.crawl.storage.postgres_storage import PostgresStorage
            return PostgresStorage
        except ImportError:
            try:
                import importlib.util
                from pathlib import Path
                
                # T√¨m ƒë∆∞·ªùng d·∫´n ƒë·∫øn postgres_storage.py
                possible_paths = [
                    # T·ª´ /opt/airflow/src (Docker default - ∆∞u ti√™n)
                    Path("/opt/airflow/src/pipelines/crawl/storage/postgres_storage.py"),
                    # T·ª´ dag_file_dir
                    Path(dag_file_dir).parent.parent / "src" / "pipelines" / "crawl" / "storage" / "postgres_storage.py",
                    # T·ª´ current working directory
                    Path(os.getcwd()) / "src" / "pipelines" / "crawl" / "storage" / "postgres_storage.py",
                    # T·ª´ workspace root
                    Path("/workspace/src/pipelines/crawl/storage/postgres_storage.py"),
                ]
                
                postgres_storage_path = None
                for path in possible_paths:
                    if path.exists() and path.is_file():
                        postgres_storage_path = path
                        break
                
                if postgres_storage_path:
                    # S·ª≠ d·ª•ng importlib ƒë·ªÉ load tr·ª±c ti·∫øp t·ª´ file
                    spec = importlib.util.spec_from_file_location(
                        "postgres_storage", postgres_storage_path
                    )
                    if spec and spec.loader:
                        postgres_storage_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(postgres_storage_module)
                        return postgres_storage_module.PostgresStorage
                
                # N·∫øu kh√¥ng t√¨m th·∫•y file, th·ª≠ th√™m src v√†o path v√† import absolute
                src_paths = [
                    Path("/opt/airflow/src"),
                    Path(dag_file_dir).parent.parent / "src",
                    Path(os.getcwd()) / "src",
                ]
                
                for src_path in src_paths:
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))
                        try:
                            from pipelines.crawl.storage import PostgresStorage
                            return PostgresStorage
                        except ImportError:
                            try:
                                from pipelines.crawl.storage.postgres_storage import PostgresStorage
                                return PostgresStorage
                            except ImportError:
                                continue
                
                return None
            except Exception:
                return None


def load_products(**context) -> dict[str, Any]:
    """
    Task: Load d·ªØ li·ªáu ƒë√£ transform v√†o database

    Returns:
        Dict: K·∫øt qu·∫£ load v·ªõi stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Load Products to Database")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y transformed file t·ª´ transform_products task
        transform_result = None
        try:
            transform_result = ti.xcom_pull(task_ids="transform_and_load.transform_products")
        except Exception:
            try:
                transform_result = ti.xcom_pull(task_ids="transform_products")
            except Exception:
                pass

        if not transform_result:
            # Fallback: t√¨m file transformed
            processed_dir = DATA_DIR / "processed"
            transformed_file = processed_dir / "products_transformed.json"
            if transformed_file.exists():
                transform_result = {"transformed_file": str(transformed_file)}
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y transform result t·ª´ XCom ho·∫∑c file")

        transformed_file = transform_result.get("transformed_file")
        if not transformed_file or not os.path.exists(transformed_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file transformed: {transformed_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc transformed file: {transformed_file}")

        # ƒê·ªçc transformed products
        with open(transformed_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products ƒë·ªÉ load: {len(products)}")

        # Import DataLoader
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n load module
            load_paths = [
                "/opt/airflow/src/pipelines/load/loader.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "load", "loader.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "load", "loader.py"),
            ]

            loader_path = None
            for path in load_paths:
                if os.path.exists(path):
                    loader_path = path
                    break

            if not loader_path:
                raise ImportError("Kh√¥ng t√¨m th·∫•y loader.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("loader", loader_path)
            loader_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(loader_module)
            DataLoader = loader_module.DataLoader

            # L·∫•y database config t·ª´ Airflow Variables ho·∫∑c environment variables
            # ∆Øu ti√™n: Airflow Variables > Environment Variables > Default
            db_host = Variable.get("POSTGRES_HOST", default_var=os.getenv("POSTGRES_HOST", "postgres"))
            db_port = int(Variable.get("POSTGRES_PORT", default_var=os.getenv("POSTGRES_PORT", "5432")))
            db_name = Variable.get("POSTGRES_DB", default_var=os.getenv("POSTGRES_DB", "crawl_data"))
            db_user = Variable.get("POSTGRES_USER", default_var=os.getenv("POSTGRES_USER", "postgres"))
            db_password = Variable.get("POSTGRES_PASSWORD", default_var=os.getenv("POSTGRES_PASSWORD", "postgres"))

            # Load v√†o database
            loader = DataLoader(
                host=db_host,
                port=db_port,
                database=db_name,
                user=db_user,
                password=db_password,
                batch_size=100,
                enable_db=True,
            )

            try:
                # L∆∞u v√†o processed directory
                processed_dir = DATA_DIR / "processed"
                processed_dir.mkdir(parents=True, exist_ok=True)
                final_file = processed_dir / "products_final.json"

                # Kh·ªüi t·∫°o bi·∫øn ƒë·ªÉ l∆∞u s·ªë l∆∞·ª£ng products
                count_before = None
                count_after = None

                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB tr∆∞·ªõc khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is None:
                        raise ImportError("Kh√¥ng th·ªÉ import PostgresStorage")
                    storage = PostgresStorage(
                        host=db_host,
                        port=db_port,
                        database=db_name,
                        user=db_user,
                        password=db_password,
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM products;")
                            count_before = cur.fetchone()[0]
                    storage.close()
                    logger.info(f"üìä S·ªë products trong DB tr∆∞·ªõc khi load: {count_before}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB: {e}")
                    count_before = None

                load_stats = loader.load_products(
                    products,
                    save_to_file=str(final_file),
                    upsert=True,  # UPDATE n·∫øu ƒë√£ t·ªìn t·∫°i, INSERT n·∫øu m·ªõi
                    validate_before_load=True,
                )

                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB sau khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is None:
                        raise ImportError("Kh√¥ng th·ªÉ import PostgresStorage")
                    storage = PostgresStorage(
                        host=db_host,
                        port=db_port,
                        database=db_name,
                        user=db_user,
                        password=db_password,
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM products;")
                            count_after = cur.fetchone()[0]
                    storage.close()
                    logger.info(f"üìä S·ªë products trong DB sau khi load: {count_after}")
                    if count_before is not None:
                        diff = count_after - count_before
                        if diff > 0:
                            logger.info(f"‚úÖ ƒê√£ th√™m {diff} products m·ªõi v√†o DB")
                        elif diff == 0:
                            logger.info(f"‚ÑπÔ∏è  Kh√¥ng c√≥ products m·ªõi (ch·ªâ UPDATE c√°c products ƒë√£ c√≥)")
                        else:
                            logger.warning(f"‚ö†Ô∏è  S·ªë l∆∞·ª£ng products gi·∫£m {abs(diff)} (c√≥ th·ªÉ do x√≥a ho·∫∑c l·ªói)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë l∆∞·ª£ng products sau khi load: {e}")
                    count_after = None

                logger.info("=" * 70)
                logger.info("üìä LOAD RESULTS")
                logger.info("=" * 70)
                logger.info(f"‚úÖ DB loaded: {load_stats['db_loaded']} products")
                if load_stats.get("inserted_count") is not None:
                    logger.info(f"   - INSERT (products m·ªõi): {load_stats.get('inserted_count', 0)}")
                    logger.info(f"   - UPDATE (products ƒë√£ c√≥): {load_stats.get('updated_count', 0)}")
                logger.info(f"‚úÖ File loaded: {load_stats['file_loaded']}")
                logger.info(f"‚ùå Failed: {load_stats['failed_count']}")
                if count_before is not None and count_after is not None:
                    diff = count_after - count_before
                    logger.info(f"üìà DB count: {count_before} ‚Üí {count_after} (thay ƒë·ªïi: {diff:+d})")
                    if diff == 0 and load_stats.get("inserted_count", 0) == 0:
                        logger.info("‚ÑπÔ∏è  Kh√¥ng c√≥ products m·ªõi - ch·ªâ UPDATE c√°c products ƒë√£ c√≥")
                    elif diff > 0:
                        logger.info(f"‚úÖ ƒê√£ th√™m {diff} products m·ªõi v√†o DB")
                logger.info("=" * 70)
                logger.info("‚ÑπÔ∏è  L∆∞u √Ω: V·ªõi upsert=True, products ƒë√£ c√≥ s·∫Ω ƒë∆∞·ª£c UPDATE (kh√¥ng tƒÉng s·ªë l∆∞·ª£ng)")
                logger.info("‚ÑπÔ∏è  Ch·ªâ products m·ªõi (product_id ch∆∞a c√≥) m·ªõi ƒë∆∞·ª£c INSERT v√† tƒÉng s·ªë l∆∞·ª£ng")
                logger.info("=" * 70)

                return {
                    "final_file": str(final_file),
                    "load_stats": load_stats,
                }

            finally:
                loader.close()

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataLoader: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong load_products task: {e}", exc_info=True)
        raise


def validate_data(**context) -> dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl

    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("=" * 70)

    try:
        ti = context["ti"]
        output_file = None

        # ∆Øu ti√™n: L·∫•y t·ª´ save_products_with_detail (c√≥ detail)
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
            logger.info(f"L·∫•y output_file t·ª´ 'crawl_product_details.save_products_with_detail': {output_file}")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'crawl_product_details.save_products_with_detail': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                logger.info(f"L·∫•y output_file t·ª´ 'save_products_with_detail': {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products_with_detail': {e}")

        # Fallback: L·∫•y t·ª´ save_products (kh√¥ng c√≥ detail) n·∫øu kh√¥ng c√≥ file v·ªõi detail
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="process_and_save.save_products")
                logger.info(f"L·∫•y output_file t·ª´ 'process_and_save.save_products' (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")

        # C√°ch 3: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products")
                logger.info(f"L·∫•y output_file t·ª´ 'save_products' (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"ƒêang validate file: {output_file}")

        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})

        # Validation
        validation_result = {
            "file_exists": True,
            "total_products": len(products),
            "crawled_count": stats.get("crawled_count", 0),  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
            "valid_products": 0,
            "invalid_products": 0,
            "errors": [],
        }

        required_fields = ["product_id", "name", "url"]

        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []

            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)

            if is_valid:
                validation_result["valid_products"] += 1
            else:
                validation_result["invalid_products"] += 1
                validation_result["errors"].append(
                    {
                        "index": i,
                        "product_id": product.get("product_id"),
                        "missing_fields": missing_fields,
                    }
                )

        logger.info("=" * 70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng s·ªë products trong file: {validation_result['total_products']}")
        
        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")
        
        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("=" * 70)

        if validation_result["invalid_products"] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning

        return validation_result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


def aggregate_and_notify(**context) -> dict[str, Any]:
    """
    Task: T·ªïng h·ª£p d·ªØ li·ªáu v·ªõi AI v√† g·ª≠i th√¥ng b√°o qua Discord

    Returns:
        Dict: K·∫øt qu·∫£ t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ü§ñ TASK: Aggregate Data and Send Discord Notification")
    logger.info("=" * 70)

    result = {
        "aggregation_success": False,
        "ai_summary_success": False,
        "discord_notification_success": False,
        "summary": None,
        "ai_summary": None,
    }

    try:
        # L·∫•y ƒë∆∞·ªùng d·∫´n file products_with_detail.json
        output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            logger.warning(f"‚ö†Ô∏è  File kh√¥ng t·ªìn t·∫°i: {output_file}")
            logger.info("   Th·ª≠ l·∫•y t·ª´ XCom...")

            ti = context["ti"]
            try:
                output_file = ti.xcom_pull(
                    task_ids="crawl_product_details.save_products_with_detail"
                )
                logger.info(f"   L·∫•y t·ª´ XCom: {output_file}")
            except Exception:
                try:
                    output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                    logger.info(f"   L·∫•y t·ª´ XCom (kh√¥ng c√≥ prefix): {output_file}")
                except Exception as e:
                    logger.warning(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ XCom: {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"üìä ƒêang t·ªïng h·ª£p d·ªØ li·ªáu t·ª´: {output_file}")

        # 1. T·ªïng h·ª£p d·ªØ li·ªáu
        if DataAggregator is None:
            logger.warning("‚ö†Ô∏è  DataAggregator module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p")
        else:
            try:
                aggregator = DataAggregator(output_file)
                if aggregator.load_data():
                    summary = aggregator.aggregate()
                    result["summary"] = summary
                    result["aggregation_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu th√†nh c√¥ng")

                    # Log th·ªëng k√™
                    stats = summary.get("statistics", {})
                    total_products = stats.get('total_products', 0)
                    crawled_count = stats.get('crawled_count', 0)
                    with_detail = stats.get('with_detail', 0)
                    failed = stats.get('failed', 0)
                    timeout = stats.get('timeout', 0)
                    
                    logger.info(f"   üì¶ T·ªïng s·∫£n ph·∫©m: {total_products}")
                    logger.info(f"   üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
                    logger.info(f"   ‚úÖ C√≥ chi ti·∫øt (success): {with_detail}")
                    logger.info(f"   ‚ùå Th·∫•t b·∫°i: {failed}")
                    logger.info(f"   ‚è±Ô∏è  Timeout: {timeout}")
                    
                    # T√≠nh v√† hi·ªÉn th·ªã t·ª∑ l·ªá th√†nh c√¥ng
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        logger.info(f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {with_detail}/{crawled_count} ({success_rate:.1f}%)")
                    else:
                        logger.warning("   ‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c crawl detail")
                else:
                    logger.error("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu ƒë·ªÉ t·ªïng h·ª£p")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p d·ªØ li·ªáu: {e}", exc_info=True)

        # 2. T·ªïng h·ª£p v·ªõi AI
        if AISummarizer is None:
            logger.warning("‚ö†Ô∏è  AISummarizer module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p AI")
        elif result.get("summary"):
            try:
                summarizer = AISummarizer()
                ai_summary = summarizer.summarize_data(result["summary"])
                if ai_summary:
                    result["ai_summary"] = ai_summary
                    result["ai_summary_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p v·ªõi AI th√†nh c√¥ng")
                    logger.info(f"   ƒê·ªô d√†i summary: {len(ai_summary)} k√Ω t·ª±")
                else:
                    logger.warning("‚ö†Ô∏è  Kh√¥ng nh·∫≠n ƒë∆∞·ª£c summary t·ª´ AI")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v·ªõi AI: {e}", exc_info=True)

        # 3. G·ª≠i th√¥ng b√°o qua Discord
        if DiscordNotifier is None:
            logger.warning("‚ö†Ô∏è  DiscordNotifier module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua g·ª≠i th√¥ng b√°o")
        else:
            try:
                notifier = DiscordNotifier()

                # Chu·∫©n b·ªã n·ªôi dung
                if result.get("ai_summary"):
                    # G·ª≠i v·ªõi AI summary
                    stats = result.get("summary", {}).get("statistics", {})
                    crawled_at = result.get("summary", {}).get("metadata", {}).get("crawled_at", "")
                    footer_text = f"Crawl l√∫c: {crawled_at}" if crawled_at else "Tiki Data Pipeline"
                    
                    success = notifier.send_summary(
                        ai_summary=result["ai_summary"],
                        stats=stats,
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("‚úÖ ƒê√£ g·ª≠i th√¥ng b√°o qua Discord (v·ªõi AI summary)")
                    else:
                        logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ g·ª≠i th√¥ng b√°o qua Discord")
                elif result.get("summary"):
                    # G·ª≠i v·ªõi summary th√¥ng th∆∞·ªùng (kh√¥ng c√≥ AI) - s·ª≠ d·ª•ng fields thay v√¨ text
                    stats = result.get("summary", {}).get("statistics", {})
                    total_products = stats.get('total_products', 0)
                    crawled_count = stats.get('crawled_count', 0)
                    with_detail = stats.get('with_detail', 0)
                    failed = stats.get('failed', 0)
                    timeout = stats.get('timeout', 0)
                    products_saved = stats.get('products_saved', 0)
                    crawled_at = result.get("summary", {}).get("metadata", {}).get("crawled_at", "N/A")
                    
                    # T√≠nh t·ª∑ l·ªá th√†nh c√¥ng ƒë·ªÉ ch·ªçn m√†u
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        if success_rate >= 80:
                            color = 0x00FF00  # Xanh l√°
                        elif success_rate >= 50:
                            color = 0xFFA500  # Cam
                        else:
                            color = 0xFF0000  # ƒê·ªè
                    else:
                        color = 0x808080  # X√°m
                        success_rate = 0
                    
                    # T·∫°o fields cho Discord embed
                    fields = []
                    
                    # Row 1: T·ªïng quan
                    if total_products > 0:
                        fields.append({
                            "name": "üì¶ T·ªïng s·∫£n ph·∫©m",
                            "value": f"**{total_products:,}**",
                            "inline": True,
                        })
                    
                    if crawled_count > 0:
                        fields.append({
                            "name": "üîÑ ƒê√£ crawl detail",
                            "value": f"**{crawled_count:,}**",
                            "inline": True,
                        })
                    
                    if products_saved > 0:
                        fields.append({
                            "name": "üíæ ƒê√£ l∆∞u",
                            "value": f"**{products_saved:,}**",
                            "inline": True,
                        })
                    
                    # Row 2: K·∫øt qu·∫£ crawl
                    if crawled_count > 0:
                        fields.append({
                            "name": "‚úÖ Th√†nh c√¥ng",
                            "value": f"**{with_detail:,}** ({success_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    if timeout > 0:
                        timeout_rate = (timeout / crawled_count * 100) if crawled_count > 0 else 0
                        fields.append({
                            "name": "‚è±Ô∏è Timeout",
                            "value": f"**{timeout:,}** ({timeout_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    if failed > 0:
                        failed_rate = (failed / crawled_count * 100) if crawled_count > 0 else 0
                        fields.append({
                            "name": "‚ùå Th·∫•t b·∫°i",
                            "value": f"**{failed:,}** ({failed_rate:.1f}%)",
                            "inline": True,
                        })
                    
                    # T·∫°o content ng·∫Øn g·ªçn
                    content = "üìä **T·ªïng h·ª£p d·ªØ li·ªáu crawl t·ª´ Tiki.vn**\n\n"
                    if crawled_count > 0:
                        content += f"T·ª∑ l·ªá th√†nh c√¥ng: **{success_rate:.1f}%** ({with_detail}/{crawled_count} products)"
                    else:
                        content += "Ch∆∞a c√≥ products n√†o ƒë∆∞·ª£c crawl detail."
                    
                    success = notifier.send_message(
                        content=content,
                        title="üìä T·ªïng h·ª£p d·ªØ li·ªáu Tiki",
                        color=color,
                        fields=fields if fields else None,
                        footer=f"Crawl l√∫c: {crawled_at}",
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("‚úÖ ƒê√£ g·ª≠i th√¥ng b√°o qua Discord (kh√¥ng c√≥ AI)")
                    else:
                        logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ g·ª≠i th√¥ng b√°o qua Discord")
                else:
                    logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ g·ª≠i th√¥ng b√°o")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi g·ª≠i th√¥ng b√°o Discord: {e}", exc_info=True)

        logger.info("=" * 70)
        logger.info("üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P V√Ä TH√îNG B√ÅO")
        logger.info("=" * 70)
        logger.info(
            f"‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu: {'Th√†nh c√¥ng' if result['aggregation_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ T·ªïng h·ª£p AI: {'Th√†nh c√¥ng' if result['ai_summary_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ G·ª≠i Discord: {'Th√†nh c√¥ng' if result['discord_notification_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info("=" * 70)

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o: {e}", exc_info=True)
        # Kh√¥ng fail task, ch·ªâ log l·ªói
        return result


# T·∫°o DAG duy nh·∫•t v·ªõi schedule c√≥ th·ªÉ config qua Variable
with DAG(**DAG_CONFIG) as dag:

    # TaskGroup: Load v√† Prepare
    with TaskGroup("load_and_prepare") as load_group:
        # Task 0: Extract v√† load categories v√†o database (ch·∫°y ƒë·∫ßu ti√™n)
        task_extract_and_load_categories = PythonOperator(
            task_id="extract_and_load_categories_to_db",
            python_callable=extract_and_load_categories_to_db,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
        )

        # Task 1: Load danh s√°ch categories t·ª´ file ƒë·ªÉ crawl
        task_load_categories = PythonOperator(
            task_id="load_categories",
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="default_pool",
        )

        # ƒê·∫£m b·∫£o extract_and_load_categories ch·∫°y tr∆∞·ªõc load_categories
        task_extract_and_load_categories >> task_load_categories

    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup("crawl_categories") as crawl_group:
        # S·ª≠ d·ª•ng expand ƒë·ªÉ Dynamic Task Mapping
        # C·∫ßn m·ªôt task helper ƒë·ªÉ l·∫•y categories v√† t·∫°o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y categories t·ª´ XCom
            categories = None

            # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

            # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids="load_categories")
                    logger.info(
                        f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                    )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

            # C√°ch 3: Th·ª≠ l·∫•y t·ª´ upstream task (ƒë∆°n gi·∫£n h√≥a ƒë·ªÉ tr√°nh timeout)
            if not categories:
                try:
                    # L·∫•y t·ª´ task trong c√πng DAG run - ƒë∆°n gi·∫£n h√≥a
                    from airflow.models import TaskInstance

                    dag_run = context["dag_run"]
                    # L·∫•y DAG t·ª´ context thay v√¨ d√πng bi·∫øn global
                    dag_obj = context.get("dag")
                    if dag_obj:
                        upstream_task = dag_obj.get_task("load_and_prepare.load_categories")
                        upstream_ti = TaskInstance(task=upstream_task, run_id=dag_run.run_id)
                        categories = upstream_ti.xcom_pull(key="return_value")
                        logger.info(
                            f"L·∫•y categories t·ª´ TaskInstance: {len(categories) if categories else 0} items"
                        )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ TaskInstance: {e}")

            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []

            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []

            logger.info(
                f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories, t·∫°o {len(categories)} tasks cho Dynamic Task Mapping"
            )

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            return [{"category": cat} for cat in categories]

        task_prepare_crawl = PythonOperator(
            task_id="prepare_crawl_kwargs",
            python_callable=prepare_crawl_kwargs,
            execution_timeout=timedelta(minutes=1),
        )

        # Dynamic Task Mapping v·ªõi expand
        # S·ª≠ d·ª•ng expand v·ªõi op_kwargs ƒë·ªÉ tr√°nh l·ªói v·ªõi PythonOperator constructor
        task_crawl_category = PythonOperator.partial(
            task_id="crawl_category",
            python_callable=crawl_single_category,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t m·ªói category
            pool="default_pool",  # C√≥ th·ªÉ t·∫°o pool ri√™ng n·∫øu c·∫ßn
            retries=1,  # Retry 1 l·∫ßn (t·ªïng 2 l·∫ßn th·ª≠: 1 l·∫ßn ƒë·∫ßu + 1 retry)
        ).expand(op_kwargs=task_prepare_crawl.output)

    # TaskGroup: Process v√† Save
    with TaskGroup("process_and_save") as process_group:
        task_merge_products = PythonOperator(
            task_id="merge_products",
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="default_pool",
            trigger_rule="all_done",  # QUAN TR·ªåNG: Ch·∫°y khi t·∫•t c·∫£ upstream tasks done (success ho·∫∑c failed)
        )

        task_save_products = PythonOperator(
            task_id="save_products",
            python_callable=save_products,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
        )

    # TaskGroup: Crawl Product Details (Dynamic Task Mapping)
    with TaskGroup("crawl_product_details") as detail_group:

        def prepare_detail_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping detail"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # L·∫•y products t·ª´ prepare_products_for_detail
            # Task n√†y n·∫±m trong TaskGroup 'crawl_product_details', n√™n task_id ƒë·∫ßy ƒë·ªß l√† 'crawl_product_details.prepare_products_for_detail'
            products_to_crawl = None

            # L·∫•y t·ª´ upstream task (prepare_products_for_detail) - c√°ch ƒë√°ng tin c·∫≠y nh·∫•t
            # Th·ª≠ l·∫•y upstream_task_ids t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau (t∆∞∆°ng th√≠ch v·ªõi c√°c phi√™n b·∫£n Airflow)
            upstream_task_ids = []
            try:
                task_instance = context.get("task_instance")
                if task_instance:
                    # Th·ª≠ v·ªõi RuntimeTaskInstance (Airflow SDK m·ªõi)
                    if hasattr(task_instance, "upstream_task_ids"):
                        upstream_task_ids = list(task_instance.upstream_task_ids)
                    # Th·ª≠ v·ªõi ti.task (c√°ch kh√°c)
                    elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                        upstream_task_ids = list(ti.task.upstream_task_ids)
            except (AttributeError, TypeError) as e:
                logger.debug(f"   Kh√¥ng th·ªÉ l·∫•y upstream_task_ids: {e}")

            if upstream_task_ids:
                logger.info(f"üîç Upstream tasks: {upstream_task_ids}")
                # Th·ª≠ l·∫•y t·ª´ t·∫•t c·∫£ upstream tasks
                for task_id in upstream_task_ids:
                    try:
                        products_to_crawl = ti.xcom_pull(task_ids=task_id)
                        if products_to_crawl:
                            logger.info(f"‚úÖ L·∫•y XCom t·ª´ upstream task: {task_id}")
                            break
                    except Exception as e:
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ {task_id}: {e}")
                        continue

            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ c√°c c√°ch kh√°c
            if not products_to_crawl:
                try:
                    # Th·ª≠ v·ªõi task_id ƒë·∫ßy ƒë·ªß (c√≥ TaskGroup prefix)
                    products_to_crawl = ti.xcom_pull(
                        task_ids="crawl_product_details.prepare_products_for_detail"
                    )
                    logger.info(
                        "‚úÖ L·∫•y XCom t·ª´ task_id: crawl_product_details.prepare_products_for_detail"
                    )
                except Exception as e1:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng l·∫•y ƒë∆∞·ª£c v·ªõi task_id ƒë·∫ßy ƒë·ªß: {e1}")
                    try:
                        # Th·ª≠ v·ªõi task_id kh√¥ng c√≥ prefix (fallback)
                        products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
                        logger.info("‚úÖ L·∫•y XCom t·ª´ task_id: prepare_products_for_detail")
                    except Exception as e2:
                        logger.error(f"‚ùå Kh√¥ng th·ªÉ l·∫•y XCom v·ªõi c·∫£ 2 c√°ch: {e1}, {e2}")

            if not products_to_crawl:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y products t·ª´ XCom!")
                try:
                    task_instance = context.get("task_instance")
                    upstream_info = []
                    if task_instance:
                        if hasattr(task_instance, "upstream_task_ids"):
                            upstream_info = list(task_instance.upstream_task_ids)
                        elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                            upstream_info = list(ti.task.upstream_task_ids)
                    logger.error(f"   Upstream tasks: {upstream_info}")
                except Exception as e:
                    logger.error(f"   Kh√¥ng th·ªÉ l·∫•y th√¥ng tin upstream tasks: {e}")
                return []

            if not isinstance(products_to_crawl, list):
                logger.error(f"‚ùå Products kh√¥ng ph·∫£i list: {type(products_to_crawl)}")
                logger.error(f"   Value: {products_to_crawl}")
                return []

            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(products_to_crawl)} products t·ª´ XCom")

            # Batch Processing: Chia products th√†nh batches 10 products/batch
            batch_size = 10
            batches = []
            for i in range(0, len(products_to_crawl), batch_size):
                batch = products_to_crawl[i : i + batch_size]
                batches.append(batch)
            
            logger.info(f"üì¶ ƒê√£ chia th√†nh {len(batches)} batches (m·ªói batch {batch_size} products)")
            logger.info(f"   - Batch ƒë·∫ßu ti√™n: {len(batches[0]) if batches else 0} products")
            logger.info(f"   - Batch cu·ªëi c√πng: {len(batches[-1]) if batches else 0} products")

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand (m·ªói dict l√† 1 batch)
            op_kwargs_list = [{"product_batch": batch, "batch_index": idx} for idx, batch in enumerate(batches)]

            logger.info(f"üî¢ T·∫°o {len(op_kwargs_list)} op_kwargs cho Dynamic Task Mapping (batches)")
            if op_kwargs_list:
                logger.info("üìã Sample batches (first 2):")
                for i, kwargs in enumerate(op_kwargs_list[:2]):
                    batch = kwargs.get("product_batch", [])
                    batch_idx = kwargs.get("batch_index", -1)
                    logger.info(
                        f"  Batch {batch_idx}: {len(batch)} products - IDs: {[p.get('product_id') for p in batch[:3]]}..."
                    )

            return op_kwargs_list

        task_prepare_detail = PythonOperator(
            task_id="prepare_products_for_detail",
            python_callable=prepare_products_for_detail,
            execution_timeout=timedelta(minutes=5),
        )

        task_prepare_detail_kwargs = PythonOperator(
            task_id="prepare_detail_kwargs",
            python_callable=prepare_detail_kwargs,
            execution_timeout=timedelta(minutes=1),
        )

        # Dynamic Task Mapping cho crawl detail (Batch Processing)
        task_crawl_product_detail = PythonOperator.partial(
            task_id="crawl_product_detail",
            python_callable=crawl_product_batch,  # D√πng batch function thay v√¨ single
            execution_timeout=timedelta(
                minutes=20
            ),  # TƒÉng timeout l√™n 20 ph√∫t cho batch (10 products √ó 2 ph√∫t)
            pool="default_pool",
            retries=2,  # Gi·∫£m retry xu·ªëng 2 v√¨ batch c√≥ th·ªÉ retry individual products
            retry_delay=timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
        ).expand(op_kwargs=task_prepare_detail_kwargs.output)

        task_merge_product_details = PythonOperator(
            task_id="merge_product_details",
            python_callable=merge_product_details,
            execution_timeout=timedelta(minutes=60),  # TƒÉng timeout l√™n 60 ph√∫t cho nhi·ªÅu products
            pool="default_pool",
            trigger_rule="all_done",  # Ch·∫°y khi t·∫•t c·∫£ upstream tasks done
            # TƒÉng heartbeat interval ƒë·ªÉ tr√°nh timeout khi x·ª≠ l√Ω nhi·ªÅu d·ªØ li·ªáu
        )

        task_save_products_with_detail = PythonOperator(
            task_id="save_products_with_detail",
            python_callable=save_products_with_detail,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
        )

        # Dependencies trong detail group
        (
            task_prepare_detail
            >> task_prepare_detail_kwargs
            >> task_crawl_product_detail
            >> task_merge_product_details
            >> task_save_products_with_detail
        )

    # TaskGroup: Transform and Load
    with TaskGroup("transform_and_load") as transform_load_group:
        task_transform_products = PythonOperator(
            task_id="transform_products",
            python_callable=transform_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="default_pool",
        )

        task_load_products = PythonOperator(
            task_id="load_products",
            python_callable=load_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="default_pool",
        )

        # Dependencies trong transform_load group
        task_transform_products >> task_load_products

    # TaskGroup: Validate
    with TaskGroup("validate") as validate_group:
        task_validate_data = PythonOperator(
            task_id="validate_data",
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="default_pool",
        )

    # TaskGroup: Aggregate and Notify
    with TaskGroup("aggregate_and_notify") as aggregate_group:
        task_aggregate_and_notify = PythonOperator(
            task_id="aggregate_and_notify",
            python_callable=aggregate_and_notify,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
            trigger_rule="all_done",  # Ch·∫°y ngay c·∫£ khi c√≥ task upstream fail
        )

    # ƒê·ªãnh nghƒ©a dependencies
    # Flow: Load -> Crawl Categories -> Merge & Save -> Prepare Detail -> Crawl Detail -> Merge & Save Detail -> Transform -> Load -> Validate -> Aggregate

    # Dependencies gi·ªØa c√°c TaskGroup
    # Load categories tr∆∞·ªõc, sau ƒë√≥ prepare crawl kwargs
    task_load_categories >> task_prepare_crawl

    # Prepare crawl kwargs -> crawl category (dynamic mapping)
    task_prepare_crawl >> task_crawl_category

    # Crawl category -> merge products (merge ch·∫°y khi t·∫•t c·∫£ crawl tasks done)
    task_crawl_category >> task_merge_products

    # Merge -> save products
    task_merge_products >> task_save_products

    # Save products -> prepare detail -> crawl detail -> merge detail -> save detail -> transform -> load -> validate -> aggregate and notify
    task_save_products >> task_prepare_detail
    # Dependencies trong detail group ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü d√≤ng 1800
    # Flow: save_products_with_detail -> transform -> load -> validate -> aggregate_and_notify
    (
        task_save_products_with_detail
        >> task_transform_products
        >> task_load_products
        >> task_validate_data
        >> task_aggregate_and_notify
    )
