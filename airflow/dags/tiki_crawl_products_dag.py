"""
DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching
"""
import os
import sys
import json
import time
import hashlib
import tempfile
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk import TaskGroup
from airflow.models import Variable
from airflow.configuration import conf
from airflow.utils.session import provide_session

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o sys.path
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa DAG file
dag_file_dir = os.path.dirname(os.path.abspath(__file__))

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
# Trong Docker, src ƒë∆∞·ª£c mount v√†o /opt/airflow/src
possible_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    '/opt/airflow/src/pipelines/crawl',
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, '..', '..', 'src', 'pipelines', 'crawl')),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, '..', 'src', 'pipelines', 'crawl')),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    '/workspace/src/pipelines/crawl',
    # T·ª´ current working directory
    os.path.join(os.getcwd(), 'src', 'pipelines', 'crawl'),
]

# T√¨m ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá
crawl_module_path = None
crawl_products_path = None

for path in possible_paths:
    test_path = os.path.join(path, 'crawl_products.py')
    if os.path.exists(test_path):
        crawl_module_path = path
        crawl_products_path = test_path
        break

if not crawl_module_path:
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ DAG file
    relative_path = os.path.abspath(os.path.join(dag_file_dir, '..', '..', 'src', 'pipelines', 'crawl'))
    test_path = os.path.join(relative_path, 'crawl_products.py')
    if os.path.exists(test_path):
        crawl_module_path = relative_path
        crawl_products_path = test_path

# Import module
if crawl_products_path and os.path.exists(crawl_products_path):
    # S·ª≠ d·ª•ng importlib ƒë·ªÉ import tr·ª±c ti·∫øp t·ª´ file (c√°ch ƒë√°ng tin c·∫≠y nh·∫•t)
    import importlib.util
    spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_path}")
    crawl_products_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(crawl_products_module)
    
    # Extract c√°c functions c·∫ßn thi·∫øt
    crawl_category_products = crawl_products_module.crawl_category_products
    get_page_with_requests = crawl_products_module.get_page_with_requests
    parse_products_from_html = crawl_products_module.parse_products_from_html
    get_total_pages = crawl_products_module.get_total_pages
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng n·∫øu ƒë√£ th√™m v√†o sys.path
    if crawl_module_path and crawl_module_path not in sys.path:
        sys.path.insert(0, crawl_module_path)
    
    try:
        from crawl_products import (
            crawl_category_products,
            get_page_with_requests,
            parse_products_from_html,
            get_total_pages
        )
    except ImportError as e:
        # Debug: ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        debug_info = {
            'dag_file_dir': dag_file_dir,
            'cwd': os.getcwd(),
            'possible_paths': possible_paths,
            'crawl_module_path': crawl_module_path,
            'crawl_products_path': crawl_products_path,
            'sys_path': sys.path[:5]  # Ch·ªâ l·∫•y 5 ƒë·∫ßu ti√™n
        }
        
        # Ki·ªÉm tra xem /opt/airflow/src c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists('/opt/airflow/src'):
            try:
                debug_info['opt_airflow_src_contents'] = os.listdir('/opt/airflow/src')
            except:
                pass
        
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products.\n"
            f"Debug info: {debug_info}\n"
            f"L·ªói g·ªëc: {e}"
        )

# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,  # Retry 3 l·∫ßn
    'retry_delay': timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    'retry_exponential_backoff': True,  # Exponential backoff
    'max_retry_delay': timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG
DAG_CONFIG = {
    'dag_id': 'tiki_crawl_products',
    'description': 'Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a',
    'default_args': DEFAULT_ARGS,
    'schedule': timedelta(days=1),  # Ch·∫°y h√†ng ng√†y
    'start_date': datetime(2024, 1, 1),
    'catchup': False,
    'tags': ['tiki', 'crawl', 'products', 'data-pipeline'],
    'max_active_runs': 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    'max_active_tasks': 20,  # T·ªëi ƒëa 20 tasks song song
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path('/opt/airflow/data'),  # Docker mount
    Path(__file__).parent.parent.parent / 'data',  # Local development
    Path(os.getcwd()) / 'data',  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    DATA_DIR = Path(__file__).parent.parent.parent / 'data'

CATEGORIES_FILE = DATA_DIR / 'raw' / 'categories_recursive_optimized.json'
OUTPUT_DIR = DATA_DIR / 'raw' / 'products'
CACHE_DIR = OUTPUT_DIR / 'cache'
OUTPUT_FILE = OUTPUT_DIR / 'products.json'

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()


def get_logger(context):
    """L·∫•y logger t·ª´ context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging
        ti = context.get('task_instance')
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging
        return logging.getLogger("airflow.task")


def load_categories(**context) -> List[Dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file
    
    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üìñ TASK: Load Categories")
    logger.info("="*70)
    
    try:
        categories_file = str(CATEGORIES_FILE)
        logger.info(f"ƒêang ƒë·ªçc file: {categories_file}")
        
        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")
        
        with open(categories_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        
        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")
        
        # L·ªçc danh m·ª•c n·∫øu c·∫ßn (v√≠ d·ª•: ch·ªâ l·∫•y level 2-4)
        # C√≥ th·ªÉ c·∫•u h√¨nh qua Airflow Variable
        try:
            min_level = int(Variable.get('TIKI_MIN_CATEGORY_LEVEL', default_var='2'))
            max_level = int(Variable.get('TIKI_MAX_CATEGORY_LEVEL', default_var='4'))
            categories = [
                cat for cat in categories 
                if min_level <= cat.get('level', 0) <= max_level
            ]
            logger.info(f"‚úì Sau khi l·ªçc level {min_level}-{max_level}: {len(categories)} danh m·ª•c")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l·ªçc theo level: {e}")
        
        # Gi·ªõi h·∫°n s·ªë danh m·ª•c n·∫øu c·∫ßn (ƒë·ªÉ test)
        try:
            max_categories = int(Variable.get('TIKI_MAX_CATEGORIES', default_var='0'))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"‚úì Gi·ªõi h·∫°n: {max_categories} danh m·ª•c")
        except:
            pass
        
        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: Dict[str, Any] = None, **context) -> Dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)
    
    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl
    
    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context
        
    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)
    
    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get('ti')
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, 'op_kwargs', {})
            if op_kwargs:
                category = op_kwargs.get('category')
        
        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get('category') or context.get('op_kwargs', {}).get('category')
    
    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get('ti')
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")
    
    category_url = category.get('url', '')
    category_name = category.get('name', 'Unknown')
    category_id = category.get('id', '')
    
    logger.info("="*70)
    logger.info(f"üõçÔ∏è  TASK: Crawl Category - {category_name}")
    logger.info(f"üîó URL: {category_url}")
    logger.info("="*70)
    
    result = {
        'category_id': category_id,
        'category_name': category_name,
        'category_url': category_url,
        'products': [],
        'status': 'failed',
        'error': None,
        'crawled_at': datetime.now().isoformat(),
        'pages_crawled': 0,
        'products_count': 0
    }
    
    try:
        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(Variable.get('TIKI_MAX_PAGES_PER_CATEGORY', default_var='20'))  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = Variable.get('TIKI_USE_SELENIUM', default_var='false').lower() == 'true'
        timeout = int(Variable.get('TIKI_CRAWL_TIMEOUT', default_var='300'))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(Variable.get('TIKI_RATE_LIMIT_DELAY', default_var='1.0'))  # Delay 1s gi·ªØa c√°c request
        
        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)
        
        # Crawl v·ªõi timeout
        start_time = time.time()
        
        products = crawl_category_products(
            category_url,
            max_pages=max_pages if max_pages > 0 else None,
            use_selenium=use_selenium,
            cache_dir=str(CACHE_DIR)
        )
        
        elapsed = time.time() - start_time
        
        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")
        
        result['products'] = products
        result['status'] = 'success'
        result['products_count'] = len(products)
        result['elapsed_time'] = elapsed
        
        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")
        
    except TimeoutError as e:
        result['error'] = str(e)
        result['status'] = 'timeout'
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
        
    except Exception as e:
        result['error'] = str(e)
        result['status'] = 'failed'
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
    
    return result


def merge_products(**context) -> Dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c
    
    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üîÑ TASK: Merge Products")
    logger.info("="*70)
    
    try:
        from airflow.models import TaskInstance
        from airflow.models.dagrun import DagRun
        
        ti = context['ti']
        dag_run = context['dag_run']
        
        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids='load_and_prepare.load_categories')
            logger.info(f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids='load_categories')
                logger.info(f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")
        
        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")
        
        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")
        
        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            'total_categories': len(categories),
            'success_categories': 0,
            'failed_categories': 0,
            'timeout_categories': 0,
            'total_products': 0,
            'unique_products': 0
        }
        
        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = 'crawl_categories.crawl_category'
        
        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(
                task_ids=task_id,
                key='return_value'
            )
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                for result in all_results:
                    if result and isinstance(result, dict):
                        if result.get('status') == 'success':
                            stats['success_categories'] += 1
                            products = result.get('products', [])
                            all_products.extend(products)
                            stats['total_products'] += len(products)
                        elif result.get('status') == 'timeout':
                            stats['timeout_categories'] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats['failed_categories'] += 1
                            logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get('status') == 'success':
                            stats['success_categories'] += 1
                            products = result.get('products', [])
                            all_products.extend(products)
                            stats['total_products'] += len(products)
                        elif result.get('status') == 'timeout':
                            stats['timeout_categories'] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats['failed_categories'] += 1
                            logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get('status') == 'success':
                    stats['success_categories'] += 1
                    products = all_results.get('products', [])
                    all_products.extend(products)
                    stats['total_products'] += len(products)
                elif all_results.get('status') == 'timeout':
                    stats['timeout_categories'] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats['failed_categories'] += 1
                    logger.warning(f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}")
            
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                logger.info("Th·ª≠ l·∫•y t·ª´ng map_index...")
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id,
                            key='return_value',
                            map_indexes=[map_index]
                        )
                        
                        if result and isinstance(result, dict):
                            if result.get('status') == 'success':
                                stats['success_categories'] += 1
                                products = result.get('products', [])
                                all_products.extend(products)
                                stats['total_products'] += len(products)
                            elif result.get('status') == 'timeout':
                                stats['timeout_categories'] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats['failed_categories'] += 1
                                logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
                    except Exception as e:
                        stats['failed_categories'] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")
        
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats['failed_categories'] = len(categories)
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        for product in all_products:
            product_id = product.get('product_id')
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                unique_products.append(product)
        
        stats['unique_products'] = len(unique_products)
        
        logger.info("="*70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("="*70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("="*70)
        
        result = {
            'products': unique_products,
            'stats': stats,
            'merged_at': datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt
    
    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)
    
    filepath = Path(filepath)
    temp_file = filepath.with_suffix('.tmp')
    
    try:
        # Ghi v√†o temporary file
        with open(temp_file, 'w', encoding='utf-8') as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))
        
        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == 'nt':  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))
        
        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")
        
    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)
    
    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üíæ TASK: Save Products")
    logger.info("="*70)
    
    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context['ti']
        merge_result = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids='process_and_save.merge_products')
            logger.info(f"L·∫•y merge_result t·ª´ 'process_and_save.merge_products'")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids='merge_products')
                logger.info(f"L·∫•y merge_result t·ª´ 'merge_products'")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")
        
        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")
        
        products = merge_result.get('products', [])
        stats = merge_result.get('stats', {})
        
        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")
        
        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(Variable.get('TIKI_SAVE_BATCH_SIZE', default_var='10000'))
        
        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i:i + batch_size]
                batch_file = OUTPUT_DIR / f'products_batch_{i // batch_size}.json'
                batch_data = {
                    'batch_index': i // batch_size,
                    'total_batches': (len(products) + batch_size - 1) // batch_size,
                    'products': batch
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            'total_products': len(products),
            'stats': stats,
            'crawled_at': datetime.now().isoformat(),
            'note': 'Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping',
            'products': products
        }
        
        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def validate_data(**context) -> Dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl
    
    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("="*70)
    
    try:
        ti = context['ti']
        output_file = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids='process_and_save.save_products')
            logger.info(f"L·∫•y output_file t·ª´ 'process_and_save.save_products': {output_file}")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids='save_products')
                logger.info(f"L·∫•y output_file t·ª´ 'save_products': {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")
        
        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")
        
        logger.info(f"ƒêang validate file: {output_file}")
        
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        products = data.get('products', [])
        
        # Validation
        validation_result = {
            'file_exists': True,
            'total_products': len(products),
            'valid_products': 0,
            'invalid_products': 0,
            'errors': []
        }
        
        required_fields = ['product_id', 'name', 'url']
        
        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []
            
            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)
            
            if is_valid:
                validation_result['valid_products'] += 1
            else:
                validation_result['invalid_products'] += 1
                validation_result['errors'].append({
                    'index': i,
                    'product_id': product.get('product_id'),
                    'missing_fields': missing_fields
                })
        
        logger.info("="*70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("="*70)
        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("="*70)
        
        if validation_result['invalid_products'] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning
        
        return validation_result
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


# T·∫°o DAG
with DAG(**DAG_CONFIG) as dag:
    
    # TaskGroup: Load v√† Prepare
    with TaskGroup('load_and_prepare', tooltip='Load categories v√† chu·∫©n b·ªã') as load_group:
        task_load_categories = PythonOperator(
            task_id='load_categories',
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool='default_pool',
        )
    
    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup('crawl_categories', tooltip='Crawl s·∫£n ph·∫©m t·ª´ c√°c danh m·ª•c') as crawl_group:
        # S·ª≠ d·ª•ng expand ƒë·ªÉ Dynamic Task Mapping
        # C·∫ßn m·ªôt task helper ƒë·ªÉ l·∫•y categories v√† t·∫°o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging
            logger = logging.getLogger("airflow.task")
            
            ti = context['ti']
            
            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y categories t·ª´ XCom
            categories = None
            
            # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids='load_and_prepare.load_categories')
                logger.info(f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")
            
            # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids='load_categories')
                    logger.info(f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items")
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")
            
            # C√°ch 3: Th·ª≠ l·∫•y t·ª´ upstream task
            if not categories:
                try:
                    # L·∫•y t·ª´ task trong c√πng DAG run
                    from airflow.models import TaskInstance
                    dag_run = context['dag_run']
                    upstream_ti = TaskInstance(
                        task=dag.get_task('load_and_prepare.load_categories'),
                        run_id=dag_run.run_id
                    )
                    categories = upstream_ti.xcom_pull(key='return_value')
                    logger.info(f"L·∫•y categories t·ª´ TaskInstance: {len(categories) if categories else 0} items")
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ TaskInstance: {e}")
            
            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []
            
            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []
            
            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories, t·∫°o {len(categories)} tasks cho Dynamic Task Mapping")
            
            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            return [{'category': cat} for cat in categories]
        
        task_prepare_crawl = PythonOperator(
            task_id='prepare_crawl_kwargs',
            python_callable=prepare_crawl_kwargs,
            execution_timeout=timedelta(minutes=1),
        )
        
        # Dynamic Task Mapping v·ªõi expand
        # S·ª≠ d·ª•ng expand v·ªõi op_kwargs ƒë·ªÉ tr√°nh l·ªói v·ªõi PythonOperator constructor
        task_crawl_category = PythonOperator.partial(
            task_id='crawl_category',
            python_callable=crawl_single_category,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t m·ªói category
            pool='default_pool',  # C√≥ th·ªÉ t·∫°o pool ri√™ng n·∫øu c·∫ßn
            retries=1,  # Retry 1 l·∫ßn (t·ªïng 2 l·∫ßn th·ª≠: 1 l·∫ßn ƒë·∫ßu + 1 retry)
        ).expand(
            op_kwargs=task_prepare_crawl.output
        )
    
    # TaskGroup: Process v√† Save
    with TaskGroup('process_and_save', tooltip='Merge v√† l∆∞u s·∫£n ph·∫©m') as process_group:
        task_merge_products = PythonOperator(
            task_id='merge_products',
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool='default_pool',
            trigger_rule='all_done',  # QUAN TR·ªåNG: Ch·∫°y khi t·∫•t c·∫£ upstream tasks done (success ho·∫∑c failed)
        )
        
        task_save_products = PythonOperator(
            task_id='save_products',
            python_callable=save_products,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool='default_pool',
        )
    
    # TaskGroup: Validate
    with TaskGroup('validate', tooltip='Validate d·ªØ li·ªáu') as validate_group:
        task_validate_data = PythonOperator(
            task_id='validate_data',
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool='default_pool',
        )
    
    # ƒê·ªãnh nghƒ©a dependencies
    task_load_categories >> task_prepare_crawl >> task_crawl_category >> task_merge_products >> task_save_products >> task_validate_data

