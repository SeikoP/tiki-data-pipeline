"""
DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching

Dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator gi·ªØa c√°c tasks.
"""

import json
import logging
import os
import re
import shutil
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any

from airflow import DAG
from airflow.models import Variable
from airflow.providers.standard.operators.python import PythonOperator

# Import Variable v√† TaskGroup v·ªõi suppress warning
# Use standard Airflow 2.x style imports to avoid SDK API issues
try:
    from airflow.utils.task_group import TaskGroup
except ImportError:
    # Fallback: n·∫øu kh√¥ng c√≥ TaskGroup, t·∫°o dummy class ho·∫∑c d√πng SDK
    try:
        from airflow.sdk import TaskGroup
    except ImportError:

        class TaskGroup:
            def __init__(self, *args, **kwargs):
                pass

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass


# Try to import redis_cache for caching
redis_cache = None
try:
    # Ensure src path in sys.path for package-style imports

    src_path = Path("/opt/airflow/src")
    if src_path.exists() and str(src_path) not in sys.path:
        sys.path.insert(0, str(src_path))

    # try:
    #     from pipelines.crawl.storage.redis_cache import get_redis_cache  # type: ignore
    #
    #     # AVOID TOP LEVEL CONNECTION!
    #     # redis_cache = get_redis_cache("redis://redis:6379/1")  # type: ignore
    # except Exception as import_err:
    #     pass
except Exception as e:
    warnings.warn(f"Redis cache initialization skipped: {e}", stacklevel=2)
    redis_cache = None


def safe_import_attr(module_path: str, attr_name: str, fallback_paths: list[Any] | None = None):
    """
    Import an attribute from a module with optional fallback file paths.

    Args:
        module_path: Dot-separated path to the module (e.g., 'pipelines.load.module')
        attr_name: Name of the attribute to import (e.g., 'load_categories')
        fallback_paths: List of directory paths to check for the .py file if standard import fails

    Returns:
        The imported attribute or None if not found
    """
    try:
        # Standard import
        module = __import__(module_path, fromlist=[attr_name])
        return getattr(module, attr_name, None)
    except ImportError:
        if not fallback_paths:
            return None

        import importlib.util

        # Try fallback file paths
        for base in fallback_paths:
            # Convert module path to file path
            rel_file_path = module_path.replace(".", "/") + ".py"
            p = Path(base) / rel_file_path

            if p.exists():
                spec = importlib.util.spec_from_file_location(module_path, str(p))
                if spec and spec.loader:
                    try:
                        mod = importlib.util.module_from_spec(spec)
                        # Pre-populate sys.path if needed for internal imports in the module
                        if str(Path(base)) not in sys.path:
                            sys.path.insert(0, str(Path(base)))
                        spec.loader.exec_module(mod)
                        return getattr(mod, attr_name, None)
                    except Exception as e:
                        warnings.warn(f"Failed to load module from {p}: {e}", stacklevel=2)
                        continue
        return None


# Wrapper function ƒë·ªÉ t·ª´ng b∆∞·ªõc thay th·∫ø print debug b·∫±ng logger.debug
def get_variable(key: str, default: Any = None) -> Any:
    try:
        return Variable.get(key, default_var=default)
    except Exception:
        return default


# ========== LOAD CATEGORY HIERARCHY MAP FOR AUTO-PARENT-DETECTION ==========
# Cache hierarchy map globally to avoid reloading in every task
_hierarchy_map_cache = None


def get_hierarchy_map(force_reload=False):
    """Load category hierarchy map for auto-parent-detection

    This map contains all categories with their parent chains,
    allowing extract_product_detail to auto-detect missing Level 0 (parent category)
    """
    global _hierarchy_map_cache

    if _hierarchy_map_cache is not None and not force_reload:
        return _hierarchy_map_cache

    try:
        hierarchy_file = "/opt/airflow/data/raw/category_hierarchy_map.json"
        if not os.path.exists(hierarchy_file):
            # Fallback to relative path
            hierarchy_file = os.path.join(os.getcwd(), "data", "raw", "category_hierarchy_map.json")

        if os.path.exists(hierarchy_file):
            with open(hierarchy_file, encoding="utf-8") as f:
                _hierarchy_map_cache = json.load(f)
                logging.info(
                    f"‚úÖ Loaded category hierarchy map: {len(_hierarchy_map_cache)} categories"
                )
                return _hierarchy_map_cache
        else:
            logging.warning(f"‚ö†Ô∏è  Hierarchy map not found at {hierarchy_file}")
            return {}
    except Exception as e:
        logging.error(f"‚ùå Error loading hierarchy map: {e}")
        return {}


# ƒê∆∞·ªùng d·∫´n c∆° s·ªü c·ªßa file DAG
dag_file_dir = os.path.dirname(__file__)

# C√°c ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ ch·ª©a module crawl
possible_paths = [
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")),
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "pipelines", "crawl")),
    "/opt/airflow/src/pipelines/crawl",
]

# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products_detail.py")
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_products_detail", crawl_products_detail_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_detail_path}")
        crawl_products_detail_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_detail_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_product_detail_with_selenium = (
            crawl_products_detail_module.crawl_product_detail_with_selenium
        )
        extract_product_detail = crawl_products_detail_module.extract_product_detail
        crawl_product_detail_async = crawl_products_detail_module.crawl_product_detail_async
        # Optional pooled-driver variant (present in optimized module)
        crawl_product_detail_with_driver = getattr(
            crawl_products_detail_module, "crawl_product_detail_with_driver", None
        )
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products_detail module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_product_detail_with_selenium(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        def crawl_product_detail_with_driver(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        extract_product_detail = crawl_product_detail_with_selenium

        async def crawl_product_detail_async(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        # Fallback SeleniumDriverPool
        SeleniumDriverPool = None

else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng
    try:
        from crawl_products_detail import (
            crawl_product_detail_async,
            crawl_product_detail_with_driver,
            crawl_product_detail_with_selenium,
            extract_product_detail,
        )

        SeleniumDriverPool = None  # Kh√¥ng c√≥ trong crawl_products_detail, s·∫Ω import t·ª´ utils
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"L·ªói g·ªëc: {e}"
        ) from e

    # Import crawl_category_products t·ª´ crawl_products
    try:
        from crawl_products import crawl_category_products
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y h√†m crawl_category_products trong crawl_products.\n" f"L·ªói g·ªëc: {e}"
        ) from e

# Import SeleniumDriverPool t·ª´ utils ·ªü module level
try:
    # utils l√† file (.py), kh√¥ng ph·∫£i package, n√™n import tr·ª±c ti·∫øp
    import importlib.util
    import sys

    src_path = Path("/opt/airflow/src")
    if src_path.exists() and str(src_path) not in sys.path:
        sys.path.insert(0, str(src_path))

    # Th·ª≠ import t·ª´ pipelines.crawl.utils
    try:
        from pipelines.crawl.utils import SeleniumDriverPool
    except Exception:
        # Fallback: direct import t·ª´ file
        utils_path = src_path / "pipelines" / "crawl" / "utils.py"
        if utils_path.exists():
            spec = importlib.util.spec_from_file_location("crawl_utils", str(utils_path))
            if spec and spec.loader:
                crawl_utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(crawl_utils_module)
                SeleniumDriverPool = getattr(crawl_utils_module, "SeleniumDriverPool", None)
            else:
                SeleniumDriverPool = None
        else:
            SeleniumDriverPool = None
except Exception:
    SeleniumDriverPool = None  # Fallback, s·∫Ω import l·∫°i trong task n·∫øu c·∫ßn

# Import module crawl_categories_batch (for category batch processing)
crawl_categories_batch_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_categories_batch.py")
    if os.path.exists(test_path):
        crawl_categories_batch_path = test_path
        break

if crawl_categories_batch_path and os.path.exists(crawl_categories_batch_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_categories_batch", crawl_categories_batch_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_categories_batch_path}")
        crawl_categories_batch_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_categories_batch_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_category_batch = crawl_categories_batch_module.crawl_category_batch
    except Exception as e:
        # N·∫øu import l·ªói, log v√† t·∫°o fallback (s·∫Ω s·ª≠ d·ª•ng crawl_single_category)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_categories_batch module: {e}", stacklevel=2)

        # T·∫°o dummy function ƒë·ªÉ tr√°nh NameError
        crawl_category_batch = None
else:
    # Module ch∆∞a t·ªìn t·∫°i, s·∫Ω fallback v·ªÅ crawl_single_category
    crawl_category_batch = None

# Import resilience patterns
# Import tr·ª±c ti·∫øp t·ª´ng module con ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ relative imports
resilience_module_path = None
for path in possible_paths:
    test_path = os.path.join(path, "resilience", "__init__.py")
    if os.path.exists(test_path):
        resilience_module_path = os.path.join(path, "resilience")
        break

if resilience_module_path and os.path.exists(resilience_module_path):
    try:
        import importlib.util
        import sys

        # Th√™m parent path (pipelines/crawl) v√†o sys.path
        parent_path = os.path.dirname(resilience_module_path)  # .../crawl
        if parent_path not in sys.path:
            sys.path.insert(0, parent_path)

        # Th√™m grandparent path (pipelines) v√†o sys.path
        grandparent_path = os.path.dirname(parent_path)  # .../pipelines
        if grandparent_path not in sys.path:
            sys.path.insert(0, grandparent_path)

        # Import tr·ª±c ti·∫øp t·ª´ng module con v·ªõi t√™n module ƒë·∫ßy ƒë·ªß
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c module c√≥ th·ªÉ import l·∫´n nhau n·∫øu c·∫ßn

        # T·∫°o package structure trong sys.modules
        import types

        if "pipelines" not in sys.modules:
            sys.modules["pipelines"] = types.ModuleType("pipelines")
        if "pipelines.crawl" not in sys.modules:
            sys.modules["pipelines.crawl"] = types.ModuleType("pipelines.crawl")
        if "pipelines.crawl.resilience" not in sys.modules:
            sys.modules["pipelines.crawl.resilience"] = types.ModuleType(
                "pipelines.crawl.resilience"
            )

        # ƒê·∫£m b·∫£o utils module ƒë√£ ƒë∆∞·ª£c import (c·∫ßn thi·∫øt cho dead_letter_queue)
        # utils module ƒë√£ ƒë∆∞·ª£c import ·ªü tr√™n (d√≤ng 137-156)
        # N·∫øu ch∆∞a c√≥, t·∫°o fake module
        if "pipelines.crawl.utils" not in sys.modules and "crawl_utils" in sys.modules:
            sys.modules["pipelines.crawl.utils"] = sys.modules["crawl_utils"]

        # 1. Import exceptions tr∆∞·ªõc (kh√¥ng c√≥ dependency)
        exceptions_path = os.path.join(resilience_module_path, "exceptions.py")
        if os.path.exists(exceptions_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.exceptions", exceptions_path
            )
            if spec and spec.loader:
                exceptions_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.exceptions"] = exceptions_module
                spec.loader.exec_module(exceptions_module)
                CrawlError = exceptions_module.CrawlError
                classify_error = exceptions_module.classify_error
            else:
                raise ImportError(f"Kh√¥ng th·ªÉ load exceptions module t·ª´ {exceptions_path}")
        else:
            raise ImportError(f"Kh√¥ng t√¨m th·∫•y exceptions.py t·∫°i {exceptions_path}")

        # 2. Import circuit_breaker (kh√¥ng c√≥ dependency)
        circuit_breaker_path = os.path.join(resilience_module_path, "circuit_breaker.py")
        if os.path.exists(circuit_breaker_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.circuit_breaker", circuit_breaker_path
            )
            if spec and spec.loader:
                circuit_breaker_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.circuit_breaker"] = circuit_breaker_module
                spec.loader.exec_module(circuit_breaker_module)
                CircuitBreaker = circuit_breaker_module.CircuitBreaker
                CircuitBreakerOpenError = circuit_breaker_module.CircuitBreakerOpenError
            else:
                raise ImportError("Kh√¥ng th·ªÉ load circuit_breaker module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y circuit_breaker.py")

        # 3. Import dead_letter_queue (c√≥ th·ªÉ import t·ª´ utils)
        dlq_path = os.path.join(resilience_module_path, "dead_letter_queue.py")
        if os.path.exists(dlq_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.dead_letter_queue", dlq_path
            )
            if spec and spec.loader:
                dlq_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.dead_letter_queue"] = dlq_module
                spec.loader.exec_module(dlq_module)
                DeadLetterQueue = dlq_module.DeadLetterQueue
                get_dlq = dlq_module.get_dlq
            else:
                raise ImportError("Kh√¥ng th·ªÉ load dead_letter_queue module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y dead_letter_queue.py")

        # 4. Import graceful_degradation (kh√¥ng c√≥ dependency)
        degradation_path = os.path.join(resilience_module_path, "graceful_degradation.py")
        if os.path.exists(degradation_path):
            spec = importlib.util.spec_from_file_location(
                "pipelines.crawl.resilience.graceful_degradation", degradation_path
            )
            if spec and spec.loader:
                degradation_module = importlib.util.module_from_spec(spec)
                sys.modules["pipelines.crawl.resilience.graceful_degradation"] = degradation_module
                spec.loader.exec_module(degradation_module)
                GracefulDegradation = degradation_module.GracefulDegradation
                DegradationLevel = degradation_module.DegradationLevel
                get_service_health = degradation_module.get_service_health
            else:
                raise ImportError("Kh√¥ng th·ªÉ load graceful_degradation module")
        else:
            raise ImportError("Kh√¥ng t√¨m th·∫•y graceful_degradation.py")

    except Exception as e:
        # N·∫øu import l·ªói, t·∫°o dummy classes ƒë·ªÉ tr√°nh NameError
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import resilience module: {e}", stacklevel=2)

        # T·∫°o dummy classes
        class CircuitBreaker:
            def __init__(self, *args, **kwargs):
                pass

            def call(self, func, *args, **kwargs):
                return func(*args, **kwargs)

        class CircuitBreakerOpenError(Exception):
            pass

        class DeadLetterQueue:
            def add(self, *args, **kwargs):
                pass

        def get_dlq(*args, **kwargs):
            return DeadLetterQueue()

        class GracefulDegradation:
            def should_skip(self):
                return False

            def record_success(self):
                pass

            def record_failure(self):
                pass

        class DegradationLevel:
            FULL = "full"
            REDUCED = "reduced"
            MINIMAL = "minimal"
            FAILED = "failed"

        def get_service_health():
            return type(
                "ServiceHealth",
                (),
                {"register_service": lambda *args, **kwargs: GracefulDegradation()},
            )()

        def classify_error(error, **kwargs):
            return error

        class CrawlError(Exception):
            pass

else:
    # Fallback: t·∫°o dummy classes
    class CircuitBreaker:
        def __init__(self, *args, **kwargs):
            pass

        def call(self, func, *args, **kwargs):
            return func(*args, **kwargs)

    class CircuitBreakerOpenError(Exception):
        pass

    class DeadLetterQueue:
        def add(self, *args, **kwargs):
            pass

    def get_dlq(*args, **kwargs):
        return DeadLetterQueue()

    class GracefulDegradation:
        def should_skip(self):
            return False

        def record_success(self):
            pass

        def record_failure(self):
            pass

    class DegradationLevel:
        FULL = "full"
        REDUCED = "reduced"
        MINIMAL = "minimal"
        FAILED = "failed"

    def get_service_health():
        return type(
            "ServiceHealth", (), {"register_service": lambda *args, **kwargs: GracefulDegradation()}
        )()

    def classify_error(error, **kwargs):
        return error

    class CrawlError(Exception):
        pass


# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    "owner": "data-team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,  # Disable retries (was 3) - SDK API issues with retries
    "retry_delay": timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    "retry_exponential_backoff": True,  # Exponential backoff
    "max_retry_delay": timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG - C√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa t·ª± ƒë·ªông v√† th·ªß c√¥ng qua Variable
# ƒê·ªçc schedule mode t·ª´ Airflow Variable (m·∫∑c ƒë·ªãnh: 'manual' ƒë·ªÉ test)
# C√≥ th·ªÉ set Variable 'TIKI_DAG_SCHEDULE_MODE' = 'scheduled' ƒë·ªÉ ch·∫°y t·ª± ƒë·ªông
schedule_mode = get_variable("TIKI_DAG_SCHEDULE_MODE", default="manual")

# X√°c ƒë·ªãnh schedule d·ª±a tr√™n mode
if schedule_mode == "scheduled":
    dag_schedule = timedelta(days=1)  # Ch·∫°y t·ª± ƒë·ªông h√†ng ng√†y
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (T·ª± ƒë·ªông ch·∫°y h√†ng ng√†y)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "scheduled"]
else:
    dag_schedule = None  # Ch·ªâ ch·∫°y khi trigger th·ªß c√¥ng
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (Ch·∫°y th·ªß c√¥ng - Test mode)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "manual"]

# C·∫•u h√¨nh DAG schedule
dag_schedule_config = dag_schedule

# Documentation ƒë∆°n gi·∫£n cho DAG
dag_doc_md = "Crawl s·∫£n ph·∫©m t·ª´ Tiki.vn v·ªõi Dynamic Task Mapping v√† Selenium"

DAG_CONFIG = {
    "dag_id": "tiki_crawl_products_v2",
    "description": dag_description,
    "doc_md": dag_doc_md,
    "default_args": DEFAULT_ARGS,
    "schedule": dag_schedule_config,
    "start_date": datetime(2025, 11, 1),  # Ng√†y c·ªë ƒë·ªãnh trong qu√° kh·ª©
    "catchup": False,  # Kh√¥ng ch·∫°y l·∫°i c√°c task ƒë√£ b·ªè l·ª°
    "tags": dag_tags,
    "max_active_runs": 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    "max_active_tasks": 4,  # T·ªëi ∆∞u cho Local: 4 tasks * 4 drivers = 16 drivers (v·ª´a ƒë·ªß 16GB RAM)
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path("/opt/airflow/data"),  # Docker mount
    Path(__file__).parent.parent.parent / "data",  # Local development
    Path(os.getcwd()) / "data",  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    DATA_DIR = Path(__file__).parent.parent.parent / "data"

CATEGORIES_FILE = DATA_DIR / "raw" / "categories_recursive_optimized.json"
CATEGORIES_TREE_FILE = DATA_DIR / "raw" / "categories_tree.json"
OUTPUT_DIR = DATA_DIR / "raw" / "products"
CACHE_DIR = OUTPUT_DIR / "cache"
DETAIL_CACHE_DIR = OUTPUT_DIR / "detail" / "cache"
OUTPUT_FILE = OUTPUT_DIR / "products.json"
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / "products_with_detail.json"
# Progress tracking cho multi-day crawling
PROGRESS_FILE = OUTPUT_DIR / "crawl_progress.json"

# Asset/Dataset ƒë√£ ƒë∆∞·ª£c x√≥a - dependencies ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng >> operator

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()

# Kh·ªüi t·∫°o resilience patterns
# Circuit breaker cho Tiki API
tiki_circuit_breaker = CircuitBreaker(
    failure_threshold=int(get_variable("TIKI_CIRCUIT_BREAKER_FAILURE_THRESHOLD", default="5")),
    recovery_timeout=int(get_variable("TIKI_CIRCUIT_BREAKER_RECOVERY_TIMEOUT", default="60")),
    expected_exception=Exception,
    name="tiki_api",
)

# Dead Letter Queue
try:
    # Th·ª≠ d√πng Redis n·∫øu c√≥
    redis_url = get_variable("REDIS_URL", default="redis://redis:6379/3")
    tiki_dlq = get_dlq(storage_type="redis", redis_url=redis_url)
except Exception:
    # Fallback v·ªÅ file-based
    try:
        dlq_path = DATA_DIR / "dlq"
        tiki_dlq = get_dlq(storage_type="file", storage_path=str(dlq_path))
    except Exception:
        # N·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c, d√πng default
        tiki_dlq = get_dlq()

# Graceful Degradation cho Tiki service
service_health = get_service_health()
tiki_degradation = service_health.register_service(
    name="tiki",
    failure_threshold=int(get_variable("TIKI_DEGRADATION_FAILURE_THRESHOLD", default="3")),
    recovery_threshold=int(get_variable("TIKI_DEGRADATION_RECOVERY_THRESHOLD", default="5")),
)

# Import modules cho AI summarization v√† Discord notification
# T√¨m c√°c th∆∞ m·ª•c: analytics/, ai/, notifications/ ·ªü common/ (sau src/)
analytics_path = None
ai_path = None
notifications_path = None
config_path = None

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ cho c√°c modules ·ªü common/
common_base_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    "/opt/airflow/src/common",
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "common")),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "common")),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    "/workspace/src/common",
    # T·ª´ current working directory
    os.path.join(os.getcwd(), "src", "common"),
]

for common_base in common_base_paths:
    test_analytics = os.path.join(common_base, "analytics", "aggregator.py")
    test_ai = os.path.join(common_base, "ai", "summarizer.py")
    test_notifications = os.path.join(common_base, "notifications", "discord.py")
    test_config = os.path.join(common_base, "config.py")

    if os.path.exists(test_analytics):
        analytics_path = test_analytics
    if os.path.exists(test_ai):
        ai_path = test_ai
    if os.path.exists(test_notifications):
        notifications_path = test_notifications
    if os.path.exists(test_config):
        config_path = test_config

    if analytics_path and ai_path and notifications_path and config_path:
        break

# IMPORTANT: Load config.py TR∆Ø·ªöC ƒë·ªÉ ƒë·∫£m b·∫£o .env ƒë∆∞·ª£c load
# ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env ƒë∆∞·ª£c set tr∆∞·ªõc khi c√°c module kh√°c import
if config_path and os.path.exists(config_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.config", config_path)
        if spec is not None and spec.loader is not None:
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            # Config module s·∫Ω t·ª± ƒë·ªông load .env khi ƒë∆∞·ª£c import
            import warnings

            warnings.warn(
                f"‚úÖ ƒê√£ load common.config t·ª´ {config_path}, .env s·∫Ω ƒë∆∞·ª£c load t·ª± ƒë·ªông",
                stacklevel=2,
            )
    except Exception as e:
        import warnings

        warnings.warn(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load common.config: {e}", stacklevel=2)

# Import DataAggregator t·ª´ common/analytics/
if analytics_path and os.path.exists(analytics_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.analytics.aggregator", analytics_path)
        if spec is not None and spec.loader is not None:
            aggregator_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(aggregator_module)
            DataAggregator = aggregator_module.DataAggregator
        else:
            DataAggregator = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.analytics.aggregator module: {e}", stacklevel=2)
        DataAggregator = None
else:
    DataAggregator = None

# Import load_categories function from pipelines/load/load_categories_to_db.py
load_categories_db_func = safe_import_attr(
    "pipelines.load.load_categories_to_db",
    "load_categories",
    fallback_paths=[Path(p).parent.parent for p in possible_paths],
)

if not load_categories_db_func:
    warnings.warn("load_categories_to_db not available; DB load will be skipped", stacklevel=2)

# Global debug flags controlled via Airflow Variables
DEBUG_LOAD_CATEGORIES = (
    get_variable("TIKI_DEBUG_LOAD_CATEGORIES", default="false").lower() == "true"
)
DEBUG_ENRICH_CATEGORY_PATH = (
    get_variable("TIKI_DEBUG_ENRICH_CATEGORY_PATH", default="false").lower() == "true"
)


# Import AISummarizer t·ª´ common/ai/
if ai_path and os.path.exists(ai_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("common.ai.summarizer", ai_path)
        if spec is not None and spec.loader is not None:
            ai_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(ai_module)
            AISummarizer = ai_module.AISummarizer
        else:
            AISummarizer = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.ai.summarizer module: {e}", stacklevel=2)
        AISummarizer = None
else:
    AISummarizer = None

# Import DiscordNotifier t·ª´ common/notifications/
if notifications_path and os.path.exists(notifications_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "common.notifications.discord", notifications_path
        )
        if spec is not None and spec.loader is not None:
            notifications_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(notifications_module)
            DiscordNotifier = notifications_module.DiscordNotifier
        else:
            DiscordNotifier = None
    except Exception as e:
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import common.notifications.discord module: {e}", stacklevel=2)
        DiscordNotifier = None
else:
    DiscordNotifier = None


def get_logger(context):
    """L·∫•y logger t·ª´ context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging

        ti = context.get("task_instance")
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging

        return logging.getLogger("airflow.task")


def _fix_sys_path_for_pipelines_import(logger=None):
    """
    S·ª≠a sys.path v√† sys.modules ƒë·ªÉ ƒë·∫£m b·∫£o pipelines c√≥ th·ªÉ ƒë∆∞·ª£c import ƒë√∫ng c√°ch.
    X√≥a c√°c ƒë∆∞·ªùng d·∫´n con nh∆∞ /opt/airflow/src/pipelines kh·ªèi sys.path,
    x√≥a c√°c fake modules kh·ªèi sys.modules, v√† ch·ªâ gi·ªØ l·∫°i /opt/airflow/src.
    """

    if logger is None:
        logger = logging.getLogger("airflow.task")

    # X√≥a c√°c fake modules kh·ªèi sys.modules (quan tr·ªçng!)
    # C√°c fake modules n√†y ƒë∆∞·ª£c t·∫°o ·ªü ƒë·∫ßu file v√† g√¢y l·ªói 'pipelines' is not a package
    modules_to_remove = [
        module_name
        for module_name in list(sys.modules.keys())
        if module_name.startswith("pipelines")
    ]

    for module_name in modules_to_remove:
        del sys.modules[module_name]
        if logger:
            logger.info(f"üóëÔ∏è  ƒê√£ x√≥a fake module kh·ªèi sys.modules: {module_name}")

    # X√≥a c√°c ƒë∆∞·ªùng d·∫´n con kh·ªèi sys.path (g√¢y l·ªói 'pipelines' is not a package)
    paths_to_remove = []
    for path in sys.path:
        # X√≥a c√°c ƒë∆∞·ªùng d·∫´n nh∆∞ /opt/airflow/src/pipelines ho·∫∑c /opt/airflow/src/pipelines/crawl
        normalized_path = path.replace("\\", "/")
        if normalized_path.endswith("/pipelines") or normalized_path.endswith("/pipelines/crawl"):
            paths_to_remove.append(path)

    for path in paths_to_remove:
        if path in sys.path:
            sys.path.remove(path)
            if logger:
                logger.info(f"üóëÔ∏è  ƒê√£ x√≥a ƒë∆∞·ªùng d·∫´n sai kh·ªèi sys.path: {path}")

    # ƒê·∫£m b·∫£o /opt/airflow/src c√≥ trong sys.path
    possible_src_paths = [
        "/opt/airflow/src",  # Docker default path
        os.path.abspath(
            os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "..", "src")
        ),  # Local dev
    ]

    for src_path in possible_src_paths:
        if os.path.exists(src_path) and os.path.isdir(src_path):
            if src_path not in sys.path:
                sys.path.insert(0, src_path)
                if logger:
                    logger.info(f"‚úÖ ƒê√£ th√™m v√†o sys.path: {src_path}")
            return src_path

    return None


def load_categories(**context) -> list[dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file

    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    if DEBUG_LOAD_CATEGORIES:
        logger.debug("DEBUG: Task load_categories starting...")

    logger.info("=" * 70)
    logger.info("üìñ TASK: Load Categories")

    if DEBUG_LOAD_CATEGORIES:
        logger.debug(f"DEBUG: CWD: {os.getcwd()}")
        logger.debug(f"DEBUG: PYTHONPATH: {sys.path}")

    try:
        categories_file = str(CATEGORIES_FILE)
        if DEBUG_LOAD_CATEGORIES:
            logger.debug(f"DEBUG: Reading file {categories_file}")

        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")

        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)

        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")

        # L·ªçc danh m·ª•c n·∫øu c·∫ßn (v√≠ d·ª•: ch·ªâ l·∫•y level 2-4)
        # C√≥ th·ªÉ c·∫•u h√¨nh qua Airflow Variable
        try:
            min_level = int(get_variable("TIKI_MIN_CATEGORY_LEVEL", default="2"))
            max_level = int(get_variable("TIKI_MAX_CATEGORY_LEVEL", default="4"))
            categories = [
                cat for cat in categories if min_level <= cat.get("level", 0) <= max_level
            ]
            logger.info(f"‚úì Sau khi l·ªçc level {min_level}-{max_level}: {len(categories)} danh m·ª•c")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l·ªçc theo level: {e}")

        # Gi·ªõi h·∫°n s·ªë danh m·ª•c n·∫øu c·∫ßn (ƒë·ªÉ test)
        try:
            max_categories = int(get_variable("TIKI_MAX_CATEGORIES", default="0"))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"‚úì Gi·ªõi h·∫°n: {max_categories} danh m·ª•c")
        except Exception:
            pass

        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def fix_missing_parent_categories(**context) -> dict[str, Any]:
    """
    Fix missing parent categories v√† rebuild category_path ƒë·∫ßy ƒë·ªß.
    Logic t·ª´ scripts/imp/fix_missing_parents.py
    """
    logger = get_logger(context)
    
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage
        import json
        import re
        
        logger.info("=" * 70)
        logger.info("üîß FIX MISSING PARENT CATEGORIES")
        logger.info("=" * 70)
        
        # 1. Load file JSON categories
        json_file = str(CATEGORIES_FILE)
        if not os.path.exists(json_file):
            logger.warning(f"‚ö†Ô∏è  File categories kh√¥ng t·ªìn t·∫°i: {json_file}, b·ªè qua fix missing parents")
            return {"status": "skipped", "message": "File not found", "fixed_count": 0}
        
        logger.info(f"üìÇ ƒêang ƒë·ªçc file: {json_file}")
        with open(json_file, encoding="utf-8") as f:
            categories = json.load(f)
        
        url_to_cat = {cat.get("url"): cat for cat in categories}
        logger.info(f"üìä Loaded {len(categories)} categories t·ª´ file JSON")
        
        # 2. T√¨m c√°c parent categories c√≤n thi·∫øu trong DB
        storage = PostgresStorage()
        
        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                # L·∫•y t·∫•t c·∫£ categories trong DB
                cur.execute("SELECT url, parent_url FROM categories")
                db_cats = cur.fetchall()
                db_urls = {cat[0] for cat in db_cats}
                
                # T√¨m c√°c parent URLs c·∫ßn thi·∫øt
                missing_parents = set()
                for db_cat in db_cats:
                    parent_url = db_cat[1] if len(db_cat) > 1 else None
                    if parent_url and parent_url not in db_urls and parent_url in url_to_cat:
                        missing_parents.add(parent_url)
                
                if not missing_parents:
                    logger.info("‚úÖ Kh√¥ng c√≥ parent categories n√†o c√≤n thi·∫øu!")
                    return {"status": "success", "fixed_count": 0}
                
                logger.info(f"üîç T√¨m th·∫•y {len(missing_parents)} parent categories c√≤n thi·∫øu")
                
                # 3. Load c√°c parent categories c√≤n thi·∫øu
                def normalize_category_id(cat_id):
                    if not cat_id:
                        return None
                    if isinstance(cat_id, int):
                        return f"c{cat_id}"
                    cat_id_str = str(cat_id).strip()
                    if cat_id_str.startswith("c"):
                        return cat_id_str
                    return f"c{cat_id_str}"
                
                saved_count = 0
                for url in missing_parents:
                    cat = url_to_cat[url]
                    
                    # Extract category_id
                    cat_id = cat.get("category_id")
                    if not cat_id and url:
                        match = re.search(r"c?(\d+)", url)
                        if match:
                            cat_id = match.group(1)
                    cat_id = normalize_category_id(cat_id)
                    
                    # Build parent chain ƒë·ªÉ c√≥ category_path
                    path = []
                    current = cat
                    visited = set()
                    depth = 0
                    while current and depth < 10:
                        if current.get("url") in visited:
                            break
                        visited.add(current.get("url"))
                        name = current.get("name", "")
                        if name:
                            path.insert(0, name)
                        parent_url = current.get("parent_url")
                        if not parent_url:
                            break
                        if parent_url in url_to_cat:
                            current = url_to_cat[parent_url]
                        elif parent_url in db_urls:
                            # Query t·ª´ DB
                            cur.execute("SELECT name, url, parent_url FROM categories WHERE url = %s", (parent_url,))
                            row = cur.fetchone()
                            if row:
                                current = {
                                    "name": row[0],
                                    "url": row[1],
                                    "parent_url": row[2]
                                }
                            else:
                                break
                        else:
                            break
                        depth += 1
                    
                    # Insert v√†o DB
                    level_1 = path[0] if len(path) > 0 else None
                    level_2 = path[1] if len(path) > 1 else None
                    level_3 = path[2] if len(path) > 2 else None
                    level_4 = path[3] if len(path) > 3 else None
                    level_5 = path[4] if len(path) > 4 else None
                    calculated_level = len(path) if path else 0
                    root_name = path[0] if path else None
                    
                    # Check if leaf
                    parent_urls_in_db = {c[1] for c in db_cats if len(c) > 1 and c[1]}
                    is_leaf = url not in parent_urls_in_db
                    
                    try:
                        cur.execute("""
                            INSERT INTO categories (
                                category_id, name, url, image_url, parent_url, level,
                                category_path, level_1, level_2, level_3, level_4, level_5,
                                root_category_name, is_leaf
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (url) DO UPDATE SET
                                category_path = EXCLUDED.category_path,
                                level_1 = EXCLUDED.level_1,
                                level_2 = EXCLUDED.level_2,
                                level_3 = EXCLUDED.level_3,
                                level_4 = EXCLUDED.level_4,
                                level_5 = EXCLUDED.level_5,
                                level = EXCLUDED.level,
                                root_category_name = EXCLUDED.root_category_name,
                                updated_at = CURRENT_TIMESTAMP
                        """, (
                            cat_id,
                            cat.get("name"),
                            url,
                            cat.get("image_url"),
                            cat.get("parent_url"),
                            calculated_level,
                            json.dumps(path, ensure_ascii=False),
                            level_1,
                            level_2,
                            level_3,
                            level_4,
                            level_5,
                            root_name,
                            is_leaf
                        ))
                        saved_count += 1
                        logger.info(f"   ‚úÖ ƒê√£ load: {cat.get('name')}")
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è  L·ªói khi load {cat.get('name')}: {e}")
                
                conn.commit()
                logger.info(f"‚úÖ ƒê√£ load {saved_count} parent categories v√†o DB")
        
        # 4. Rebuild category_path cho t·∫•t c·∫£ categories (sau khi ƒë√≥ng connection)
        if saved_count > 0:
            logger.info("üîß ƒêang rebuild category_path cho t·∫•t c·∫£ categories...")
            
            # T·∫°o storage m·ªõi ƒë·ªÉ rebuild
            storage_rebuild = PostgresStorage()
            try:
                with storage_rebuild.get_connection() as conn_rebuild:
                    with conn_rebuild.cursor() as cur_rebuild:
                        cur_rebuild.execute("SELECT url FROM categories")
                        all_db_urls = [row[0] for row in cur_rebuild.fetchall()]
                
                categories_to_rebuild = []
                for url in all_db_urls:
                    if url in url_to_cat:
                        categories_to_rebuild.append(url_to_cat[url])
                
                if categories_to_rebuild:
                    # Rebuild paths b·∫±ng c√°ch g·ªçi save_categories l·∫°i
                    # (s·∫Ω t·ª± ƒë·ªông rebuild paths v·ªõi logic ƒë√£ ƒë∆∞·ª£c s·ª≠a)
                    rebuild_count = storage_rebuild.save_categories(
                        categories_to_rebuild,
                        only_leaf=False,
                        sync_with_products=False
                    )
                    logger.info(f"‚úÖ ƒê√£ rebuild {rebuild_count} categories")
            finally:
                storage_rebuild.close()
        
        logger.info("=" * 70)
        return {"status": "success", "fixed_count": saved_count}
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi fix missing parent categories: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "fixed_count": 0}


def load_categories_to_db_wrapper(**context):
    """
    Task wrapper to load categories from JSON file into PostgreSQL database.
    Sau khi load, t·ª± ƒë·ªông fix missing parent categories v√† rebuild paths.
    """
    logger = get_logger(context)

    try:
        json_file = str(CATEGORIES_FILE)
        if not os.path.exists(json_file):
            logger.error(f"‚ùå File categories kh√¥ng t·ªìn t·∫°i: {json_file}")
            return {"status": "error", "message": "File not found", "count": 0}

        if not load_categories_db_func:
            logger.error("‚ùå load_categories_db_func not available")
            return {"status": "error", "message": "Import failed"}

        logger.info(f"üöÄ Loading categories to DB from {json_file}")
        load_categories_db_func(json_file)
        
        # Sau khi load, fix missing parent categories
        logger.info("üîß Fixing missing parent categories...")
        fix_result = fix_missing_parent_categories(**context)
        
        if fix_result.get("status") == "success":
            fixed_count = fix_result.get("fixed_count", 0)
            if fixed_count > 0:
                logger.info(f"‚úÖ ƒê√£ fix {fixed_count} missing parent categories")
            else:
                logger.info("‚úÖ Kh√¥ng c√≥ parent categories n√†o c·∫ßn fix")
        else:
            logger.warning(f"‚ö†Ô∏è  Fix missing parents c√≥ v·∫•n ƒë·ªÅ: {fix_result.get('message', 'Unknown error')}")

        return {"status": "success", "fixed_parents": fix_result.get("fixed_count", 0)}

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories v√†o DB: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def cleanup_incomplete_products_wrapper(**context):
    """
    Task wrapper to cleanup products with missing required fields (seller and/or brand).
    Run this BEFORE crawling to allow re-crawling of incomplete data.
    
    This is a PREVENTIVE cleanup - better to clean before crawl than after load.
    """
    logger = get_logger(context)
    
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage
        
        logger.info("üßπ Starting cleanup of incomplete products (missing seller/brand)...")
        storage = PostgresStorage()
        
        # Clean up products missing seller OR brand (or both)
        # Both are required for quality data
        result = storage.cleanup_incomplete_products(
            require_seller=True, 
            require_brand=True
        )
        
        deleted_count = result["deleted_count"]
        deleted_no_seller = result.get("deleted_no_seller", 0)
        deleted_no_brand = result.get("deleted_no_brand", 0)
        deleted_both = result.get("deleted_both", 0)
        
        logger.info("=" * 70)
        logger.info(f"‚úÖ Cleanup complete: {deleted_count} products deleted")
        if deleted_count > 0:
            logger.info(f"   - Missing seller only: {deleted_no_seller}")
            logger.info(f"   - Missing brand only: {deleted_no_brand}")
            logger.info(f"   - Missing both: {deleted_both}")
        logger.info("üí° These products will be re-crawled in the next run")
        logger.info("=" * 70)
        
        return {
            "status": "success", 
            "deleted_count": deleted_count,
            "deleted_no_seller": deleted_no_seller,
            "deleted_no_brand": deleted_no_brand,
            "deleted_both": deleted_both
        }
    
    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}", exc_info=True)
        return {
            "status": "error", 
            "message": str(e), 
            "deleted_count": 0,
            "deleted_no_seller": 0,
            "deleted_no_brand": 0,
            "deleted_both": 0
        }


def cleanup_products_without_seller_wrapper(**context):
    """
    DEPRECATED: Use cleanup_incomplete_products_wrapper() instead.
    Kept for backward compatibility.
    """
    logger = get_logger(context)
    logger.warning("‚ö†Ô∏è  Using deprecated cleanup_products_without_seller_wrapper. Use cleanup_incomplete_products_wrapper instead.")
    
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage
        
        storage = PostgresStorage()
        deleted_count = storage.cleanup_products_without_seller()
        
        logger.info(f"‚úÖ Cleanup complete: {deleted_count} products deleted")
        return {"status": "success", "deleted_count": deleted_count}
    
    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "deleted_count": 0}


def cleanup_orphan_categories_wrapper(**context):
    """
    Task wrapper to cleanup categories that don't have any matching products.
    Run this after loading categories to keep the table clean.
    
    X√≥a:
    1. Categories c√≥ product_count = 0 (ho·∫∑c NULL)
    2. Leaf categories kh√¥ng c√≥ products trong b·∫£ng products
    """
    logger = get_logger(context)
    
    try:
        from pipelines.crawl.storage.postgres_storage import PostgresStorage
        
        logger.info("=" * 70)
        logger.info("üßπ CLEANUP ORPHAN CATEGORIES")
        logger.info("=" * 70)
        
        storage = PostgresStorage()
        
        # 1. X√≥a categories c√≥ product_count = 0 ho·∫∑c NULL
        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                # X√≥a categories c√≥ product_count = 0 ho·∫∑c NULL
                cur.execute("""
                    DELETE FROM categories
                    WHERE (product_count = 0 OR product_count IS NULL)
                    AND is_leaf = true
                """)
                deleted_zero_count = cur.rowcount
                
                # 2. X√≥a leaf categories kh√¥ng c√≥ products trong b·∫£ng products
                cur.execute("""
                    DELETE FROM categories
                    WHERE is_leaf = true
                    AND NOT EXISTS (
                        SELECT 1 FROM products p 
                        WHERE p.category_id = categories.category_id
                    )
                """)
                deleted_no_products = cur.rowcount
                
                conn.commit()
                
                total_deleted = deleted_zero_count + deleted_no_products
                
                logger.info(f"üìä Cleanup results:")
                logger.info(f"   - Categories v·ªõi product_count = 0: {deleted_zero_count}")
                logger.info(f"   - Categories kh√¥ng c√≥ products: {deleted_no_products}")
                logger.info(f"   - T·ªïng c·ªông: {total_deleted} categories ƒë√£ x√≥a")
                logger.info("=" * 70)
        
        # G·ªçi cleanup_orphan_categories t·ª´ storage ƒë·ªÉ ƒë·∫£m b·∫£o consistency
        # (n√≥ s·∫Ω x√≥a c√°c categories kh√¥ng c√≥ products)
        additional_deleted = storage.cleanup_orphan_categories()
        
        total_deleted = total_deleted + additional_deleted
        
        logger.info(f"‚úÖ Cleanup complete: {total_deleted} orphan categories deleted")
        return {
            "status": "success", 
            "deleted_count": total_deleted,
            "deleted_zero_count": deleted_zero_count,
            "deleted_no_products": deleted_no_products + additional_deleted
        }
    
    except Exception as e:
        logger.error(f"‚ùå Error during cleanup: {e}", exc_info=True)
        return {"status": "error", "message": str(e), "deleted_count": 0}


def crawl_single_category(category: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)

    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl

    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)

    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get("ti")
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                category = op_kwargs.get("category")

        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get("category") or context.get("op_kwargs", {}).get("category")

    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get("ti")
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")

    category_url = category.get("url", "")
    category_name = category.get("name", "Unknown")
    category_id = category.get("id", "")

    logger.info("=" * 70)
    logger.info(f"üõçÔ∏è  TASK: Crawl Category - {category_name}")
    logger.info(f"üîó URL: {category_url}")
    logger.info("=" * 70)

    result = {
        "category_id": category_id,
        "category_name": category_name,
        "category_url": category_url,
        "products": [],
        "status": "failed",
        "error": None,
        "crawled_at": datetime.now().isoformat(),
        "pages_crawled": 0,
        "products_count": 0,
    }

    try:
        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            logger.warning(f"‚ö†Ô∏è  Service degraded, skip category {category_name}")
            return result

        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(
            get_variable("TIKI_MAX_PAGES_PER_CATEGORY", default="20")
        )  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = get_variable("TIKI_USE_SELENIUM", default="false").lower() == "true"
        timeout = int(get_variable("TIKI_CRAWL_TIMEOUT", default="300"))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(
            get_variable("TIKI_RATE_LIMIT_DELAY", default="1.0")
        )  # Delay 1s gi·ªØa c√°c request

        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        def _crawl_with_params():
            """Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker"""
            return crawl_category_products(
                category_url,
                max_pages=max_pages if max_pages > 0 else None,
                use_selenium=use_selenium,
                cache_dir=str(CACHE_DIR),
                use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
            )

        try:
            # G·ªçi v·ªõi circuit breaker
            products = tiki_circuit_breaker.call(_crawl_with_params)
            tiki_degradation.record_success()
        except CircuitBreakerOpenError as e:
            # Circuit breaker ƒëang m·ªü
            result["error"] = f"Circuit breaker open: {str(e)}"
            result["status"] = "circuit_breaker_open"
            logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho category {category_name}: {e}")
            # Th√™m v√†o DLQ
            try:
                crawl_error = classify_error(
                    e, context={"category_url": category_url, "category_id": category_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_category_{category_id}",
                    task_type="crawl_category",
                    error=crawl_error,
                    context={
                        "category_url": category_url,
                        "category_name": category_name,
                        "category_id": category_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result
        except Exception:
            # Ghi nh·∫≠n failure
            tiki_degradation.record_failure()
            raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")

        result["products"] = products
        result["status"] = "success"
        result["products_count"] = len(products)
        result["elapsed_time"] = elapsed

        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"category_url": category_url, "category_id": category_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_category_{category_id}",
                task_type="crawl_category",
                error=crawl_error,
                context={
                    "category_url": category_url,
                    "category_name": category_name,
                    "category_id": category_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_category_{category_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    return result


def merge_products(**context) -> dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c

    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Products")
    logger.info("=" * 70)

    try:

        ti = context["ti"]

        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
            logger.info(
                f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
            )
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")

        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            "total_categories": len(categories),
            "success_categories": 0,
            "failed_categories": 0,
            "timeout_categories": 0,
            "total_products": 0,
            "unique_products": 0,
        }

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = "crawl_categories.crawl_category"

        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(task_ids=task_id, key="return_value")

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                # V·ªõi batch processing, m·ªói ph·∫ßn t·ª≠ c√≥ th·ªÉ l√† list of results (t·ª´ 1 batch)
                for result in all_results:
                    # Check if result is a list (batch result) ho·∫∑c dict (single category result)
                    if isinstance(result, list):
                        # Batch result - flatten it
                        for single_result in result:
                            if single_result and isinstance(single_result, dict):
                                if single_result.get("status") == "success":
                                    stats["success_categories"] += 1
                                    products = single_result.get("products", [])
                                    all_products.extend(products)
                                    stats["total_products"] += len(products)
                                elif single_result.get("status") == "timeout":
                                    stats["timeout_categories"] += 1
                                    logger.warning(
                                        f"‚è±Ô∏è  Category {single_result.get('category_name')} timeout"
                                    )
                                else:
                                    stats["failed_categories"] += 1
                                    logger.warning(
                                        f"‚ùå Category {single_result.get('category_name')} failed: {single_result.get('error')}"
                                    )
                    elif result and isinstance(result, dict):
                        # Single category result
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get("status") == "success":
                    stats["success_categories"] += 1
                    products = all_results.get("products", [])
                    all_products.extend(products)
                    stats["total_products"] += len(products)
                elif all_results.get("status") == "timeout":
                    stats["timeout_categories"] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats["failed_categories"] += 1
                    logger.warning(
                        f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}"
                    )

            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                # Try fetching individual map_index results
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )

                        if result and isinstance(result, dict):
                            if result.get("status") == "success":
                                stats["success_categories"] += 1
                                products = result.get("products", [])
                                all_products.extend(products)
                                stats["total_products"] += len(products)
                            elif result.get("status") == "timeout":
                                stats["timeout_categories"] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats["failed_categories"] += 1
                                logger.warning(
                                    f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                                )
                    except Exception as e:
                        stats["failed_categories"] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")

        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats["failed_categories"] = len(categories)

        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        products_with_sales_count = 0
        for product in all_products:
            product_id = product.get("product_id")
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                # ƒê·∫£m b·∫£o sales_count lu√¥n c√≥ trong product (k·ªÉ c·∫£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                elif product.get("sales_count") is not None:
                    products_with_sales_count += 1
                unique_products.append(product)

        # Log th·ªëng k√™ sales_count
        logger.info(
            f"üìä Products c√≥ sales_count: {products_with_sales_count}/{len(unique_products)} ({products_with_sales_count/len(unique_products)*100:.1f}%)"
            if unique_products
            else "üìä Products c√≥ sales_count: 0/0"
        )

        stats["unique_products"] = len(unique_products)

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("=" * 70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("=" * 70)

        result = {
            "products": unique_products,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt

    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi v√†o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == "nt":  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))

        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")

    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)

    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products")
    logger.info("=" * 70)

    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context["ti"]
        merge_result = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
            # Get merge result from upstream task
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
                # Fallback to merge_products without prefix
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")

        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(get_variable("TIKI_SAVE_BATCH_SIZE", default="10000"))

        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                batch_file = OUTPUT_DIR / f"products_batch_{i // batch_size}.json"
                batch_data = {
                    "batch_index": i // batch_size,
                    "total_batches": (len(products) + batch_size - 1) // batch_size,
                    "products": batch,
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def prepare_products_for_detail(**context) -> list[dict[str, Any]]:
    """
    Task: Chu·∫©n b·ªã danh s√°ch products ƒë·ªÉ crawl detail

    T·ªëi ∆∞u cho multi-day crawling:
    - Ch·ªâ crawl products ch∆∞a c√≥ detail
    - Chia th√†nh batches theo ng√†y (c√≥ th·ªÉ crawl trong nhi·ªÅu ng√†y)
    - Ki·ªÉm tra cache v√† progress ƒë·ªÉ tr√°nh crawl l·∫°i
    - Track progress ƒë·ªÉ resume t·ª´ ƒëi·ªÉm d·ª´ng

    Returns:
        List[Dict]: List c√°c dict ch·ª©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìã TASK: Prepare Products for Detail Crawling (Multi-Day Support)")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products t·ª´ task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products: {len(products)}")

        # ƒê·ªçc progress file ƒë·ªÉ bi·∫øt ƒë√£ crawl ƒë·∫øn ƒë√¢u
        progress = {
            "crawled_product_ids": set(),
            "last_crawled_index": 0,
            "total_crawled": 0,
            "last_updated": None,
        }

        if PROGRESS_FILE.exists():
            try:
                with open(PROGRESS_FILE, encoding="utf-8") as f:
                    saved_progress = json.load(f)
                    progress["crawled_product_ids"] = set(
                        saved_progress.get("crawled_product_ids", [])
                    )
                    progress["last_crawled_index"] = saved_progress.get("last_crawled_index", 0)
                    progress["total_crawled"] = saved_progress.get("total_crawled", 0)
                    progress["last_updated"] = saved_progress.get("last_updated")
                    logger.info(
                        f"üìÇ ƒê√£ load progress: {len(progress['crawled_product_ids'])} products ƒë√£ crawl"
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c progress file: {e}")

        # L·ªçc products c·∫ßn crawl detail
        products_to_crawl = []
        cache_hits = 0
        already_crawled = 0
        db_hits = 0  # Products ƒë√£ c√≥ trong DB

        products_per_day = int(
            get_variable("TIKI_PRODUCTS_PER_DAY", default="500")
        )  # M·∫∑c ƒë·ªãnh 280 products/ng√†y (~30 ph√∫t)
        max_products = int(
            get_variable("TIKI_MAX_PRODUCTS_FOR_DETAIL", default="0")
        )  # 0 = kh√¥ng gi·ªõi h·∫°n

        logger.info(
            f"‚öôÔ∏è  C·∫•u h√¨nh: {products_per_day} products/ng√†y, max: {max_products if max_products > 0 else 'kh√¥ng gi·ªõi h·∫°n'}"
        )

        # Ki·ªÉm tra products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß (ƒë·ªÉ tr√°nh crawl l·∫°i)
        # Ch·ªâ skip products c√≥ price v√† sales_count (detail ƒë·∫ßy ƒë·ªß)
        existing_product_ids_in_db = set()
        try:
            PostgresStorage = _import_postgres_storage()
            if PostgresStorage is None:
                logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ import PostgresStorage, b·ªè qua ki·ªÉm tra database")
            else:
                # L·∫•y database config
                db_host = get_variable(
                    "POSTGRES_HOST", default=os.getenv("POSTGRES_HOST", "postgres")
                )
                db_port = int(
                    get_variable("POSTGRES_PORT", default=os.getenv("POSTGRES_PORT", "5432"))
                )
                db_name = get_variable(
                    "POSTGRES_DB", default=os.getenv("POSTGRES_DB", "crawl_data")
                )
                db_user = get_variable(
                    "POSTGRES_USER", default=os.getenv("POSTGRES_USER", "postgres")
                )
                # trufflehog:ignore - Fallback for development, production uses Airflow Variables
                db_password = get_variable(
                    "POSTGRES_PASSWORD", default=os.getenv("POSTGRES_PASSWORD", "postgres")
                )

                storage = PostgresStorage(
                    host=db_host,
                    port=db_port,
                    database=db_name,
                    user=db_user,
                    password=db_password,
                )

                # L·∫•y danh s√°ch product_ids t·ª´ products list
                product_ids_to_check = [
                    p.get("product_id") for p in products if p.get("product_id")
                ]

                if product_ids_to_check:
                    logger.info(
                        f"üîç ƒêang ki·ªÉm tra {len(product_ids_to_check)} products trong database..."
                    )
                    logger.info(
                        "   (ch·ªâ skip products c√≥ price, sales_count V√Ä brand - detail ƒë·∫ßy ƒë·ªß)"
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            # Chia nh·ªè query n·∫øu c√≥ qu√° nhi·ªÅu product_ids
                            # Ch·ªâ l·∫•y products c√≥ price, sales_count V√Ä brand (detail ƒë·∫ßy ƒë·ªß)
                            # Products kh√¥ng c√≥ brand s·∫Ω ƒë∆∞·ª£c crawl l·∫°i
                            for i in range(0, len(product_ids_to_check), 1000):
                                batch_ids = product_ids_to_check[i : i + 1000]
                                placeholders = ",".join(["%s"] * len(batch_ids))
                                cur.execute(
                                    f"""
                                    SELECT product_id
                                    FROM products
                                    WHERE product_id IN ({placeholders})
                                      AND price IS NOT NULL
                                      AND sales_count IS NOT NULL
                                      AND brand IS NOT NULL
                                      AND brand != ''
                                      AND seller_name IS NOT NULL
                                      AND seller_name != ''
                                    """,
                                    batch_ids,
                                )
                                existing_product_ids_in_db.update(row[0] for row in cur.fetchall())

                    logger.info(
                        f"‚úÖ T√¨m th·∫•y {len(existing_product_ids_in_db)} products ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß trong database"
                    )
                    logger.info("   (c√≥ price, sales_count V√Ä brand - s·∫Ω skip crawl l·∫°i)")
                    logger.info(
                        "   üí° Products kh√¥ng c√≥ brand s·∫Ω ƒë∆∞·ª£c crawl l·∫°i ƒë·ªÉ l·∫•y ƒë·∫ßy ƒë·ªß th√¥ng tin"
                    )
                    storage.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra database: {e}")
            logger.info("   S·∫Ω ti·∫øp t·ª•c v·ªõi cache v√† progress file")

        # B·∫Øt ƒë·∫ßu t·ª´ index ƒë√£ crawl
        start_index = progress["last_crawled_index"]

        # Ki·ªÉm tra n·∫øu start_index v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng products hi·ªán t·∫°i
        # (c√≥ th·ªÉ do test mode gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products)
        if start_index >= len(products):
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  RESET PROGRESS INDEX!")
            logger.warning(f"   - Progress index: {start_index}")
            logger.warning(f"   - S·ªë products hi·ªán t·∫°i: {len(products)}")
            logger.warning("   - Index v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng products")
            logger.warning("   - C√≥ th·ªÉ do test mode gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products")
            logger.warning("   - Reset v·ªÅ index 0 ƒë·ªÉ crawl l·∫°i t·ª´ ƒë·∫ßu")
            logger.warning("=" * 70)
            start_index = 0
            # Reset progress ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
            progress["last_crawled_index"] = 0
            progress["total_crawled"] = 0
            # Gi·ªØ l·∫°i crawled_product_ids ƒë·ªÉ tr√°nh crawl l·∫°i products ƒë√£ c√≥

        products_to_check = products[start_index:]

        logger.info(
            f"üîÑ B·∫Øt ƒë·∫ßu t·ª´ index {start_index} (ƒë√£ crawl {progress['total_crawled']} products, t·ªïng products: {len(products)})"
        )

        # T·ªëi ∆∞u: Duy·ªát t·∫•t c·∫£ products ƒë·ªÉ t√¨m products ch∆∞a c√≥ trong DB
        # Thay v√¨ d·ª´ng khi ƒë·∫°t max_products nh∆∞ng to√†n b·ªô l√† skip
        skipped_count = 0
        max_skipped_before_stop = 100  # D·ª´ng n·∫øu skip li√™n ti·∫øp 100 products

        for idx, product in enumerate(products_to_check):
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not product_id or not product_url:
                continue

            # Ki·ªÉm tra xem ƒë√£ crawl ch∆∞a (t·ª´ progress)
            if product_id in progress["crawled_product_ids"]:
                already_crawled += 1
                skipped_count += 1
                continue

            # Ki·ªÉm tra xem ƒë√£ c√≥ trong database ch∆∞a (v·ªõi detail ƒë·∫ßy ƒë·ªß)
            # existing_product_ids_in_db ch·ªâ ch·ª©a products c√≥ price v√† sales_count
            if product_id in existing_product_ids_in_db:
                # ƒê√£ c√≥ trong DB v·ªõi detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count)
                # ‚Üí Skip crawl l·∫°i
                db_hits += 1
                progress["crawled_product_ids"].add(product_id)
                already_crawled += 1
                skipped_count += 1
                continue

            # Ki·ªÉm tra cache v·ªõi Redis (thay v√¨ file cache)
            cache_hit = False

            if redis_cache:
                # Chu·∫©n h√≥a URL tr∆∞·ªõc khi check cache (CRITICAL)
                product_id_for_cache = product_id

                # Th·ª≠ l·∫•y t·ª´ Redis cache v·ªõi flexible validation
                cached_detail, is_valid = redis_cache.get_product_detail_with_validation(
                    product_id_for_cache
                )

                if is_valid:
                    cache_hits += 1
                    cache_hit = True
                    progress["crawled_product_ids"].add(product_id)
                    already_crawled += 1
                    skipped_count += 1

            # N·∫øu ch∆∞a c√≥ valid cache, th√™m v√†o danh s√°ch crawl
            if not cache_hit:
                products_to_crawl.append(
                    {
                        "product_id": product_id,
                        "url": product_url,
                        "name": product.get("name", ""),
                        "product": product,  # Gi·ªØ nguy√™n product data
                        "index": start_index + idx,  # L∆∞u index ƒë·ªÉ track progress
                    }
                )
                skipped_count = 0  # Reset counter khi t√¨m th·∫•y product m·ªõi

            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products crawl trong ng√†y n√†y
            if len(products_to_crawl) >= products_per_day:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {products_per_day} products cho ng√†y h√¥m nay")
                break

            # Gi·ªõi h·∫°n t·ªïng s·ªë (n·∫øu c√≥)
            if max_products > 0 and len(products_to_crawl) >= max_products:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n t·ªïng {max_products} products")
                break

            # D·ª´ng n·∫øu skip qu√° nhi·ªÅu products li√™n ti·∫øp (c√≥ th·ªÉ ƒë√£ h·∫øt products m·ªõi)
            if skipped_count >= max_skipped_before_stop:
                logger.info(
                    f"‚ö†Ô∏è  ƒê√£ skip {skipped_count} products li√™n ti·∫øp, c√≥ th·ªÉ ƒë√£ h·∫øt products m·ªõi"
                )
                logger.info(f"   - ƒê√£ t√¨m ƒë∆∞·ª£c {len(products_to_crawl)} products ƒë·ªÉ crawl")
                break

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä PREPARE PRODUCTS FOR DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products ƒë·∫ßu v√†o: {len(products)}")
        logger.info(f"‚úÖ Products c·∫ßn crawl h√¥m nay: {len(products_to_crawl)}")

        # Cache hit rate analytics
        total_checked = cache_hits + db_hits + (already_crawled - db_hits - cache_hits)
        if total_checked > 0:
            cache_hit_rate = (cache_hits / total_checked) * 100
        else:
            cache_hit_rate = 0.0

        logger.info(f"üî• Cache hits (Redis - c√≥ data h·ª£p l·ªá): {cache_hits}")
        logger.info(f"üíæ DB hits (ƒë√£ c√≥ trong DB): {db_hits}")
        logger.info(f"‚úì ƒê√£ crawl tr∆∞·ªõc ƒë√≥ (t·ª´ progress): {already_crawled - db_hits - cache_hits}")
        logger.info(f"üìà T·ªïng ƒë√£ ki·ªÉm tra: {total_checked}")
        logger.info(f"üìä **CACHE HIT RATE: {cache_hit_rate:.1f}%** ‚Üê TARGET: 60-80%")
        logger.info(f"üìà T·ªïng ƒë√£ crawl to√†n b·ªô: {progress['total_crawled'] + already_crawled}")
        logger.info(
            f"üìâ C√≤n l·∫°i ch∆∞a crawl: {len(products) - (progress['total_crawled'] + already_crawled + len(products_to_crawl))}"
        )
        logger.info("=" * 70)

        if len(products_to_crawl) == 0:
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  KH√îNG C√ì PRODUCTS N√ÄO C·∫¶N CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("üí° L√Ω do:")
            if already_crawled > 0:
                logger.warning(
                    f"   - ƒê√£ c√≥ trong progress: {already_crawled - db_hits - cache_hits} products"
                )
            if cache_hits > 0:
                logger.warning(
                    f"   - ƒê√£ c√≥ trong cache (c√≥ price v√† sales_count): {cache_hits} products"
                )
            if db_hits > 0:
                logger.warning(
                    f"   - ƒê√£ c√≥ trong database (c√≥ price v√† sales_count): {db_hits} products"
                )
            logger.warning("=" * 70)
            logger.warning("üí° ƒê·ªÉ force crawl l·∫°i, b·∫°n c√≥ th·ªÉ:")
            logger.warning("   1. X√≥a progress file: data/processed/detail_crawl_progress.json")
            logger.warning("   2. X√≥a cache files trong: data/raw/products/detail/cache/")
            logger.warning("   3. X√≥a products trong database (n·∫øu mu·ªën crawl l·∫°i)")
            logger.warning("=" * 70)

        # L∆∞u progress (s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau khi crawl xong)
        if products_to_crawl:
            # L∆∞u index c·ªßa product cu·ªëi c√πng s·∫Ω ƒë∆∞·ª£c crawl
            last_index = products_to_crawl[-1]["index"]
            progress["last_crawled_index"] = last_index + 1
            progress["last_updated"] = datetime.now().isoformat()

            # L∆∞u progress v√†o file
            try:
                with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "crawled_product_ids": list(progress["crawled_product_ids"]),
                            "last_crawled_index": progress["last_crawled_index"],
                            "total_crawled": progress["total_crawled"] + already_crawled,
                            "last_updated": progress["last_updated"],
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
                logger.info(f"üíæ ƒê√£ l∆∞u progress: index {progress['last_crawled_index']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng l∆∞u ƒë∆∞·ª£c progress: {e}")

        # Debug: Log m·ªôt v√†i products ƒë·∫ßu ti√™n
        if products_to_crawl:
            sample_names = [p.get("product_id", "N/A") for p in products_to_crawl[:3]]
            logger.info(f"üìã Sample products: {', '.join(sample_names)}...")
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o c·∫ßn crawl detail h√¥m nay!")
            logger.info("üí° T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl ho·∫∑c c√≥ cache h·ª£p l·ªá")

        logger.info(f"üî¢ Tr·∫£ v·ªÅ {len(products_to_crawl)} products cho Dynamic Task Mapping")

        return products_to_crawl

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi prepare products: {e}", exc_info=True)
        raise


def crawl_product_batch(
    product_batch: list[dict[str, Any]] = None, batch_index: int = -1, **context
) -> list[dict[str, Any]]:
    """
    Task: Crawl detail cho m·ªôt batch products (Batch Processing v·ªõi Driver Pooling v√† Async)

    T·ªëi ∆∞u:
    - Batch processing: 10 products/batch
    - Driver pooling: Reuse Selenium drivers trong batch
    - Async/aiohttp: Crawl parallel trong batch
    - Fallback Selenium: N·∫øu aiohttp thi·∫øu sales_count

    Args:
        product_batch: List products trong batch (t·ª´ expand_kwargs)
        batch_index: Index c·ªßa batch
        context: Airflow context

    Returns:
        List[Dict]: List k·∫øt qu·∫£ crawl cho batch
    """
    try:
        logger = get_logger(context)
    except Exception:
        import logging

        logger = logging.getLogger("airflow.task")

    # L·∫•y product_batch t·ª´ op_kwargs n·∫øu ch∆∞a c√≥
    if not product_batch:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_batch = op_kwargs.get("product_batch")
                batch_index = op_kwargs.get("batch_index", -1)

        if not product_batch:
            product_batch = context.get("product_batch") or context.get("op_kwargs", {}).get(
                "product_batch"
            )
            batch_index = context.get("batch_index", -1)

    if not product_batch:
        logger.error("=" * 70)
        logger.error("‚ùå KH√îNG T√åM TH·∫§Y PRODUCT_BATCH TRONG CONTEXT!")
        logger.error("=" * 70)
        logger.error("üí° Debug info:")
        logger.error(f"   - Context keys: {list(context.keys())}")
        if ti:
            logger.error(f"   - ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        logger.error("=" * 70)
        return []

    # Validate product_batch
    if not isinstance(product_batch, list):
        logger.error("=" * 70)
        logger.error(f"‚ùå PRODUCT_BATCH KH√îNG PH·∫¢I LIST: {type(product_batch)}")
        logger.error(f"   - Value: {product_batch}")
        logger.error("=" * 70)
        return []

    if len(product_batch) == 0:
        logger.warning("=" * 70)
        logger.warning(f"‚ö†Ô∏è  BATCH {batch_index} R·ªñNG - Kh√¥ng c√≥ products n√†o")
        logger.warning("=" * 70)
        return []

    logger.info("=" * 70)
    logger.info(f"üì¶ BATCH {batch_index}: Crawl {len(product_batch)} products")
    logger.info(f"   - Product IDs: {[p.get('product_id', 'unknown') for p in product_batch[:5]]}")
    if len(product_batch) > 5:
        logger.info(f"   - ... v√† {len(product_batch) - 5} products n·ªØa")
    logger.info("=" * 70)

    results = []

    try:
        import asyncio

        # Import SeleniumDriverPool t·ª´ utils n·∫øu ch∆∞a c√≥ (cho task scope)
        global SeleniumDriverPool
        _SeleniumDriverPool = SeleniumDriverPool
        if _SeleniumDriverPool is None:
            # Fallback: th·ª≠ import t·ª´ utils tr·ª±c ti·∫øp n·∫øu kh√¥ng th√†nh c√¥ng
            try:
                _fix_sys_path_for_pipelines_import(logger)
                # utils l√† file (.py), kh√¥ng ph·∫£i package
                import importlib.util

                src_path = Path("/opt/airflow/src")
                utils_path = src_path / "pipelines" / "crawl" / "utils.py"
                if utils_path.exists():
                    spec = importlib.util.spec_from_file_location(
                        "crawl_utils_fallback", str(utils_path)
                    )
                    if spec and spec.loader:
                        crawl_utils = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(crawl_utils)
                        _SeleniumDriverPool = getattr(crawl_utils, "SeleniumDriverPool", None)
                if _SeleniumDriverPool:
                    logger.info("‚úÖ Imported SeleniumDriverPool from utils.py file")
                else:
                    raise ImportError("Kh√¥ng t√¨m th·∫•y SeleniumDriverPool trong utils.py")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import SeleniumDriverPool t·ª´ pipelines: {e}")
                raise ImportError("SeleniumDriverPool ch∆∞a ƒë∆∞·ª£c import t·ª´ utils module") from e

        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        # crawl_product_detail_async v√† SeleniumDriverPool ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
        pool_size = int(
            get_variable("TIKI_DETAIL_POOL_SIZE", default="2")
        )  # T·ªëi ∆∞u: tƒÉng t·ª´ 5 -> 15
        driver_pool = _SeleniumDriverPool(
            pool_size=pool_size, headless=True, timeout=120
        )  # T·ªëi ∆∞u: tƒÉng t·ª´ 60 -> 120s ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß

        # T·∫°o event loop tr∆∞·ªõc
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Session s·∫Ω ƒë∆∞·ª£c t·∫°o b√™n trong async function (c·∫ßn async context)
        session = None

        async def crawl_single_async(product_info: dict) -> dict[str, Any]:
            """Crawl m·ªôt product v·ªõi async"""
            product_id = product_info.get("product_id", "unknown")
            product_url = product_info.get("url", "")

            result = {
                "product_id": product_id,
                "url": product_url,
                "status": "failed",
                "error": None,
                "detail": None,
                "crawled_at": datetime.now().isoformat(),
            }

            try:
                # Th·ª≠ async crawl tr∆∞·ªõc
                if session:
                    detail = await crawl_product_detail_async(
                        product_url, session=session, use_selenium_fallback=True, verbose=False
                    )

                    # Ki·ªÉm tra n·∫øu crawl_product_detail_async tr·∫£ v·ªÅ HTML string (do fallback v·ªÅ Selenium)
                    if isinstance(detail, str) and detail.strip().startswith("<"):
                        # Ph√¢n t√≠ch HTML ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i
                        html_preview = detail[:500] if len(detail) > 500 else detail
                        html_lower = detail.lower()

                        # Ki·ªÉm tra c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
                        # Ki·ªÉm tra error page - c·∫ßn ki·ªÉm tra k·ªπ h∆°n ƒë·ªÉ tr√°nh false positive
                        # Error page th∆∞·ªùng c√≥ title ho·∫∑c heading ch·ª©a "404", "not found", etc.
                        is_error_page = False
                        error_keywords = [
                            "404",
                            "not found",
                            "page not found",
                            "500",
                            "internal server error",
                            "403",
                            "forbidden",
                            "access denied",
                        ]
                        # Ch·ªâ coi l√† error page n·∫øu c√≥ keyword trong title ho·∫∑c heading, kh√¥ng ph·∫£i trong to√†n b·ªô HTML
                        # V√¨ m·ªôt s·ªë product c√≥ th·ªÉ c√≥ "404" trong t√™n ho·∫∑c m√¥ t·∫£
                        if any(keyword in html_lower for keyword in error_keywords):
                            # Ki·ªÉm tra trong title tag ho·∫∑c h1 tag (n∆°i th∆∞·ªùng c√≥ error message)
                            title_match = re.search(
                                r"<title[^>]*>(.*?)</title>", html_lower, re.IGNORECASE | re.DOTALL
                            )
                            h1_match = re.search(
                                r"<h1[^>]*>(.*?)</h1>", html_lower, re.IGNORECASE | re.DOTALL
                            )

                            title_text = title_match.group(1) if title_match else ""
                            h1_text = h1_match.group(1) if h1_match else ""

                            # Ch·ªâ coi l√† error n·∫øu keyword xu·∫•t hi·ªán trong title ho·∫∑c h1
                            is_error_page = any(
                                keyword in title_text or keyword in h1_text
                                for keyword in error_keywords
                            )

                        is_captcha = any(
                            keyword in html_lower
                            for keyword in [
                                "captcha",
                                "recaptcha",
                                "cloudflare",
                                "checking your browser",
                            ]
                        )
                        has_next_data = (
                            "__next_data__" in html_lower or 'id="__NEXT_DATA__"' in html_lower
                        )

                        # Ki·ªÉm tra xem c√≥ ph·∫£i l√† HTML b√¨nh th∆∞·ªùng c·ªßa Tiki kh√¥ng
                        is_tiki_page = any(
                            indicator in html_lower
                            for indicator in [
                                "tiki.vn",
                                "tiki",
                                "pdp_product_name",
                                "product-detail",
                                "data-view-id",
                                "pdp-product",
                            ]
                        )

                        if is_error_page:
                            logger.warning(
                                f"‚ö†Ô∏è  HTML l√† error page cho product {product_id}: {html_preview[:200]}..."
                            )
                            detail = None
                        elif is_captcha:
                            logger.warning(
                                f"‚ö†Ô∏è  HTML l√† captcha/block page cho product {product_id}"
                            )
                            detail = None
                        elif not is_tiki_page and not has_next_data:
                            # N·∫øu kh√¥ng ph·∫£i Tiki page v√† kh√¥ng c√≥ __NEXT_DATA__, c√≥ th·ªÉ l√† page l·∫°
                            logger.warning(
                                f"‚ö†Ô∏è  HTML kh√¥ng gi·ªëng Tiki product page cho product {product_id}"
                            )
                            logger.warning(f"   - C√≥ __NEXT_DATA__: {has_next_data}")
                            logger.warning(f"   - HTML preview: {html_preview[:300]}...")
                            # V·∫´n th·ª≠ parse, c√≥ th·ªÉ v·∫´n extract ƒë∆∞·ª£c m·ªôt s·ªë th√¥ng tin
                        else:
                            logger.info(
                                f"‚ÑπÔ∏è  crawl_product_detail_async tr·∫£ v·ªÅ HTML (fallback Selenium) cho product {product_id}"
                            )
                            logger.info(f"   - HTML length: {len(detail)} chars")
                            logger.info(f"   - C√≥ __NEXT_DATA__: {has_next_data}")

                            # Parse HTML th√†nh dict
                            try:
                                hierarchy_map = get_hierarchy_map()
                                detail = extract_product_detail(
                                    detail, product_url, verbose=False, hierarchy_map=hierarchy_map
                                )
                                if detail and isinstance(detail, dict):
                                    # Ki·ªÉm tra xem c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin kh√¥ng
                                    has_name = bool(detail.get("name"))
                                    has_price = bool(detail.get("price", {}).get("current_price"))
                                    has_sales = detail.get("sales_count") is not None
                                    logger.info(
                                        f"‚úÖ ƒê√£ parse HTML th√†nh c√¥ng cho product {product_id}"
                                    )
                                    logger.info(
                                        f"   - C√≥ name: {has_name}, c√≥ price: {has_price}, c√≥ sales_count: {has_sales}"
                                    )
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ None ho·∫∑c kh√¥ng ph·∫£i dict cho product {product_id}"
                                    )
                                    detail = None
                            except Exception as parse_error:
                                logger.warning(
                                    f"‚ö†Ô∏è  L·ªói khi parse HTML t·ª´ crawl_product_detail_async: {parse_error}"
                                )
                                logger.debug(f"   HTML preview: {html_preview}")
                                detail = None

                    # ƒê·∫£m b·∫£o detail l√† dict
                    if detail and not isinstance(detail, dict):
                        logger.warning(
                            f"‚ö†Ô∏è  crawl_product_detail_async tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}"
                        )
                        detail = None
                else:
                    # Fallback v·ªÅ Selenium n·∫øu kh√¥ng c√≥ aiohttp
                    # ∆Øu ti√™n d√πng driver pool n·∫øu c√≥
                    html = None
                    try:
                        if "crawl_product_detail_with_driver" in globals() and callable(
                            crawl_product_detail_with_driver
                        ):
                            drv = driver_pool.get_driver()
                            if drv is not None:
                                try:
                                    html = crawl_product_detail_with_driver(
                                        drv,
                                        product_url,
                                        save_html=False,
                                        verbose=False,
                                        timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß
                                        use_redis_cache=True,
                                        use_rate_limiting=True,
                                    )
                                finally:
                                    driver_pool.return_driver(drv)
                    except Exception as pooled_err:
                        logger.warning(f"‚ö†Ô∏è  L·ªói khi d√πng pooled driver: {pooled_err}")
                        html = None

                    # Fallback cu·ªëi c√πng: t·∫°o driver ri√™ng qua h√†m s·∫µn c√≥
                    if html is None:
                        html = crawl_product_detail_with_selenium(
                            product_url,
                            verbose=False,
                            max_retries=2,
                            timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ trang load ƒë·∫ßy ƒë·ªß
                            use_redis_cache=True,
                            use_rate_limiting=True,
                        )
                    if html:
                        # S·ª≠ d·ª•ng h√†m ƒë√£ ƒë∆∞·ª£c import ·ªü ƒë·∫ßu file
                        hierarchy_map = get_hierarchy_map()
                        detail = extract_product_detail(
                            html, product_url, verbose=False, hierarchy_map=hierarchy_map
                        )

                        # Ki·ªÉm tra n·∫øu extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict
                        if isinstance(detail, str) and detail.strip().startswith("<"):
                            logger.warning(
                                f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ HTML thay v√¨ dict cho product {product_id}, th·ª≠ parse l·∫°i"
                            )
                            # Th·ª≠ parse l·∫°i HTML
                            try:
                                detail = extract_product_detail(
                                    html, product_url, verbose=False, hierarchy_map=hierarchy_map
                                )
                            except Exception as parse_error:
                                logger.warning(f"‚ö†Ô∏è  L·ªói khi parse l·∫°i HTML: {parse_error}")
                                detail = None

                        # ƒê·∫£m b·∫£o detail l√† dict, kh√¥ng ph·∫£i HTML string
                        if not isinstance(detail, dict):
                            logger.warning(
                                f"‚ö†Ô∏è  extract_product_detail tr·∫£ v·ªÅ {type(detail)} thay v√¨ dict cho product {product_id}"
                            )
                            detail = None
                    else:
                        detail = None

                if detail and isinstance(detail, dict):
                    result["detail"] = detail
                    result["status"] = "success"
                else:
                    result["error"] = "Kh√¥ng th·ªÉ crawl detail ho·∫∑c extract detail kh√¥ng h·ª£p l·ªá"
                    result["status"] = "failed"

            except Exception as e:
                result["error"] = str(e)
                result["status"] = "failed"
                logger.warning(f"‚ö†Ô∏è  L·ªói khi crawl product {product_id}: {e}")

            return result

        # Crawl t·∫•t c·∫£ products trong batch song song v·ªõi async
        # (Event loop ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n)
        # S·ª≠ d·ª•ng asyncio.gather() ƒë·ªÉ crawl parallel
        rate_limit_delay = float(get_variable("TIKI_DETAIL_RATE_LIMIT_DELAY", default="0.1"))

        # T·∫°o semaphore ƒë·ªÉ limit concurrent tasks (t·ªëi ∆∞u throughput)
        max_concurrent = int(get_variable("TIKI_DETAIL_MAX_CONCURRENT_TASKS", default="12"))
        semaphore = asyncio.Semaphore(max_concurrent)

        async def bounded_task(task_coro):
            """Wrap task ƒë·ªÉ respect semaphore limit"""
            async with semaphore:
                return await task_coro

        # T·∫°o tasks v·ªõi rate limiting: stagger start times
        async def crawl_batch_parallel():
            """Crawl batch v·ªõi parallel processing v√† rate limiting"""
            # T·∫°o session ngay l·∫≠p t·ª©c trong async context (tr∆∞·ªõc khi t·∫°o tasks)
            # ƒê·∫£m b·∫£o session ƒë∆∞·ª£c t·∫°o trong async context c√≥ event loop
            nonlocal session
            if session is None:
                try:
                    import aiohttp

                    timeout = aiohttp.ClientTimeout(total=30)
                    # T·∫°o connector v·ªõi optimized pooling (s·ª≠ d·ª•ng config)
                    # ƒê·∫£m b·∫£o sys.path ƒë∆∞·ª£c c·∫•u h√¨nh tr∆∞·ªõc khi import
                    src_path = Path("/opt/airflow/src")
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))

                    from pipelines.crawl.config import (
                        HTTP_CONNECTOR_LIMIT,
                        HTTP_CONNECTOR_LIMIT_PER_HOST,
                        HTTP_DNS_CACHE_TTL,
                    )

                    connector = aiohttp.TCPConnector(
                        limit=HTTP_CONNECTOR_LIMIT,  # S·ª≠ d·ª•ng config (150)
                        limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,  # S·ª≠ d·ª•ng config (15)
                        ttl_dns_cache=HTTP_DNS_CACHE_TTL,  # S·ª≠ d·ª•ng config (1800s = 30 min)
                        force_close=False,  # Keep connections alive for reuse
                        enable_cleanup_closed=True,
                    )
                    # T·∫°o session trong async context (c√≥ event loop ƒëang ch·∫°y)
                    # ƒê√¢y l√† async function n√™n event loop ƒë√£ c√≥ s·∫µn
                    session = aiohttp.ClientSession(
                        timeout=timeout,
                        connector=connector,
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        },
                    )
                    logger.info("‚úÖ ƒê√£ t·∫°o aiohttp session trong async context")
                except RuntimeError as e:
                    # L·ªói "no running event loop" - fallback v·ªÅ Selenium
                    logger.warning(
                        f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session (no event loop): {e}, s·∫Ω d√πng Selenium"
                    )
                    session = None
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o aiohttp session: {e}, s·∫Ω d√πng Selenium")
                    session = None

            # Factory function ƒë·ªÉ tr√°nh closure issue
            def create_crawl_task(product_info, delay_value):
                async def crawl_with_delay():
                    if delay_value > 0:
                        await asyncio.sleep(delay_value)
                    return await crawl_single_async(product_info)

                return crawl_with_delay()

            tasks = []
            for i, product in enumerate(product_batch):
                delay = i * rate_limit_delay / len(product_batch)  # Ph√¢n t√°n delay
                task = create_crawl_task(product, delay)
                # Wrap v·ªõi bounded_task ƒë·ªÉ respect semaphore
                bounded = bounded_task(task)
                tasks.append(bounded)

            # Ch·∫°y t·∫•t c·∫£ tasks song song (limited b·ªüi semaphore)
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # X·ª≠ l√Ω exceptions
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    product_info = product_batch[i]
                    processed_results.append(
                        {
                            "product_id": product_info.get("product_id", "unknown"),
                            "url": product_info.get("url", ""),
                            "status": "failed",
                            "error": str(result),
                            "detail": None,
                            "crawled_at": datetime.now().isoformat(),
                        }
                    )
                else:
                    processed_results.append(result)

            return processed_results

        results = loop.run_until_complete(crawl_batch_parallel())

        # ƒê√≥ng session
        if session:
            loop.run_until_complete(session.close())

        # Cleanup driver pool
        driver_pool.cleanup()

        # Th·ªëng k√™
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = len(results) - success_count

        logger.info(f"‚úÖ Batch {batch_index} ho√†n th√†nh:")
        logger.info(f"   - Success: {success_count}/{len(product_batch)}")
        logger.info(f"   - Failed: {failed_count}/{len(product_batch)}")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi crawl batch {batch_index}: {e}", exc_info=True)
        # Tr·∫£ v·ªÅ results v·ªõi status failed cho t·∫•t c·∫£
        if product_batch and isinstance(product_batch, list):
            for product_info in product_batch:
                results.append(
                    {
                        "product_id": product_info.get("product_id", "unknown"),
                        "url": product_info.get("url", ""),
                        "status": "failed",
                        "error": f"Batch error: {str(e)}",
                        "detail": None,
                        "crawled_at": datetime.now().isoformat(),
                    }
                )
        else:
            logger.error("‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫°o failed results v√¨ product_batch kh√¥ng h·ª£p l·ªá")

    return results


def crawl_single_product_detail(product_info: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task: Crawl detail cho m·ªôt product (Dynamic Task Mapping)

    T·ªëi ∆∞u:
    - S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Rate limiting
    - Error handling: ti·∫øp t·ª•c v·ªõi product kh√°c khi l·ªói
    - Atomic write cache

    Args:
        product_info: Th√¥ng tin product (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi detail v√† metadata
    """
    # Kh·ªüi t·∫°o result m·∫∑c ƒë·ªãnh
    default_result = {
        "product_id": "unknown",
        "url": "",
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    try:
        logger = get_logger(context)
    except Exception as e:
        # N·∫øu kh√¥ng th·ªÉ t·∫°o logger, v·∫´n ti·∫øp t·ª•c v·ªõi default result
        import logging

        logger = logging.getLogger("airflow.task")
        logger.error(f"Kh√¥ng th·ªÉ t·∫°o logger t·ª´ context: {e}")

    # L·∫•y product_info t·ª´ keyword argument ho·∫∑c context
    if not product_info:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_info = op_kwargs.get("product_info")

        if not product_info:
            product_info = context.get("product_info") or context.get("op_kwargs", {}).get(
                "product_info"
            )

    if not product_info:
        logger.error(f"Kh√¥ng t√¨m th·∫•y product_info. Context keys: {list(context.keys())}")
        # Return result v·ªõi status failed thay v√¨ raise exception
        return {
            "product_id": "unknown",
            "url": "",
            "status": "failed",
            "error": "Kh√¥ng t√¨m th·∫•y product_info trong context",
            "detail": None,
            "crawled_at": datetime.now().isoformat(),
        }

    product_id = product_info.get("product_id", "")
    product_url = product_info.get("url", "")
    product_name = product_info.get("name", "Unknown")

    logger.info("=" * 70)
    logger.info(f"üîç TASK: Crawl Product Detail - {product_name}")
    logger.info(f"üÜî Product ID: {product_id}")
    logger.info(f"üîó URL: {product_url}")
    logger.info("=" * 70)

    result = {
        "product_id": product_id,
        "url": product_url,
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    # Ki·ªÉm tra cache tr∆∞·ªõc - ∆∞u ti√™n Redis, fallback v·ªÅ file
    # Ki·ªÉm tra xem c√≥ force refresh kh√¥ng (t·ª´ Airflow Variable)
    force_refresh = get_variable("TIKI_FORCE_REFRESH_CACHE", default="false").lower() == "true"

    if force_refresh:
        logger.info(f"üîÑ FORCE REFRESH MODE: B·ªè qua cache cho product {product_id}")
    else:
        # Th·ª≠ Redis cache tr∆∞·ªõc (nhanh h∆°n, distributed)
        logger.info(f"üîç ƒêang ki·ªÉm tra cache cho product {product_id}...")
        redis_cache = None
        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache

            redis_cache = get_redis_cache("redis://redis:6379/1")
            if redis_cache:
                cached_detail = redis_cache.get_cached_product_detail(product_id)
                if cached_detail:
                    has_price = cached_detail.get("price", {}).get("current_price")
                    has_sales_count = cached_detail.get("sales_count") is not None
                    brand_value = cached_detail.get("brand")
                    has_brand = bool(
                        brand_value and (not isinstance(brand_value, str) or brand_value.strip())
                    )
                    seller_info = cached_detail.get("seller", {})
                    seller_name = (
                        seller_info.get("name") if isinstance(seller_info, dict) else None
                    )
                    has_seller = bool(
                        seller_name and (not isinstance(seller_name, str) or seller_name.strip())
                    )

                    if has_price and has_sales_count and has_brand and has_seller:
                        logger.info("=" * 70)
                        logger.info(f"‚úÖ SKIP CRAWL - Redis Cache Hit cho product {product_id}")
                        logger.info(f"   - C√≥ price: {has_price}")
                        logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                        logger.info(f"   - C√≥ brand: {has_brand}")
                        logger.info(f"   - C√≥ seller: {has_seller}")
                        logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                        logger.info("=" * 70)
                        result["detail"] = cached_detail
                        result["status"] = "cached"
                        return result
                    if has_price or has_sales_count or has_brand or has_seller:
                        logger.info(
                            f"[Redis Cache] ‚ö†Ô∏è  Cache thi·∫øu d·ªØ li·ªáu cho product {product_id}, s·∫Ω crawl l·∫°i"
                        )
                else:
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache kh√¥ng ƒë·∫ßy ƒë·ªß cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
        except Exception:
            # Redis kh√¥ng available, fallback v·ªÅ file cache
            pass

        # Fallback: Ki·ªÉm tra file cache n·∫øu Redis kh√¥ng available ho·∫∑c kh√¥ng c√≥ cache
        if not force_refresh:
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None
                        brand_value = cached_detail.get("brand")
                        has_brand = bool(
                            brand_value
                            and (not isinstance(brand_value, str) or brand_value.strip())
                        )
                        seller_info = cached_detail.get("seller", {})
                        seller_name = (
                            seller_info.get("name") if isinstance(seller_info, dict) else None
                        )
                        has_seller = bool(
                            seller_name
                            and (not isinstance(seller_name, str) or seller_name.strip())
                        )

                        if has_price and has_sales_count and has_brand and has_seller:
                            logger.info("=" * 70)
                            logger.info(f"‚úÖ SKIP CRAWL - File Cache Hit cho product {product_id}")
                            logger.info(f"   - C√≥ price: {has_price}")
                            logger.info(f"   - C√≥ sales_count: {has_sales_count}")
                            logger.info(f"   - C√≥ brand: {has_brand}")
                            logger.info(f"   - C√≥ seller: {has_seller}")
                            logger.info("   - S·ª≠ d·ª•ng cache, kh√¥ng c·∫ßn crawl l·∫°i")
                            logger.info("=" * 70)
                            result["detail"] = cached_detail
                            result["status"] = "cached"
                            return result
                except Exception:
                    # File cache l·ªói, ti·∫øp t·ª•c crawl
                    pass

    # Ti·∫øp t·ª•c crawl n·∫øu kh√¥ng c√≥ cache ho·∫∑c force refresh
    # (File cache check ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü tr√™n trong else block)

    # B·∫Øt ƒë·∫ßu crawl product detail
    try:
        # Ki·ªÉm tra graceful degradation
        if tiki_degradation.should_skip():
            logger.warning("=" * 70)
            logger.warning(f"‚ö†Ô∏è  SKIP CRAWL - Service Degraded cho product {product_id}")
            logger.warning("   - Service ƒëang ·ªü tr·∫°ng th√°i FAILED")
            logger.warning("   - Graceful degradation: skip crawl ƒë·ªÉ tr√°nh l√†m t·ªá h∆°n")
            logger.warning("=" * 70)
            result["error"] = "Service ƒëang ·ªü tr·∫°ng th√°i FAILED, skip crawl"
            result["status"] = "degraded"
            return result

        # Validate URL
        if not product_url or not product_url.startswith("http"):
            raise ValueError(f"URL kh√¥ng h·ª£p l·ªá: {product_url}")

        # L·∫•y c·∫•u h√¨nh
        rate_limit_delay = float(
            get_variable("TIKI_DETAIL_RATE_LIMIT_DELAY", default="0.1")
        )  # Delay 1.5s cho detail (t·ªëi ∆∞u t·ª´ 2.0s)
        timeout = int(
            get_variable("TIKI_DETAIL_CRAWL_TIMEOUT", default="180")
        )  # 3 ph√∫t m·ªói product (tƒÉng t·ª´ 120s ƒë·ªÉ tr√°nh timeout)

        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout v√† circuit breaker
        start_time = time.time()

        # S·ª≠ d·ª•ng Selenium ƒë·ªÉ crawl detail (c·∫ßn thi·∫øt cho dynamic content)
        html_content = None
        try:
            # Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker
            def _crawl_detail_with_params():
                """Wrapper function ƒë·ªÉ g·ªçi v·ªõi circuit breaker"""
                return crawl_product_detail_with_selenium(
                    product_url,
                    save_html=False,
                    verbose=False,  # Kh√¥ng verbose trong Airflow
                    max_retries=3,  # Retry 3 l·∫ßn (tƒÉng t·ª´ 2)
                    timeout=120,  # TƒÉng t·ª´ 60 -> 120s (2 ph√∫t) ƒë·ªÉ ƒë·ªß th·ªùi gian cho trang load ƒë·∫ßy ƒë·ªß
                    use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                    use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
                )

            try:
                # G·ªçi v·ªõi circuit breaker
                html_content = tiki_circuit_breaker.call(_crawl_detail_with_params)
                tiki_degradation.record_success()
            except CircuitBreakerOpenError as e:
                # Circuit breaker ƒëang m·ªü
                result["error"] = f"Circuit breaker open: {str(e)}"
                result["status"] = "circuit_breaker_open"
                logger.warning(f"‚ö†Ô∏è  Circuit breaker open cho product {product_id}: {e}")
                # Th√™m v√†o DLQ
                try:
                    crawl_error = classify_error(
                        e, context={"product_url": product_url, "product_id": product_id}
                    )
                    tiki_dlq.add(
                        task_id=f"crawl_detail_{product_id}",
                        task_type="crawl_product_detail",
                        error=crawl_error,
                        context={
                            "product_url": product_url,
                            "product_name": product_name,
                            "product_id": product_id,
                        },
                        retry_count=0,
                    )
                    logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
                except Exception as dlq_error:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
                return result
            except Exception:
                # Ghi nh·∫≠n failure
                tiki_degradation.record_failure()
                raise  # Re-raise ƒë·ªÉ x·ª≠ l√Ω b√™n d∆∞·ªõi

            if not html_content or len(html_content) < 100:
                raise ValueError(
                    f"HTML content qu√° ng·∫Øn ho·∫∑c r·ªóng: {len(html_content) if html_content else 0} k√Ω t·ª±"
                )

        except Exception as selenium_error:
            # Log l·ªói Selenium chi ti·∫øt
            error_type = type(selenium_error).__name__
            error_msg = str(selenium_error)

            # R√∫t g·ªçn error message n·∫øu qu√° d√†i
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."

            logger.error(f"‚ùå L·ªói Selenium ({error_type}): {error_msg}")

            # Ki·ªÉm tra c√°c l·ªói ph·ªï bi·∫øn v√† ph√¢n lo·∫°i
            error_msg_lower = error_msg.lower()
            if (
                "chrome" in error_msg_lower
                or "driver" in error_msg_lower
                or "webdriver" in error_msg_lower
            ):
                result["error"] = f"Chrome/Driver error: {error_msg}"
                result["status"] = "selenium_error"
            elif (
                "timeout" in error_msg_lower
                or "timed out" in error_msg_lower
                or "time-out" in error_msg_lower
            ):
                result["error"] = f"Timeout: {error_msg}"
                result["status"] = "timeout"
            elif (
                "connection" in error_msg_lower
                or "network" in error_msg_lower
                or "refused" in error_msg_lower
            ):
                result["error"] = f"Network error: {error_msg}"
                result["status"] = "network_error"
            elif "memory" in error_msg_lower or "out of memory" in error_msg_lower:
                result["error"] = f"Memory error: {error_msg}"
                result["status"] = "memory_error"
            else:
                result["error"] = f"Selenium error: {error_msg}"
                result["status"] = "failed"

            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    selenium_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            # Kh√¥ng raise, return result v·ªõi status failed
            return result

        # Extract detail
        try:
            hierarchy_map = get_hierarchy_map()
            detail = extract_product_detail(
                html_content, product_url, verbose=False, hierarchy_map=hierarchy_map
            )

            if not detail:
                raise ValueError("Kh√¥ng extract ƒë∆∞·ª£c detail t·ª´ HTML")

        except Exception as extract_error:
            error_type = type(extract_error).__name__
            error_msg = str(extract_error)
            logger.error(f"‚ùå L·ªói khi extract detail ({error_type}): {error_msg}")
            result["error"] = f"Extract error: {error_msg}"
            result["status"] = "extract_error"
            # Ghi nh·∫≠n failure v√† th√™m v√†o DLQ
            tiki_degradation.record_failure()
            try:
                crawl_error = classify_error(
                    extract_error, context={"product_url": product_url, "product_id": product_id}
                )
                tiki_dlq.add(
                    task_id=f"crawl_detail_{product_id}",
                    task_type="crawl_product_detail",
                    error=crawl_error,
                    context={
                        "product_url": product_url,
                        "product_name": product_name,
                        "product_id": product_id,
                    },
                    retry_count=0,
                )
                logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
            except Exception as dlq_error:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
            return result

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(
                f"Crawl detail v∆∞·ª£t qu√° timeout {timeout}s (elapsed: {elapsed:.1f}s)"
            )

        result["detail"] = detail
        result["status"] = "success"
        result["elapsed_time"] = elapsed

        # L∆∞u v√†o cache - ∆∞u ti√™n Redis, fallback v·ªÅ file
        # Redis cache (nhanh, distributed) - CRITICAL: Chu·∫©n h√≥a URL tr∆∞·ªõc khi cache
        if redis_cache:
            try:
                # IMPORTANT: S·ª≠ d·ª•ng product_id (kh√¥ng ph·ª• thu·ªôc v√†o URL) ƒë·ªÉ cache
                # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o r·∫±ng c√πng 1 product t·ª´ category kh√°c nhau s·∫Ω hit cache
                redis_cache.cache_product_detail(product_id, detail, ttl=604800)  # 7 ng√†y
                logger.info(
                    f"[Redis Cache] ‚úÖ ƒê√£ cache detail cho product {product_id} (TTL: 7 days)"
                )
            except Exception as e:
                logger.warning(f"[Redis Cache] ‚ö†Ô∏è  L·ªói khi cache v√†o Redis: {e}")

        # File cache (fallback)
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

            temp_file = cache_file.with_suffix(".tmp")
            logger.debug(f"üíæ ƒêang l∆∞u cache v√†o: {cache_file}")

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)

            # Atomic move
            if os.name == "nt":  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))

            # Verify cache file was created
            if cache_file.exists():
                logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {elapsed:.1f}s, ƒë√£ cache v√†o {cache_file}")
                # Log sales_count n·∫øu c√≥
                if detail.get("sales_count") is not None:
                    logger.info(f"   üìä sales_count: {detail.get('sales_count')}")
                else:
                    logger.warning("   ‚ö†Ô∏è  sales_count: None (kh√¥ng t√¨m th·∫•y)")
            else:
                logger.error(f"‚ùå Cache file kh√¥ng ƒë∆∞·ª£c t·∫°o: {cache_file}")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng l∆∞u ƒë∆∞·ª£c cache: {e}", exc_info=True)
            # Kh√¥ng fail task v√¨ ƒë√£ crawl th√†nh c√¥ng, ch·ªâ kh√¥ng l∆∞u ƒë∆∞·ª£c cache

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        tiki_degradation.record_failure()
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except ValueError as e:
        result["error"] = str(e)
        result["status"] = "validation_error"
        tiki_degradation.record_failure()
        logger.error(f"‚ùå Validation error: {e}")
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        tiki_degradation.record_failure()
        error_type = type(e).__name__
        logger.error(f"‚ùå L·ªói khi crawl detail ({error_type}): {e}", exc_info=True)
        # Th√™m v√†o DLQ
        try:
            crawl_error = classify_error(
                e, context={"product_url": product_url, "product_id": product_id}
            )
            tiki_dlq.add(
                task_id=f"crawl_detail_{product_id}",
                task_type="crawl_product_detail",
                error=crawl_error,
                context={
                    "product_url": product_url,
                    "product_name": product_name,
                    "product_id": product_id,
                },
                retry_count=0,
            )
            logger.info(f"üì¨ ƒê√£ th√™m v√†o DLQ: crawl_detail_{product_id}")
        except Exception as dlq_error:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ th√™m v√†o DLQ: {dlq_error}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi product kh√°c

    # ƒê·∫£m b·∫£o lu√¥n return result, kh√¥ng bao gi·ªù raise exception
    # Ki·ªÉm tra result c√≥ h·ª£p l·ªá kh√¥ng tr∆∞·ªõc khi return
    if not result or not isinstance(result, dict):
        logger.warning("‚ö†Ô∏è  Result kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng default_result")
        result = default_result.copy()
        result["error"] = "Result kh√¥ng h·ª£p l·ªá"
        result["status"] = "failed"

    # ƒê·∫£m b·∫£o result c√≥ ƒë·∫ßy ƒë·ªß c√°c field c·∫ßn thi·∫øt
    if "product_id" not in result:
        result["product_id"] = product_id if "product_id" in locals() else "unknown"
    if "url" not in result:
        result["url"] = product_url if "product_url" in locals() else ""
    if "status" not in result:
        result["status"] = "failed"
    if "crawled_at" not in result:
        result["crawled_at"] = datetime.now().isoformat()

    try:
        return result
    except Exception as e:
        # N·∫øu c√≥ l·ªói khi return (kh√¥ng th·ªÉ x·∫£y ra nh∆∞ng ƒë·ªÉ an to√†n)
        logger.error(f"‚ùå L·ªói khi return result: {e}", exc_info=True)
        default_result["error"] = f"L·ªói khi return result: {str(e)}"
        default_result["product_id"] = product_id if "product_id" in locals() else "unknown"
        default_result["url"] = product_url if "product_url" in locals() else ""
        return default_result


def merge_product_details(**context) -> dict[str, Any]:
    """
    Task: Merge product details v√†o products list

    Returns:
        Dict: Products v·ªõi detail ƒë√£ merge
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Product Details")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products g·ªëc
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file
            if OUTPUT_FILE.exists():
                import json as json_module  # noqa: F401

                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json_module.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")

        # L·∫•y s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl t·ª´ prepare_products_for_detail
        # ƒê√¢y l√† s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø, kh√¥ng ph·∫£i t·ªïng s·ªë products
        products_to_crawl = None
        try:
            products_to_crawl = ti.xcom_pull(
                task_ids="crawl_product_details.prepare_products_for_detail"
            )
        except Exception:
            try:
                products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
            except Exception:
                pass

        # S·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl
        expected_products_count = len(products_to_crawl) if products_to_crawl else 0
        # V·ªõi batch processing, s·ªë map_index = s·ªë batches, kh√¥ng ph·∫£i s·ªë products
        # L·∫•y batch size t·ª´ config
        try:
            from pipelines.crawl.config import PRODUCT_BATCH_SIZE

            batch_size = PRODUCT_BATCH_SIZE
        except Exception:
            batch_size = 12  # Default fallback
        expected_crawl_count = (
            (expected_products_count + batch_size - 1) // batch_size
            if expected_products_count > 0
            else 0
        )
        logger.info(
            f"üìä S·ªë products: {expected_products_count}, S·ªë batches d·ª± ki·∫øn: {expected_crawl_count}"
        )

        # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø c√≥ s·∫µn b·∫±ng c√°ch th·ª≠ l·∫•y XCom
        # ƒêi·ªÅu n√†y gi√∫p x·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt s·ªë tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong
        actual_crawl_count = expected_crawl_count
        if expected_crawl_count > 0:
            # Th·ª≠ l·∫•y XCom t·ª´ map_index cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø
            # T√¨m map_index cao nh·∫•t c√≥ XCom
            task_id = "crawl_product_details.crawl_product_detail"
            max_found_index = -1

            # Binary search ƒë·ªÉ t√¨m map_index cao nh·∫•t c√≥ XCom (t·ªëi ∆∞u h∆°n linear search)
            # Th·ª≠ m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max index
            logger.info(
                f"üîç ƒêang ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø (d·ª± ki·∫øn: {expected_crawl_count})..."
            )
            test_indices = []
            if expected_crawl_count > 1000:
                # V·ªõi s·ªë l∆∞·ª£ng l·ªõn, test m·ªôt s·ªë ƒëi·ªÉm ƒë·ªÉ t√¨m max
                step = max(100, expected_crawl_count // 20)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            elif expected_crawl_count > 100:
                # V·ªõi s·ªë l∆∞·ª£ng trung b√¨nh, test nhi·ªÅu ƒëi·ªÉm h∆°n
                step = max(50, expected_crawl_count // 10)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            else:
                # V·ªõi s·ªë l∆∞·ª£ng nh·ªè, test t·∫•t c·∫£
                test_indices = list(range(expected_crawl_count))

            # T√¨m t·ª´ cu·ªëi v·ªÅ ƒë·∫ßu ƒë·ªÉ t√¨m max index nhanh h∆°n
            for test_idx in reversed(test_indices):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[test_idx]
                    )
                    if result:
                        max_found_index = test_idx
                        logger.info(f"‚úÖ T√¨m th·∫•y XCom t·∫°i map_index {test_idx}")
                        break
                except Exception as e:
                    logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {test_idx}: {e}")
                    pass

            if max_found_index >= 0:
                # T√¨m ch√≠nh x√°c map_index cao nh·∫•t b·∫±ng c√°ch t√¨m t·ª´ max_found_index
                # Ch·ªâ th·ª≠ th√™m t·ªëi ƒëa 200 map_index ti·∫øp theo ƒë·ªÉ tr√°nh qu√° l√¢u
                logger.info(f"üîç ƒêang t√¨m ch√≠nh x√°c max index t·ª´ {max_found_index}...")
                search_range = min(max_found_index + 200, expected_crawl_count)
                for idx in range(max_found_index + 1, search_range):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[idx]
                        )
                        if result:
                            max_found_index = idx
                        else:
                            # N·∫øu kh√¥ng c√≥ result, d·ª´ng l·∫°i (c√≥ th·ªÉ ƒë√£ ƒë·∫øn cu·ªëi)
                            break
                    except Exception as e:
                        # N·∫øu exception, c√≥ th·ªÉ l√† h·∫øt map_index
                        logger.debug(f"   Kh√¥ng c√≥ XCom t·∫°i map_index {idx}: {e}")
                        break

                actual_crawl_count = max_found_index + 1
                logger.info(
                    f"‚úÖ Ph√°t hi·ªán {actual_crawl_count} map_index th·ª±c t·∫ø c√≥ XCom (d·ª± ki·∫øn: {expected_crawl_count})"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y XCom n√†o, s·ª≠ d·ª•ng expected_crawl_count: {expected_crawl_count}. "
                    f"C√≥ th·ªÉ t·∫•t c·∫£ tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong."
                )
                actual_crawl_count = expected_crawl_count

        if actual_crawl_count == 0:
            logger.warning("=" * 70)
            logger.warning("‚ö†Ô∏è  KH√îNG C√ì PRODUCTS N√ÄO ƒê∆Ø·ª¢C CRAWL DETAIL!")
            logger.warning("=" * 70)
            logger.warning("üí° Nguy√™n nh√¢n c√≥ th·ªÉ:")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong database v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ c√≥ trong cache v·ªõi detail ƒë·∫ßy ƒë·ªß")
            logger.warning("   - T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl tr∆∞·ªõc ƒë√≥ (t·ª´ progress file)")
            logger.warning("   - Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c prepare ƒë·ªÉ crawl")
            logger.warning("=" * 70)
            logger.warning("üí° ƒê·ªÉ force crawl l·∫°i, ki·ªÉm tra task 'prepare_products_for_detail' log")
            logger.warning("=" * 70)
            # Tr·∫£ v·ªÅ products g·ªëc kh√¥ng c√≥ detail
            return {
                "products": products,
                "stats": {
                    "total_products": len(products),
                    "with_detail": 0,
                    "cached": 0,
                    "failed": 0,
                    "timeout": 0,
                    "crawled_count": 0,
                },
                "merged_at": datetime.now().isoformat(),
            }

        # L·∫•y detail results t·ª´ Dynamic Task Mapping
        task_id = "crawl_product_details.crawl_product_detail"
        all_detail_results = []

        # L·∫•y t·∫•t c·∫£ results b·∫±ng c√°ch l·∫•y t·ª´ng map_index ƒë·ªÉ tr√°nh gi·ªõi h·∫°n XCom
        # CH·ªà l·∫•y t·ª´ map_index 0 ƒë·∫øn actual_crawl_count - 1 (kh√¥ng ph·∫£i len(products))
        # Fetch detail results from crawled products

        # L·∫•y theo batch ƒë·ªÉ t·ªëi ∆∞u
        batch_size = 100
        total_batches = (actual_crawl_count + batch_size - 1) // batch_size
        logger.info(
            f"üì¶ S·∫Ω l·∫•y {actual_crawl_count} results trong {total_batches} batches (m·ªói batch {batch_size})"
        )

        for batch_num, start_idx in enumerate(range(0, actual_crawl_count, batch_size), 1):
            end_idx = min(start_idx + batch_size, actual_crawl_count)
            batch_map_indexes = list(range(start_idx, end_idx))

            # Heartbeat: log m·ªói batch ƒë·ªÉ Airflow bi·∫øt task v·∫´n ƒëang ch·∫°y
            if batch_num % 5 == 0 or batch_num == 1:
                logger.info(
                    f"üíì [Heartbeat] ƒêang x·ª≠ l√Ω batch {batch_num}/{total_batches} (index {start_idx}-{end_idx-1})..."
                )

            try:
                batch_results = ti.xcom_pull(
                    task_ids=task_id, key="return_value", map_indexes=batch_map_indexes
                )

                if batch_results:
                    if isinstance(batch_results, list):
                        # List results theo th·ª© t·ª± map_indexes
                        # M·ªói result c√≥ th·ªÉ l√† list (t·ª´ batch) ho·∫∑c dict (t·ª´ single)
                        for result in batch_results:
                            if result:
                                if isinstance(result, list):
                                    # Batch result: flatten list of results
                                    all_detail_results.extend([r for r in result if r])
                                elif isinstance(result, dict):
                                    # Single result
                                    all_detail_results.append(result)
                    elif isinstance(batch_results, dict):
                        # Dict v·ªõi key l√† map_index ho·∫∑c string
                        # L·∫•y t·∫•t c·∫£ values, s·∫Øp x·∫øp theo map_index n·∫øu c√≥ th·ªÉ
                        for value in batch_results.values():
                            if value:
                                if isinstance(value, list):
                                    # Batch result: flatten
                                    all_detail_results.extend([r for r in value if r])
                                elif isinstance(value, dict):
                                    # Single result
                                    all_detail_results.append(value)
                    else:
                        # Single result
                        if isinstance(batch_results, list):
                            # Batch result: flatten
                            all_detail_results.extend([r for r in batch_results if r])
                        else:
                            all_detail_results.append(batch_results)

                # Log progress m·ªói 5 batches ho·∫∑c m·ªói 10% progress
                if batch_num % max(5, total_batches // 10) == 0:
                    progress_pct = (
                        (len(all_detail_results) / actual_crawl_count * 100)
                        if actual_crawl_count > 0
                        else 0
                    )
                    logger.info(
                        f"üìä ƒê√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%)..."
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi l·∫•y batch {start_idx}-{end_idx}: {e}")
                logger.warning("   S·∫Ω th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y...")
                # Th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª trong batch n√†y
                for map_index in batch_map_indexes:
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )
                        if result:
                            if isinstance(result, list):
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                    except Exception as e2:
                        # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e2}")
                        pass

        logger.info(
            f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(all_detail_results)} detail results qua batch (mong ƒë·ª£i {actual_crawl_count})"
        )

        # N·∫øu kh√¥ng l·∫•y ƒë·ªß ho·∫∑c c√≥ l·ªói khi l·∫•y batch, th·ª≠ l·∫•y t·ª´ng map_index m·ªôt ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu
        # KH√îNG reset all_detail_results, ch·ªâ l·∫•y th√™m nh·ªØng map_index ch∆∞a c√≥
        if len(all_detail_results) < actual_crawl_count * 0.8:  # N·∫øu thi·∫øu h∆°n 20%
            # Log c·∫£nh b√°o n·∫øu thi·∫øu nhi·ªÅu
            missing_pct = (
                ((actual_crawl_count - len(all_detail_results)) / actual_crawl_count * 100)
                if actual_crawl_count > 0
                else 0
            )
            if missing_pct > 30:
                logger.warning(
                    f"‚ö†Ô∏è  Thi·∫øu {missing_pct:.1f}% results ({actual_crawl_count - len(all_detail_results)}/{actual_crawl_count}), "
                    f"c√≥ th·ªÉ do nhi·ªÅu tasks failed ho·∫∑c timeout"
                )
            logger.warning(
                f"‚ö†Ô∏è  Ch·ªâ l·∫•y ƒë∆∞·ª£c {len(all_detail_results)}/{actual_crawl_count} results qua batch, "
                f"th·ª≠ l·∫•y t·ª´ng map_index ƒë·ªÉ b√π v√†o ph·∫ßn thi·∫øu..."
            )

            # T·∫°o set c√°c product_id ƒë√£ c√≥ ƒë·ªÉ tr√°nh duplicate
            existing_product_ids = set()
            for result in all_detail_results:
                if isinstance(result, dict) and result.get("product_id"):
                    existing_product_ids.add(result.get("product_id"))

            missing_count = actual_crawl_count - len(all_detail_results)
            logger.info(
                f"üìä C·∫ßn l·∫•y th√™m ~{missing_count} results t·ª´ {actual_crawl_count} map_indexes"
            )

            # Heartbeat: log th∆∞·ªùng xuy√™n trong v√≤ng l·∫∑p d√†i
            fetched_count = 0
            for map_index in range(actual_crawl_count):  # CH·ªà l·∫•y t·ª´ 0 ƒë·∫øn actual_crawl_count - 1
                # Heartbeat m·ªói 100 items ƒë·ªÉ tr√°nh timeout
                if map_index % 100 == 0 and map_index > 0:
                    logger.info(
                        f"üíì [Heartbeat] ƒêang l·∫•y t·ª´ng map_index: {map_index}/{actual_crawl_count} "
                        f"(ƒë√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count})..."
                    )

                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[map_index]
                    )
                    if result:
                        # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ (tr√°nh duplicate)
                        product_id_to_check = None
                        if isinstance(result, dict):
                            product_id_to_check = result.get("product_id")
                        elif (
                            isinstance(result, list)
                            and len(result) > 0
                            and isinstance(result[0], dict)
                        ):
                            product_id_to_check = result[0].get("product_id")

                        # Ch·ªâ th√™m n·∫øu product_id ch∆∞a c√≥ trong danh s√°ch
                        if (
                            not product_id_to_check
                            or product_id_to_check not in existing_product_ids
                        ):
                            if isinstance(result, list):
                                for r in result:
                                    if isinstance(r, dict) and r.get("product_id"):
                                        existing_product_ids.add(r.get("product_id"))
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                if product_id_to_check:
                                    existing_product_ids.add(product_id_to_check)
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                            fetched_count += 1

                    # Log progress m·ªói 200 items
                    if (map_index + 1) % 200 == 0:
                        progress_pct = (
                            (len(all_detail_results) / actual_crawl_count * 100)
                            if actual_crawl_count > 0
                            else 0
                        )
                        logger.info(
                            f"üìä ƒê√£ l·∫•y t·ªïng {len(all_detail_results)}/{actual_crawl_count} results ({progress_pct:.1f}%) t·ª´ng map_index..."
                        )
                except Exception as e:
                    # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                    logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e}")
                    pass

            logger.info(
                f"‚úÖ Sau khi l·∫•y t·ª´ng map_index: t·ªïng {len(all_detail_results)} detail results (l·∫•y th√™m {fetched_count})"
            )

        # T·∫°o dict ƒë·ªÉ lookup nhanh
        detail_dict = {}
        stats = {
            "total_products": len(products),
            "crawled_count": 0,  # S·ªë l∆∞·ª£ng products th·ª±c s·ª± ƒë∆∞·ª£c crawl detail
            "with_detail": 0,
            "cached": 0,
            "failed": 0,
            "timeout": 0,
            "degraded": 0,
            "circuit_breaker_open": 0,
        }

        logger.info(f"üìä ƒêang x·ª≠ l√Ω {len(all_detail_results)} detail results...")

        # Ki·ªÉm tra n·∫øu c√≥ qu√° nhi·ªÅu k·∫øt qu·∫£ None ho·∫∑c invalid
        valid_results = 0
        error_details = {}  # Th·ªëng k√™ chi ti·∫øt c√°c lo·∫°i l·ªói
        failed_products = []  # Danh s√°ch products b·ªã fail ƒë·ªÉ ph√¢n t√≠ch

        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get("product_id")
                if product_id:
                    detail_dict[product_id] = detail_result
                    valid_results += 1
                    status = detail_result.get("status", "failed")
                    error = detail_result.get("error")

                    # ƒê·∫øm s·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl (t·∫•t c·∫£ c√°c status tr·ª´ "not_crawled")
                    if status in [
                        "success",
                        "cached",
                        "failed",
                        "timeout",
                        "degraded",
                        "circuit_breaker_open",
                        "selenium_error",
                        "network_error",
                        "extract_error",
                        "validation_error",
                        "memory_error",
                    ]:
                        stats["crawled_count"] += 1

                    if status == "success":
                        stats["with_detail"] += 1
                    elif status == "cached":
                        stats["cached"] += 1
                    elif status == "timeout":
                        stats["timeout"] += 1
                        error_details["timeout"] = error_details.get("timeout", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "degraded":
                        stats["degraded"] += 1
                        error_details["degraded"] = error_details.get("degraded", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "circuit_breaker_open":
                        stats["circuit_breaker_open"] += 1
                        error_details["circuit_breaker_open"] = (
                            error_details.get("circuit_breaker_open", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "selenium_error":
                        stats["failed"] += 1
                        error_details["selenium_error"] = error_details.get("selenium_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "extract_error":
                        stats["failed"] += 1
                        error_details["extract_error"] = error_details.get("extract_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "network_error":
                        stats["failed"] += 1
                        error_details["network_error"] = error_details.get("network_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "memory_error":
                        stats["failed"] += 1
                        error_details["memory_error"] = error_details.get("memory_error", 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    elif status == "validation_error":
                        stats["failed"] += 1
                        error_details["validation_error"] = (
                            error_details.get("validation_error", 0) + 1
                        )
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )
                    else:
                        stats["failed"] += 1
                        error_type = status if status else "unknown"
                        error_details[error_type] = error_details.get(error_type, 0) + 1
                        failed_products.append(
                            {"product_id": product_id, "status": status, "error": error}
                        )

        logger.info(
            f"üìä C√≥ {valid_results} detail results h·ª£p l·ªá t·ª´ {len(all_detail_results)} results"
        )

        if valid_results < len(all_detail_results):
            logger.warning(
                f"‚ö†Ô∏è  C√≥ {len(all_detail_results) - valid_results} results kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu product_id"
            )

        # Log chi ti·∫øt v·ªÅ c√°c l·ªói
        if error_details:
            logger.info("=" * 70)
            logger.info("üìã PH√ÇN T√çCH C√ÅC LO·∫†I L·ªñI")
            logger.info("=" * 70)
            for error_type, count in sorted(
                error_details.items(), key=lambda x: x[1], reverse=True
            ):
                logger.info(f"  ‚ùå {error_type}: {count} products")
            logger.info("=" * 70)

            # Log m·ªôt s·ªë products b·ªã fail ƒë·∫ßu ti√™n ƒë·ªÉ ph√¢n t√≠ch
            if failed_products:
                logger.info(f"üìù M·∫´u {min(10, len(failed_products))} products b·ªã fail ƒë·∫ßu ti√™n:")
                for i, failed in enumerate(failed_products[:10], 1):
                    logger.info(
                        f"  {i}. Product ID: {failed['product_id']}, Status: {failed['status']}, Error: {failed.get('error', 'N/A')[:100]}"
                    )

        # L∆∞u th√¥ng tin l·ªói v√†o stats ƒë·ªÉ ph√¢n t√≠ch sau
        stats["error_details"] = error_details
        stats["failed_products_count"] = len(failed_products)

        # Merge detail v√†o products
        # CH·ªà l∆∞u products c√≥ detail V√Ä status == "success" (kh√¥ng l∆∞u cached ho·∫∑c failed)
        products_with_detail = []
        products_without_detail = 0
        products_cached = 0
        products_failed = 0
        products_no_brand = 0  # ƒê·∫øm s·ªë products b·ªã lo·∫°i b·ªè v√¨ brand null

        for product in products:
            product_id = product.get("product_id")
            detail_result = detail_dict.get(product_id)

            if detail_result and detail_result.get("detail"):
                status = detail_result.get("status", "failed")

                # CH·ªà l∆∞u products c√≥ status == "success" (ƒë√£ crawl th√†nh c√¥ng, kh√¥ng ph·∫£i t·ª´ cache)
                if status == "success":
                    # Merge detail v√†o product
                    detail = detail_result["detail"]

                    # Ki·ªÉm tra n·∫øu detail l√† None ho·∫∑c r·ªóng
                    if detail is None:
                        logger.warning(f"‚ö†Ô∏è  Detail l√† None cho product {product_id}")
                        products_failed += 1
                        continue

                    # Ki·ªÉm tra n·∫øu detail l√† string (JSON), parse n√≥
                    if isinstance(detail, str):
                        # B·ªè qua string r·ªóng
                        if not detail.strip():
                            logger.warning(f"‚ö†Ô∏è  Detail l√† string r·ªóng cho product {product_id}")
                            products_failed += 1
                            continue

                        try:
                            import json

                            detail = json.loads(detail)
                        except (json.JSONDecodeError, TypeError) as e:
                            logger.warning(
                                f"‚ö†Ô∏è  Kh√¥ng th·ªÉ parse detail JSON cho product {product_id}: {e}, detail type: {type(detail)}, detail value: {str(detail)[:100]}"
                            )
                            products_failed += 1
                            continue

                    # Ki·ªÉm tra n·∫øu detail kh√¥ng ph·∫£i l√† dict
                    if not isinstance(detail, dict):
                        logger.warning(
                            f"‚ö†Ô∏è  Detail kh√¥ng ph·∫£i l√† dict cho product {product_id}: {type(detail)}, value: {str(detail)[:100]}"
                        )
                        products_failed += 1
                        continue

                    product_with_detail = {**product}

                    # Update c√°c tr∆∞·ªùng t·ª´ detail
                    if detail.get("price"):
                        product_with_detail["price"] = detail["price"]
                    if detail.get("rating"):
                        product_with_detail["rating"] = detail["rating"]
                    if detail.get("description"):
                        product_with_detail["description"] = detail["description"]
                    if detail.get("specifications"):
                        product_with_detail["specifications"] = detail["specifications"]
                    if detail.get("images"):
                        product_with_detail["images"] = detail["images"]
                    if detail.get("brand"):
                        product_with_detail["brand"] = detail["brand"]
                    if detail.get("seller"):
                        product_with_detail["seller"] = detail["seller"]
                    if detail.get("stock"):
                        product_with_detail["stock"] = detail["stock"]
                    if detail.get("shipping"):
                        product_with_detail["shipping"] = detail["shipping"]
                    # C·∫≠p nh·∫≠t sales_count: ∆∞u ti√™n t·ª´ detail, n·∫øu kh√¥ng c√≥ th√¨ d√πng t·ª´ product g·ªëc
                    # Ch·ªâ c·∫ßn c√≥ trong m·ªôt trong hai l√† ƒë·ªß
                    if detail.get("sales_count") is not None:
                        product_with_detail["sales_count"] = detail["sales_count"]
                    elif product.get("sales_count") is not None:
                        product_with_detail["sales_count"] = product["sales_count"]
                    # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng c√≥, gi·ªØ None (ƒë√£ c√≥ trong product g·ªëc)

                    # Th√™m metadata
                    product_with_detail["detail_crawled_at"] = detail_result.get("crawled_at")
                    product_with_detail["detail_status"] = status
                    detail_metadata = detail.get("_metadata")
                    if detail_metadata:
                        product_with_detail["_metadata"] = detail_metadata
                        product_with_detail["_metadata"]["crawl_status"] = status
                        if detail_result.get("crawled_at"):
                            product_with_detail["_metadata"]["completed_at"] = detail_result.get(
                                "crawled_at"
                            )
                    elif status or detail_result.get("crawled_at"):
                        product_with_detail["_metadata"] = {
                            "crawl_status": status,
                            "completed_at": detail_result.get("crawled_at"),
                        }

                    # CRITICAL: L·ªçc b·ªè products c√≥ brand null/empty
                    # Brand thi·∫øu th∆∞·ªùng d·∫´n ƒë·∫øn nhi·ªÅu tr∆∞·ªùng kh√°c c≈©ng thi·∫øu
                    # Seller c√≥ th·ªÉ l√† "Unknown" - v·∫´n l∆∞u l·∫°i
                    # Nh·ªØng products n√†y s·∫Ω ƒë∆∞·ª£c crawl l·∫°i trong l·∫ßn ch·∫°y ti·∫øp theo
                    brand = product_with_detail.get("brand")

                    # Only skip if BRAND is missing/empty (seller can be "Unknown")
                    if not brand or (isinstance(brand, str) and not brand.strip()):
                        logger.warning(
                            f"‚ö†Ô∏è  Product {product_id} ({product_with_detail.get('name', 'Unknown')[:50]}) "
                            f"c√≥ brand null/empty, s·∫Ω b·ªè qua ƒë·ªÉ crawl l·∫°i l·∫ßn sau"
                        )
                        products_no_brand += 1
                        products_failed += 1
                        continue

                    products_with_detail.append(product_with_detail)
                elif status == "cached":
                    # Kh√¥ng l∆∞u products t·ª´ cache (ch·ªâ l∆∞u products ƒë√£ crawl m·ªõi)
                    products_cached += 1
                else:
                    # Kh√¥ng l∆∞u products b·ªã fail
                    products_failed += 1
            else:
                # Kh√¥ng l∆∞u products kh√¥ng c√≥ detail
                products_without_detail += 1

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä MERGE DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products ban ƒë·∫ßu: {stats['total_products']}")
        logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {stats['crawled_count']}")
        logger.info(f"‚úÖ C√≥ detail (success): {stats['with_detail']}")
        logger.info(f"üì¶ C√≥ detail (cached): {stats['cached']}")
        logger.info(f"‚ö†Ô∏è  Degraded: {stats['degraded']}")
        logger.info(f"‚ö° Circuit breaker open: {stats['circuit_breaker_open']}")
        logger.info(f"‚ùå Failed: {stats['failed']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout']}")

        # T√≠nh t·ªïng c√≥ detail (success + cached)
        total_with_detail = stats["with_detail"] + stats["cached"]

        # T·ª∑ l·ªá th√†nh c√¥ng d·ª±a tr√™n s·ªë l∆∞·ª£ng ƒë∆∞·ª£c crawl (quan tr·ªçng h∆°n)
        if stats["crawled_count"] > 0:
            success_rate = (stats["with_detail"] / stats["crawled_count"]) * 100
            logger.info(
                f"üìà T·ª∑ l·ªá th√†nh c√¥ng (d·ª±a tr√™n crawled): {stats['with_detail']}/{stats['crawled_count']} ({success_rate:.1f}%)"
            )

        # T·ª∑ l·ªá c√≥ detail trong t·ªïng products (ƒë·ªÉ tham kh·∫£o)
        if stats["total_products"] > 0:
            detail_coverage = total_with_detail / stats["total_products"] * 100
            logger.info(
                f"üìä T·ª∑ l·ªá c√≥ detail (trong t·ªïng products): {total_with_detail}/{stats['total_products']} ({detail_coverage:.1f}%)"
            )

        logger.info("=" * 70)
        logger.info(
            f"üíæ Products ƒë∆∞·ª£c l∆∞u v√†o file: {len(products_with_detail)} (ch·ªâ l∆∞u products c√≥ status='success')"
        )
        logger.info(f"üì¶ Products t·ª´ cache (ƒë√£ b·ªè qua): {products_cached}")
        logger.info(f"‚ùå Products b·ªã fail (ƒë√£ b·ªè qua): {products_failed}")
        logger.info(f"üö´ Products kh√¥ng c√≥ brand (ƒë√£ b·ªè qua ƒë·ªÉ crawl l·∫°i): {products_no_brand}")
        logger.info(f"üö´ Products kh√¥ng c√≥ detail (ƒë√£ b·ªè qua): {products_without_detail}")
        logger.info("=" * 70)

        # C·∫£nh b√°o n·∫øu c√≥ nhi·ªÅu products kh√¥ng c√≥ brand (>10% total products)
        if products_no_brand > 0 and stats["total_products"] > 0:
            no_brand_rate = (products_no_brand / stats["total_products"]) * 100
            if no_brand_rate > 10:
                logger.warning("=" * 70)
                logger.warning(
                    f"‚ö†Ô∏è  C·∫¢NH B√ÅO: C√≥ {products_no_brand} products ({no_brand_rate:.1f}%) kh√¥ng c√≥ brand!"
                )
                logger.warning("   Nh·ªØng products n√†y s·∫Ω ƒë∆∞·ª£c crawl l·∫°i trong l·∫ßn ch·∫°y ti·∫øp theo.")
                logger.warning("   Nguy√™n nh√¢n c√≥ th·ªÉ:")
                logger.warning("   - Trang detail kh√¥ng load ƒë·∫ßy ƒë·ªß (network issue, timeout)")
                logger.warning("   - HTML structure thay ƒë·ªïi (c·∫ßn update selector)")
                logger.warning("   - Rate limit qu√° cao (c·∫ßn gi·∫£m TIKI_DETAIL_RATE_LIMIT_DELAY)")
                logger.warning("=" * 70)
            elif no_brand_rate > 0:
                logger.info(
                    f"üí° C√≥ {products_no_brand} products ({no_brand_rate:.1f}%) kh√¥ng c√≥ brand, s·∫Ω crawl l·∫°i l·∫ßn sau"
                )

        # C·∫≠p nh·∫≠t stats ƒë·ªÉ ph·∫£n √°nh s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c l∆∞u
        stats["products_saved"] = len(products_with_detail)
        stats["products_skipped"] = products_without_detail
        stats["products_cached_skipped"] = products_cached
        stats["products_failed_skipped"] = products_failed
        stats["products_no_brand_skipped"] = products_no_brand

        result = {
            "products": products_with_detail,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
            "note": f"Ch·ªâ l∆∞u {len(products_with_detail)} products c√≥ status='success' v√† brand/seller kh√¥ng null (ƒë√£ b·ªè qua {products_cached} cached, {products_failed} failed, {products_no_brand} kh√¥ng c√≥ brand, {products_without_detail} kh√¥ng c√≥ detail)",
        }

        return result

    except ValueError as e:
        logger.error(f"‚ùå Validation error khi merge details: {e}", exc_info=True)
        # N·∫øu l√† validation error (thi·∫øu products), return empty result thay v√¨ raise
        return {
            "products": [],
            "stats": {
                "total_products": 0,
                "crawled_count": 0,  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
                "with_detail": 0,
                "cached": 0,
                "failed": 0,
                "timeout": 0,
            },
            "merged_at": datetime.now().isoformat(),
            "error": str(e),
        }
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge details: {e}", exc_info=True)
        # Log chi ti·∫øt context ƒë·ªÉ debug
        logger.error(f"   Context keys: {list(context.keys()) if context else 'None'}")
        try:
            ti = context.get("ti")
            if ti:
                logger.error(f"   Task ID: {ti.task_id}, DAG ID: {ti.dag_id}, Run ID: {ti.run_id}")
        except Exception:
            pass
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: L∆∞u products v·ªõi detail v√†o file

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products with Detail")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y k·∫øt qu·∫£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="crawl_product_details.merge_product_details")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_product_details")
            except Exception:
                pass

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y merge result t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})
        note = merge_result.get("note", "Crawl t·ª´ Airflow DAG v·ªõi product details")

        logger.info(f"üíæ ƒêang l∆∞u {len(products)} products v·ªõi detail...")

        # Log th√¥ng tin v·ªÅ crawl detail
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")

        if stats.get("products_skipped"):
            logger.info(f"üö´ ƒê√£ b·ªè qua {stats.get('products_skipped')} products kh√¥ng c√≥ detail")

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": note,
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} products v·ªõi detail v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products with detail: {e}", exc_info=True)
        raise


def transform_products(**context) -> dict[str, Any]:
    """
    Task: Transform d·ªØ li·ªáu s·∫£n ph·∫©m (normalize, validate, compute fields)

    Returns:
        Dict: K·∫øt qu·∫£ transform v·ªõi transformed products v√† stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Transform Products")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y file t·ª´ save_products_with_detail
        output_file = None
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
        except Exception:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
            except Exception:
                pass

        if not output_file:
            output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {output_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc file: {output_file}")

        # ƒê·ªçc products t·ª´ file
        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})
        logger.info(f"üìä T·ªïng s·ªë products trong file: {len(products)}")

        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")

        # B·ªï sung category_url v√† category_id tr∆∞·ªõc khi transform
        logger.info("üîó ƒêang b·ªï sung category_url v√† category_id...")

        # B∆∞·ªõc 1: Load category_url mapping t·ª´ products.json (n·∫øu c√≥)
        category_url_mapping = {}  # product_id -> category_url
        products_file = OUTPUT_DIR / "products.json"
        if products_file.exists():
            try:
                logger.info(f"üìñ ƒêang ƒë·ªçc category_url mapping t·ª´: {products_file}")
                with open(products_file, encoding="utf-8") as f:
                    products_data = json.load(f)

                products_list = []
                if isinstance(products_data, list):
                    products_list = products_data
                elif isinstance(products_data, dict):
                    if "products" in products_data:
                        products_list = products_data["products"]
                    elif "data" in products_data and isinstance(products_data["data"], dict):
                        products_list = products_data["data"].get("products", [])

                for product in products_list:
                    product_id = product.get("product_id")
                    category_url = product.get("category_url")
                    if product_id and category_url:
                        category_url_mapping[product_id] = category_url

                logger.info(
                    f"‚úÖ ƒê√£ load {len(category_url_mapping)} category_url mappings t·ª´ products.json"
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc products.json: {e}")

        # B∆∞·ªõc 2: Import utility ƒë·ªÉ extract category_id
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n utils module
            utils_paths = [
                "/opt/airflow/src/pipelines/crawl/utils.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl", "utils.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "crawl", "utils.py"),
            ]

            utils_path = None
            for path in utils_paths:
                if os.path.exists(path):
                    utils_path = path
                    break

            if utils_path:
                import importlib.util

                spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
                utils_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utils_module)
                extract_category_id_from_url = utils_module.extract_category_id_from_url
            else:
                # Fallback: ƒë·ªãnh nghƒ©a h√†m ƒë∆°n gi·∫£n
                import re

                def extract_category_id_from_url(url: str) -> str | None:
                    if not url:
                        return None
                    match = re.search(r"/c(\d+)", url)
                    if match:
                        return f"c{match.group(1)}"
                    return None

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ import extract_category_id_from_url: {e}")
            import re

            def extract_category_id_from_url(url: str) -> str | None:
                if not url:
                    return None
                match = re.search(r"/c(\d+)", url)
                if match:
                    return f"c{match.group(1)}"
                return None

        # B∆∞·ªõc 3: B·ªï sung category_url, category_id v√† ENRICH category_path cho products
        updated_count = 0
        category_id_added = 0
        category_path_count = 0
        category_path_enriched = 0

        # B∆∞·ªõc 3a: Build category_path lookup t·ª´ categories file
        category_path_lookup: dict[str, list] = {}  # category_id -> category_path

        if CATEGORIES_FILE.exists():
            try:
                logger.info(f"üìñ ƒêang load categories t·ª´: {CATEGORIES_FILE}")
                with open(CATEGORIES_FILE, encoding="utf-8") as cf:
                    raw_categories = json.load(cf)

                for cat in raw_categories:
                    cat_id = cat.get("category_id")
                    cat_path = cat.get("category_path")

                    # Ch·ªâ th√™m v√†o lookup n·∫øu c√≥ category_id v√† category_path
                    if cat_id and cat_path:
                        category_path_lookup[cat_id] = cat_path

                logger.info(f"‚úÖ Loaded {len(category_path_lookup)} category_path t·ª´ file")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è L·ªói ƒë·ªçc categories file: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è Categories file kh√¥ng t·ªìn t·∫°i: {CATEGORIES_FILE}")

        for product in products:
            product_id = product.get("product_id")

            # B·ªï sung category_url n·∫øu ch∆∞a c√≥
            if not product.get("category_url") and product_id in category_url_mapping:
                product["category_url"] = category_url_mapping[product_id]
                updated_count += 1

            # Extract category_id t·ª´ category_url n·∫øu c√≥
            category_url = product.get("category_url")
            if category_url and not product.get("category_id"):
                category_id = extract_category_id_from_url(category_url)
                if category_id:
                    product["category_id"] = category_id
                    category_id_added += 1

            # Enrich category_path t·ª´ lookup map (n·∫øu ch∆∞a c√≥)
            if product.get("category_id") and not product.get("category_path"):
                cat_id = product["category_id"]
                if cat_id in category_path_lookup:
                    product["category_path"] = category_path_lookup[cat_id]
                    category_path_enriched += 1

            # ƒê·∫£m b·∫£o category_path ƒë∆∞·ª£c gi·ªØ l·∫°i
            if product.get("category_path"):
                category_path_count += 1

        if updated_count > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_url cho {updated_count} products")
        if category_id_added > 0:
            logger.info(f"‚úÖ ƒê√£ b·ªï sung category_id cho {category_id_added} products")
        if category_path_enriched > 0:
            logger.info(f"‚úÖ ƒê√£ enrich category_path cho {category_path_enriched} products")
        if category_path_count > 0:
            logger.info(f"‚úÖ T·ªïng products c√≥ category_path: {category_path_count}/{len(products)}")

        # Import DataTransformer
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n transform module
            transform_paths = [
                "/opt/airflow/src/pipelines/transform/transformer.py",
                os.path.abspath(
                    os.path.join(
                        dag_file_dir, "..", "..", "src", "pipelines", "transform", "transformer.py"
                    )
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "transform", "transformer.py"),
            ]

            transformer_path = None
            for path in transform_paths:
                if os.path.exists(path):
                    transformer_path = path
                    break

            if not transformer_path:
                raise ImportError("Kh√¥ng t√¨m th·∫•y transformer.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("transformer", transformer_path)
            transformer_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(transformer_module)
            DataTransformer = transformer_module.DataTransformer

            # Transform products
            transformer = DataTransformer(
                strict_validation=False, remove_invalid=True, normalize_fields=True
            )

            transformed_products, transform_stats = transformer.transform_products(
                products, validate=True
            )

            logger.info("=" * 70)
            logger.info("üìä TRANSFORM RESULTS")
            logger.info("=" * 70)
            logger.info(f"‚úÖ Valid products: {transform_stats['valid_products']}")
            logger.info(f"‚ùå Invalid products: {transform_stats['invalid_products']}")
            logger.info(f"üîÑ Duplicates removed: {transform_stats['duplicates_removed']}")
            logger.info("=" * 70)

            # L∆∞u transformed products v√†o file
            processed_dir = DATA_DIR / "processed"
            processed_dir.mkdir(parents=True, exist_ok=True)
            transformed_file = processed_dir / "products_transformed.json"

            output_data = {
                "transformed_at": datetime.now().isoformat(),
                "source_file": output_file,
                "total_products": len(products),
                "transform_stats": transform_stats,
                "products": transformed_products,
            }

            atomic_write_file(str(transformed_file), output_data, **context)
            logger.info(
                f"‚úÖ ƒê√£ l∆∞u {len(transformed_products)} transformed products v√†o: {transformed_file}"
            )

            return {
                "transformed_file": str(transformed_file),
                "transformed_count": len(transformed_products),
                "transform_stats": transform_stats,
            }

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataTransformer: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi transform products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong transform_products task: {e}", exc_info=True)
        raise


def _import_postgres_storage():
    """
    Helper function ƒë·ªÉ import PostgresStorage v·ªõi fallback logic
    H·ªó tr·ª£ c·∫£ m√¥i tr∆∞·ªùng Airflow (importlib) v√† m√¥i tr∆∞·ªùng b√¨nh th∆∞·ªùng

    Returns:
        PostgresStorage class ho·∫∑c None n·∫øu kh√¥ng th·ªÉ import
    """
    try:
        # Th·ª≠ import t·ª´ __init__.py c·ªßa storage module
        from pipelines.crawl.storage import PostgresStorage

        return PostgresStorage
    except ImportError:
        try:
            # Th·ª≠ import tr·ª±c ti·∫øp t·ª´ file
            from pipelines.crawl.storage.postgres_storage import PostgresStorage

            return PostgresStorage
        except ImportError:
            try:
                import importlib.util
                from pathlib import Path

                # T√¨m ƒë∆∞·ªùng d·∫´n ƒë·∫øn postgres_storage.py
                possible_paths = [
                    # T·ª´ /opt/airflow/src (Docker default - ∆∞u ti√™n)
                    Path("/opt/airflow/src/pipelines/crawl/storage/postgres_storage.py"),
                    # T·ª´ dag_file_dir
                    Path(dag_file_dir).parent.parent
                    / "src"
                    / "pipelines"
                    / "crawl"
                    / "storage"
                    / "postgres_storage.py",
                    # T·ª´ current working directory
                    Path(os.getcwd())
                    / "src"
                    / "pipelines"
                    / "crawl"
                    / "storage"
                    / "postgres_storage.py",
                    # T·ª´ workspace root
                    Path("/workspace/src/pipelines/crawl/storage/postgres_storage.py"),
                ]

                postgres_storage_path = None
                for path in possible_paths:
                    if path.exists() and path.is_file():
                        postgres_storage_path = path
                        break

                if postgres_storage_path:
                    # S·ª≠ d·ª•ng importlib ƒë·ªÉ load tr·ª±c ti·∫øp t·ª´ file
                    spec = importlib.util.spec_from_file_location(
                        "postgres_storage", postgres_storage_path
                    )
                    if spec and spec.loader:
                        postgres_storage_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(postgres_storage_module)
                        return postgres_storage_module.PostgresStorage

                # N·∫øu kh√¥ng t√¨m th·∫•y file, th·ª≠ th√™m src v√†o path v√† import absolute
                src_paths = [
                    Path("/opt/airflow/src"),
                    Path(dag_file_dir).parent.parent / "src",
                    Path(os.getcwd()) / "src",
                ]

                for src_path in src_paths:
                    if src_path.exists() and str(src_path) not in sys.path:
                        sys.path.insert(0, str(src_path))
                        try:
                            from pipelines.crawl.storage import PostgresStorage

                            return PostgresStorage
                        except ImportError:
                            try:
                                from pipelines.crawl.storage.postgres_storage import PostgresStorage

                                return PostgresStorage
                            except ImportError:
                                continue

                return None
            except Exception:
                return None


def load_products(**context) -> dict[str, Any]:
    """
    Task: Load d·ªØ li·ªáu ƒë√£ transform v√†o database

    Returns:
        Dict: K·∫øt qu·∫£ load v·ªõi stats
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Load Products to Database")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y transformed file t·ª´ transform_products task
        transform_result = None
        try:
            transform_result = ti.xcom_pull(task_ids="transform_and_load.transform_products")
        except Exception:
            try:
                transform_result = ti.xcom_pull(task_ids="transform_products")
            except Exception:
                pass

        if not transform_result:
            # Fallback: t√¨m file transformed
            processed_dir = DATA_DIR / "processed"
            transformed_file = processed_dir / "products_transformed.json"
            if transformed_file.exists():
                transform_result = {"transformed_file": str(transformed_file)}
            else:
                raise ValueError("Kh√¥ng t√¨m th·∫•y transform result t·ª´ XCom ho·∫∑c file")

        transformed_file = transform_result.get("transformed_file")
        if not transformed_file or not os.path.exists(transformed_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file transformed: {transformed_file}")

        logger.info(f"üìÇ ƒêang ƒë·ªçc transformed file: {transformed_file}")

        # ƒê·ªçc transformed products
        with open(transformed_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products ƒë·ªÉ load: {len(products)}")

        # Import DataLoader
        try:
            # T√¨m ƒë∆∞·ªùng d·∫´n load module
            load_paths = [
                "/opt/airflow/src/pipelines/load/loader.py",
                os.path.abspath(
                    os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "load", "loader.py")
                ),
                os.path.join(os.getcwd(), "src", "pipelines", "load", "loader.py"),
            ]

            loader_path = None
            for path in load_paths:
                if os.path.exists(path):
                    loader_path = path
                    break

            if not loader_path:
                raise ImportError("Kh√¥ng t√¨m th·∫•y loader.py")

            import importlib.util

            spec = importlib.util.spec_from_file_location("loader", loader_path)
            loader_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(loader_module)
            DataLoader = loader_module.DataLoader

            # L·∫•y database config t·ª´ Airflow Variables ho·∫∑c environment variables
            # ∆Øu ti√™n: Airflow Variables > Environment Variables > Default
            db_host = get_variable("POSTGRES_HOST", default=os.getenv("POSTGRES_HOST", "postgres"))
            db_port = int(get_variable("POSTGRES_PORT", default=os.getenv("POSTGRES_PORT", "5432")))
            db_name = get_variable("POSTGRES_DB", default=os.getenv("POSTGRES_DB", "crawl_data"))
            db_user = get_variable("POSTGRES_USER", default=os.getenv("POSTGRES_USER", "postgres"))
            # trufflehog:ignore - Fallback for development, production uses Airflow Variables
            db_password = get_variable(
                "POSTGRES_PASSWORD", default=os.getenv("POSTGRES_PASSWORD", "postgres")
            )

            # Load v√†o database
            loader = DataLoader(
                host=db_host,
                port=db_port,
                database=db_name,
                user=db_user,
                password=db_password,
                batch_size=int(get_variable("TIKI_SAVE_BATCH_SIZE", default=2000)),
                enable_db=True,
            )

            try:
                # L∆∞u v√†o processed directory
                processed_dir = DATA_DIR / "processed"
                processed_dir.mkdir(parents=True, exist_ok=True)
                final_file = processed_dir / "products_final.json"

                # Kh·ªüi t·∫°o bi·∫øn ƒë·ªÉ l∆∞u s·ªë l∆∞·ª£ng products
                count_before = None
                count_after = None
                deleted_no_brand_count = 0  # Cleanup ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü task cleanup_incomplete_products

                # NOTE: Cleanup incomplete products (missing seller/brand) ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω TR∆Ø·ªöC crawl
                # (task cleanup_incomplete_products) ƒë·ªÉ tr√°nh l√£ng ph√≠ t√†i nguy√™n.
                # Kh√¥ng c·∫ßn cleanup l·∫°i ·ªü ƒë√¢y n·ªØa (preventive approach > reactive cleanup).
                
                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB tr∆∞·ªõc khi load (for stats)
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is not None:
                        storage = PostgresStorage(
                            host=db_host,
                            port=db_port,
                            database=db_name,
                            user=db_user,
                            password=db_password,
                        )
                        with storage.get_connection() as conn:
                            with conn.cursor() as cur:
                                cur.execute("SELECT COUNT(*) FROM products;")
                                count_before = cur.fetchone()[0]
                        storage.close()
                        logger.info(f"üìä S·ªë products trong DB tr∆∞·ªõc khi load: {count_before}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë products trong DB: {e}")
                    count_before = None

                load_stats = loader.load_products(
                    products,
                    save_to_file=str(final_file),
                    upsert=True,  # UPDATE n·∫øu ƒë√£ t·ªìn t·∫°i, INSERT n·∫øu m·ªõi
                    validate_before_load=True,
                )

                # Ki·ªÉm tra s·ªë l∆∞·ª£ng products trong DB sau khi load
                try:
                    PostgresStorage = _import_postgres_storage()
                    if PostgresStorage is None:
                        raise ImportError("Kh√¥ng th·ªÉ import PostgresStorage")
                    storage = PostgresStorage(
                        host=db_host,
                        port=db_port,
                        database=db_name,
                        user=db_user,
                        password=db_password,
                    )
                    with storage.get_connection() as conn:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM products;")
                            count_after = cur.fetchone()[0]
                    storage.close()
                    logger.info(f"üìä S·ªë products trong DB sau khi load: {count_after}")
                    if count_before is not None:
                        diff = count_after - count_before
                        if diff > 0:
                            logger.info(f"‚úÖ ƒê√£ th√™m {diff} products m·ªõi v√†o DB")
                        elif diff == 0:
                            logger.info("‚ÑπÔ∏è  Kh√¥ng c√≥ products m·ªõi (ch·ªâ UPDATE c√°c products ƒë√£ c√≥)")
                        else:
                            logger.warning(
                                f"‚ö†Ô∏è  S·ªë l∆∞·ª£ng products gi·∫£m {abs(diff)} (c√≥ th·ªÉ do x√≥a ho·∫∑c l·ªói)"
                            )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ki·ªÉm tra s·ªë l∆∞·ª£ng products sau khi load: {e}")
                    count_after = None

                logger.info("=" * 70)
                logger.info("üìä LOAD RESULTS")
                logger.info("=" * 70)
                if deleted_no_brand_count > 0:
                    logger.info(f"üóëÔ∏è  Deleted (brand null): {deleted_no_brand_count} products")
                logger.info(f"‚úÖ DB loaded: {load_stats['db_loaded']} products")
                if load_stats.get("inserted_count") is not None:
                    logger.info(
                        f"   - INSERT (products m·ªõi): {load_stats.get('inserted_count', 0)}"
                    )
                    logger.info(
                        f"   - UPDATE (products ƒë√£ c√≥): {load_stats.get('updated_count', 0)}"
                    )
                logger.info(f"‚úÖ File loaded: {load_stats['file_loaded']}")
                logger.info(f"‚ùå Failed: {load_stats['failed_count']}")
                if count_before is not None and count_after is not None:
                    diff = count_after - count_before
                    logger.info(
                        f"üìà DB count: {count_before} ‚Üí {count_after} (thay ƒë·ªïi: {diff:+d})"
                    )
                    if diff == 0 and load_stats.get("inserted_count", 0) == 0:
                        logger.info("‚ÑπÔ∏è  Kh√¥ng c√≥ products m·ªõi - ch·ªâ UPDATE c√°c products ƒë√£ c√≥")
                    elif diff > 0:
                        logger.info(f"‚úÖ ƒê√£ th√™m {diff} products m·ªõi v√†o DB")
                logger.info("=" * 70)
                logger.info(
                    "‚ÑπÔ∏è  L∆∞u √Ω: V·ªõi upsert=True, products ƒë√£ c√≥ s·∫Ω ƒë∆∞·ª£c UPDATE (kh√¥ng tƒÉng s·ªë l∆∞·ª£ng)"
                )
                logger.info(
                    "‚ÑπÔ∏è  Ch·ªâ products m·ªõi (product_id ch∆∞a c√≥) m·ªõi ƒë∆∞·ª£c INSERT v√† tƒÉng s·ªë l∆∞·ª£ng"
                )
                logger.info("=" * 70)

                return {
                    "final_file": str(final_file),
                    "load_stats": load_stats,
                }

            finally:
                loader.close()

        except ImportError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ import DataLoader: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load products: {e}", exc_info=True)
            raise

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong load_products task: {e}", exc_info=True)
        raise


def validate_data(**context) -> dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl

    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("=" * 70)

    try:
        ti = context["ti"]
        output_file = None

        # ∆Øu ti√™n: L·∫•y t·ª´ save_products_with_detail (c√≥ detail)
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids="crawl_product_details.save_products_with_detail")
            logger.info(
                f"L·∫•y output_file t·ª´ 'crawl_product_details.save_products_with_detail': {output_file}"
            )
        except Exception as e:
            logger.warning(
                f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'crawl_product_details.save_products_with_detail': {e}"
            )

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                logger.debug(f"Output from save_products_with_detail: {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products_with_detail': {e}")

        # Fallback: L·∫•y t·ª´ save_products (kh√¥ng c√≥ detail) n·∫øu kh√¥ng c√≥ file v·ªõi detail
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="process_and_save.save_products")
                logger.info(
                    f"L·∫•y output_file t·ª´ 'process_and_save.save_products' (fallback): {output_file}"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")

        # C√°ch 3: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products")
                logger.debug(f"Output from save_products (fallback): {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"ƒêang validate file: {output_file}")

        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])
        stats = data.get("stats", {})

        # Validation
        validation_result = {
            "file_exists": True,
            "total_products": len(products),
            "crawled_count": stats.get("crawled_count", 0),  # S·ªë l∆∞·ª£ng products ƒë∆∞·ª£c crawl detail
            "valid_products": 0,
            "invalid_products": 0,
            "errors": [],
        }

        required_fields = ["product_id", "name", "url"]

        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []

            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)

            if is_valid:
                validation_result["valid_products"] += 1
            else:
                validation_result["invalid_products"] += 1
                validation_result["errors"].append(
                    {
                        "index": i,
                        "product_id": product.get("product_id"),
                        "missing_fields": missing_fields,
                    }
                )

        logger.info("=" * 70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng s·ªë products trong file: {validation_result['total_products']}")

        # Log th√¥ng tin v·ªÅ crawl detail n·∫øu c√≥
        crawled_count = stats.get("crawled_count", 0)
        if crawled_count > 0:
            logger.info(f"üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
            logger.info(f"‚úÖ Products c√≥ detail (success): {stats.get('with_detail', 0)}")
            if stats.get("timeout", 0) > 0:
                logger.info(f"‚è±Ô∏è  Products timeout: {stats.get('timeout', 0)}")
            if stats.get("failed", 0) > 0:
                logger.info(f"‚ùå Products failed: {stats.get('failed', 0)}")

        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("=" * 70)

        if validation_result["invalid_products"] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning

        return validation_result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


def aggregate_and_notify(**context) -> dict[str, Any]:
    """
    Task: T·ªïng h·ª£p d·ªØ li·ªáu v·ªõi AI v√† g·ª≠i th√¥ng b√°o qua Discord

    Returns:
        Dict: K·∫øt qu·∫£ t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("ü§ñ TASK: Aggregate Data and Send Discord Notification")
    logger.info("=" * 70)

    result = {
        "aggregation_success": False,
        "ai_summary_success": False,
        "discord_notification_success": False,
        "summary": None,
        "ai_summary": None,
    }

    try:
        # L·∫•y ƒë∆∞·ªùng d·∫´n file products_with_detail.json
        output_file = str(OUTPUT_FILE_WITH_DETAIL)

        if not os.path.exists(output_file):
            logger.warning(f"‚ö†Ô∏è  File kh√¥ng t·ªìn t·∫°i: {output_file}")
            logger.info("   Th·ª≠ l·∫•y t·ª´ XCom...")

            ti = context["ti"]
            try:
                output_file = ti.xcom_pull(
                    task_ids="crawl_product_details.save_products_with_detail"
                )
                logger.info(f"   L·∫•y t·ª´ XCom: {output_file}")
            except Exception:
                try:
                    output_file = ti.xcom_pull(task_ids="save_products_with_detail")
                    logger.info(f"   L·∫•y t·ª´ XCom (kh√¥ng c√≥ prefix): {output_file}")
                except Exception as e:
                    logger.warning(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ XCom: {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"üìä ƒêang t·ªïng h·ª£p d·ªØ li·ªáu t·ª´: {output_file}")

        # 1. T·ªïng h·ª£p d·ªØ li·ªáu
        if DataAggregator is None:
            logger.warning("‚ö†Ô∏è  DataAggregator module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p")
        else:
            try:
                aggregator = DataAggregator(output_file)
                if aggregator.load_data():
                    summary = aggregator.aggregate()
                    result["summary"] = summary
                    result["aggregation_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu th√†nh c√¥ng")

                    # Log th·ªëng k√™
                    stats = summary.get("statistics", {})
                    total_products = stats.get("total_products", 0)
                    crawled_count = stats.get("crawled_count", 0)
                    with_detail = stats.get("with_detail", 0)
                    failed = stats.get("failed", 0)
                    timeout = stats.get("timeout", 0)

                    logger.info(f"   üì¶ T·ªïng s·∫£n ph·∫©m: {total_products}")
                    logger.info(f"   üîÑ Products ƒë∆∞·ª£c crawl detail: {crawled_count}")
                    logger.info(f"   ‚úÖ C√≥ chi ti·∫øt (success): {with_detail}")
                    logger.info(f"   ‚ùå Th·∫•t b·∫°i: {failed}")
                    logger.info(f"   ‚è±Ô∏è  Timeout: {timeout}")

                    # T√≠nh v√† hi·ªÉn th·ªã t·ª∑ l·ªá th√†nh c√¥ng
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        logger.info(
                            f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {with_detail}/{crawled_count} ({success_rate:.1f}%)"
                        )
                    else:
                        logger.warning("   ‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c crawl detail")
                else:
                    logger.error("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu ƒë·ªÉ t·ªïng h·ª£p")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p d·ªØ li·ªáu: {e}", exc_info=True)

        # 2. T·ªïng h·ª£p v·ªõi AI
        if AISummarizer is None:
            logger.warning("‚ö†Ô∏è  AISummarizer module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua t·ªïng h·ª£p AI")
        elif result.get("summary"):
            try:
                summarizer = AISummarizer()
                ai_summary = summarizer.summarize_data(result["summary"])
                if ai_summary:
                    result["ai_summary"] = ai_summary
                    result["ai_summary_success"] = True
                    logger.info("‚úÖ T·ªïng h·ª£p v·ªõi AI th√†nh c√¥ng")
                    logger.info(f"   ƒê·ªô d√†i summary: {len(ai_summary)} k√Ω t·ª±")
                else:
                    logger.warning("‚ö†Ô∏è  Kh√¥ng nh·∫≠n ƒë∆∞·ª£c summary t·ª´ AI")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v·ªõi AI: {e}", exc_info=True)

        # 3. G·ª≠i th√¥ng b√°o qua Discord (r√∫t g·ªçn n·ªôi dung nh∆∞ng gi·ªØ l·∫°i l·ªói chi ti·∫øt)
        if DiscordNotifier is None:
            logger.warning("DiscordNotifier module ch∆∞a ƒë∆∞·ª£c import, b·ªè qua g·ª≠i th√¥ng b√°o")
        else:
            try:
                notifier = DiscordNotifier()

                if result.get("summary"):
                    # L·∫•y stats
                    stats = result["summary"].get("statistics", {})
                    total_products = stats.get("total_products", 0)
                    crawled_count = stats.get("crawled_count", 0)
                    with_detail = stats.get("with_detail", 0)
                    failed = stats.get("failed", 0)
                    timeout = stats.get("timeout", 0)
                    products_saved = stats.get("products_saved", 0)
                    crawled_at = result["summary"].get("metadata", {}).get("crawled_at", "N/A")

                    # T√≠nh m√†u theo success rate
                    if crawled_count > 0:
                        success_rate = (with_detail / crawled_count) * 100
                        color = (
                            0x00B894
                            if success_rate >= 80
                            else (0xF39C12 if success_rate >= 50 else 0xE74C3C)
                        )
                    else:
                        success_rate = 0
                        color = 0x95A5A6

                    # Fields v·ªõi error analysis ƒë·∫ßy ƒë·ªß
                    fields = []
                    fields.append({"name": "Total", "value": f"{total_products:,}", "inline": True})
                    fields.append(
                        {"name": "Crawled", "value": f"{crawled_count:,}", "inline": True}
                    )
                    fields.append(
                        {
                            "name": "Success",
                            "value": f"{with_detail:,} ({success_rate:.1f}%)",
                            "inline": True,
                        }
                    )

                    # Th√™m error analysis chi ti·∫øt
                    if failed > 0 or timeout > 0:
                        total_errors = failed + timeout
                        error_rate = (
                            (total_errors / crawled_count * 100) if crawled_count > 0 else 0
                        )
                        err_info = f"**Total Errors: {total_errors}** ({error_rate:.1f}%)\n"
                        if failed > 0:
                            failed_rate = (failed / crawled_count * 100) if crawled_count > 0 else 0
                            err_info += f"‚Ä¢ Failed: {failed} ({failed_rate:.1f}%)\n"
                        if timeout > 0:
                            timeout_rate = (
                                (timeout / crawled_count * 100) if crawled_count > 0 else 0
                            )
                            err_info += f"‚Ä¢ Timeout: {timeout} ({timeout_rate:.1f}%)"
                        fields.append(
                            {"name": "Error Analysis", "value": err_info.strip(), "inline": False}
                        )

                    if products_saved:
                        fields.append(
                            {"name": "Saved to DB", "value": f"{products_saved:,}", "inline": True}
                        )

                    # N·ªôi dung r√µ r√†ng
                    content = "T·ªïng h·ª£p d·ªØ li·ªáu crawl Tiki.vn\n"
                    if crawled_count > 0:
                        content += f"```\nTh√†nh c√¥ng: {success_rate:.1f}% ({with_detail}/{crawled_count})\n```"
                    else:
                        content += "Ch∆∞a c√≥ s·∫£n ph·∫©m ƒë∆∞·ª£c crawl detail."

                    success = notifier.send_message(
                        content=content,
                        title="T·ªïng h·ª£p d·ªØ li·ªáu Tiki",
                        color=color,
                        fields=fields,
                        footer=f"Crawl l√∫c: {crawled_at}",
                    )
                    if success:
                        result["discord_notification_success"] = True
                        logger.info("ƒê√£ g·ª≠i th√¥ng b√°o Discord")
                    else:
                        logger.warning("Kh√¥ng th·ªÉ g·ª≠i th√¥ng b√°o qua Discord")
                else:
                    logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ g·ª≠i th√¥ng b√°o")
            except Exception as e:
                logger.error(f"L·ªói khi g·ª≠i th√¥ng b√°o Discord: {e}", exc_info=True)

        logger.info("=" * 70)
        logger.info("üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P V√Ä TH√îNG B√ÅO")
        logger.info("=" * 70)
        logger.info(
            f"‚úÖ T·ªïng h·ª£p d·ªØ li·ªáu: {'Th√†nh c√¥ng' if result['aggregation_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ T·ªïng h·ª£p AI: {'Th√†nh c√¥ng' if result['ai_summary_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info(
            f"‚úÖ G·ª≠i Discord: {'Th√†nh c√¥ng' if result['discord_notification_success'] else 'Th·∫•t b·∫°i'}"
        )
        logger.info("=" * 70)

        # Performance Summary
        try:
            dag_run = context.get("dag_run")
            if dag_run and dag_run.start_date:
                start_time = dag_run.start_date
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                total_products = result.get("with_detail", 0)  # Use crawled products count

                # Calculate throughput
                throughput = total_products / duration if duration > 0 else 0
                avg_time = duration / total_products if total_products > 0 else 0

                logger.info("=" * 70)
                logger.info("‚ö° PERFORMANCE SUMMARY")
                logger.info(f"‚è±Ô∏è  Duration: {duration/60:.1f} min | Products: {total_products}")
                if throughput > 0:
                    logger.info(
                        f"üìà Throughput: {throughput:.2f} products/s | Avg: {avg_time:.1f}s/product"
                    )
                logger.info("=" * 70)

                result["performance"] = {
                    "duration_minutes": round(duration / 60, 2),
                    "total_products": total_products,
                    "throughput": round(throughput, 2),
                    "avg_time_per_product": round(avg_time, 2),
                }
        except Exception as perf_error:
            logger.warning(f"‚ö†Ô∏è  Performance summary error: {perf_error}")

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·ªïng h·ª£p v√† g·ª≠i th√¥ng b√°o: {e}", exc_info=True)
        # Kh√¥ng fail task, ch·ªâ log l·ªói
        return result


def cleanup_redis_cache(**context) -> dict[str, Any]:
    """
    Task: Cleanup Redis cache

    Cleanup Redis cache ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ v√† ƒë·∫£m b·∫£o cache kh√¥ng qu√° c≈©.
    Task n√†y ch·∫°y v·ªõi trigger_rule="all_done" ƒë·ªÉ ch·∫°y ngay c·∫£ khi upstream tasks fail.

    Returns:
        Dict: K·∫øt qu·∫£ cleanup
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üßπ TASK: Cleanup Redis Cache")
    logger.info("=" * 70)

    result = {
        "status": "failed",
        "redis_reset": False,
        "stats_before": {},
        "stats_after": {},
    }

    try:
        # Ensure src path in sys.path for package-style imports
        import sys
        from pathlib import Path

        src_path = Path("/opt/airflow/src")
        if src_path.exists() and str(src_path) not in sys.path:
            sys.path.insert(0, str(src_path))

        try:
            from pipelines.crawl.storage.redis_cache import get_redis_cache  # type: ignore
        except Exception as import_err:
            logger.warning(
                f"‚ö†Ô∏è  Import get_redis_cache failed: {import_err} -> trying dynamic import"
            )
            # Dynamic import fallback
            import importlib.util

            rc_path = src_path / "pipelines" / "crawl" / "storage" / "redis_cache.py"
            get_redis_cache = None  # type: ignore
            if rc_path.exists():
                spec = importlib.util.spec_from_file_location("redis_cache_dyn", rc_path)
                if spec and spec.loader:
                    mod = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(mod)  # type: ignore
                    get_redis_cache = getattr(mod, "get_redis_cache", None)  # type: ignore
            if not get_redis_cache:
                raise RuntimeError(
                    "Kh√¥ng th·ªÉ import get_redis_cache (dynamic import c≈©ng th·∫•t b·∫°i)"
                ) from import_err

        # K·∫øt n·ªëi Redis
        redis_cache = get_redis_cache("redis://redis:6379/1")  # type: ignore
        if not redis_cache:
            logger.warning("‚ö†Ô∏è  Kh√¥ng th·ªÉ k·∫øt n·ªëi Redis, skip cleanup")
            result["status"] = "skipped"
            result["reason"] = "Redis not available"
            return result

        # L·∫•y stats tr∆∞·ªõc khi cleanup (d√πng client.info())
        try:
            info_before = redis_cache.client.info()
            db_key = f"db{redis_cache.client.connection_pool.connection_kwargs.get('db', 1)}"
            keys_before = info_before.get(db_key, {}).get("keys", 0)
            hits = info_before.get("keyspace_hits", 0)
            misses = info_before.get("keyspace_misses", 0)
            hit_rate = (hits / (hits + misses) * 100) if (hits + misses) > 0 else 0.0
            stats_before = {
                "keys": keys_before,
                "used_memory_human": info_before.get("used_memory_human"),
                "hit_rate": hit_rate,
                "keyspace_hits": hits,
                "keyspace_misses": misses,
            }
            result["stats_before"] = stats_before
            logger.info("üìä Redis stats tr∆∞·ªõc cleanup:")
            logger.info(f"   - Keys: {keys_before}")
            logger.info(f"   - Memory used: {stats_before.get('used_memory_human', 'N/A')}")
            logger.info(f"   - Hit rate: {hit_rate:.1f}%")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y stats tr∆∞·ªõc cleanup: {e}")

        # Reset cache
        logger.info("üßπ ƒêang cleanup Redis cache...")
        try:
            redis_cache.client.flushdb()
            result["redis_reset"] = True
            logger.info("‚úÖ ƒê√£ flush DB Redis cache th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå Flush DB th·∫•t b·∫°i: {e}")

        # L·∫•y stats sau khi cleanup
        try:
            import time as _t

            _t.sleep(1)
            info_after = redis_cache.client.info()
            db_key = f"db{redis_cache.client.connection_pool.connection_kwargs.get('db', 1)}"
            keys_after = info_after.get(db_key, {}).get("keys", 0)
            hits_a = info_after.get("keyspace_hits", 0)
            misses_a = info_after.get("keyspace_misses", 0)
            hit_rate_a = (hits_a / (hits_a + misses_a) * 100) if (hits_a + misses_a) > 0 else 0.0
            stats_after = {
                "keys": keys_after,
                "used_memory_human": info_after.get("used_memory_human"),
                "hit_rate": hit_rate_a,
                "keyspace_hits": hits_a,
                "keyspace_misses": misses_a,
            }
            result["stats_after"] = stats_after
            logger.info("üìä Redis stats sau cleanup:")
            logger.info(f"   - Keys: {keys_after}")
            logger.info(f"   - Memory used: {stats_after.get('used_memory_human', 'N/A')}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y stats sau cleanup: {e}")

        result["status"] = "success"
        logger.info("‚úÖ Cleanup Redis cache ho√†n t·∫•t")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi cleanup Redis cache: {e}", exc_info=True)
        result["error"] = str(e)

    logger.info("=" * 70)
    return result


def backup_database(**context) -> dict[str, Any]:
    """
    Task: Backup PostgreSQL database

    Backup database crawl_data v√†o th∆∞ m·ª•c backups/postgres sau khi c√°c tasks kh√°c ho√†n th√†nh.

    Returns:
        Dict: K·∫øt qu·∫£ backup
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Backup Database")
    logger.info("=" * 70)

    try:
        import subprocess
        from pathlib import Path

        # ƒê∆∞·ªùng d·∫´n script backup
        script_path = Path("/opt/airflow/scripts/helper/backup_postgres.py")
        if not script_path.exists():
            # Fallback: th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
            script_path = (
                Path(__file__).parent.parent.parent / "scripts" / "helper" / "backup_postgres.py"
            )

        if not script_path.exists():
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y script backup t·∫°i: {script_path}")
            logger.info("üí° S·ª≠ d·ª•ng pg_dump tr·ª±c ti·∫øp...")

            # Fallback: s·ª≠ d·ª•ng pg_dump tr·ª±c ti·∫øp
            container_name = "tiki-data-pipeline-postgres-1"
            # Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n backup
            backup_dirs = [
                Path("/opt/airflow/backups/postgres"),  # Trong container Airflow
                Path("/backups"),  # Mount t·ª´ postgres container
                Path("/opt/airflow/data/backups/postgres"),  # Fallback
            ]
            backup_dir = None
            for bd in backup_dirs:
                try:
                    bd.mkdir(parents=True, exist_ok=True)
                    # Test write
                    test_file = bd / ".test_write"
                    test_file.write_text("test")
                    test_file.unlink()
                    backup_dir = bd
                    break
                except Exception:
                    continue

            if not backup_dir:
                logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c backup c√≥ th·ªÉ ghi, s·ª≠ d·ª•ng /tmp")
                backup_dir = Path("/tmp/backups")
                backup_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = backup_dir / f"crawl_data_{timestamp}.sql"  # ƒê·ªïi .dump -> .sql

            # L·∫•y th√¥ng tin t·ª´ environment variables
            postgres_user = os.getenv("POSTGRES_USER", "airflow_user")
            postgres_password = os.getenv("POSTGRES_PASSWORD", "")

            if not postgres_password:
                logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y POSTGRES_PASSWORD trong environment")
                return {"status": "skipped", "reason": "No password"}

            logger.info("üì¶ ƒêang backup database: crawl_data...")
            logger.info(f"   File: {backup_file}")

            # Ch·∫°y pg_dump trong container - d√πng plain SQL format
            cmd = [
                "docker",
                "exec",
                "-e",
                f"PGPASSWORD={postgres_password}",
                container_name,
                "pg_dump",
                "-U",
                postgres_user,
                "--format=plain",  # Plain SQL format - d·ªÖ restore, t∆∞∆°ng th√≠ch
                "--no-owner",  # Kh√¥ng dump owner info
                "--no-acl",  # Kh√¥ng dump access privileges
                "crawl_data",
            ]

            try:
                with open(backup_file, "wb") as f:
                    result = subprocess.run(
                        cmd,
                        stdout=f,
                        stderr=subprocess.PIPE,
                        check=False,
                        timeout=600,  # 10 ph√∫t timeout
                    )

                if result.returncode == 0:
                    file_size = backup_file.stat().st_size
                    size_mb = file_size / (1024 * 1024)
                    logger.info(f"‚úÖ ƒê√£ backup th√†nh c√¥ng: {backup_file.name}")
                    logger.info(f"   Size: {size_mb:.2f} MB")
                    return {
                        "status": "success",
                        "backup_file": str(backup_file),
                        "size_mb": round(size_mb, 2),
                    }
                else:
                    error_msg = result.stderr.decode("utf-8", errors="ignore")
                    logger.error(f"‚ùå L·ªói khi backup: {error_msg}")
                    if backup_file.exists():
                        backup_file.unlink()
                    return {"status": "failed", "error": error_msg}

            except subprocess.TimeoutExpired:
                logger.error("‚ùå Timeout khi backup database")
                if backup_file.exists():
                    backup_file.unlink()
                return {"status": "failed", "error": "Timeout"}
            except Exception as e:
                logger.error(f"‚ùå Exception khi backup: {e}")
                if backup_file.exists():
                    backup_file.unlink()
                return {"status": "failed", "error": str(e)}
        else:
            # S·ª≠ d·ª•ng script backup (d√πng format sql ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ version dump)
            logger.info(f"üì¶ ƒêang backup database b·∫±ng script: {script_path}")

            cmd = ["python", str(script_path), "--database", "crawl_data", "--format", "sql"]

            try:
                result = subprocess.run(
                    cmd, capture_output=True, text=True, check=False, timeout=600  # 10 ph√∫t timeout
                )

                if result.returncode == 0:
                    logger.info("‚úÖ Backup th√†nh c√¥ng!")
                    logger.info(result.stdout)
                    return {
                        "status": "success",
                        "output": result.stdout,
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è  Backup c√≥ l·ªói (exit code: {result.returncode})")
                    logger.warning(result.stderr)
                    # Kh√¥ng fail task, ch·ªâ log warning
                    return {
                        "status": "warning",
                        "error": result.stderr,
                    }
            except subprocess.TimeoutExpired:
                logger.error("‚ùå Timeout khi backup database")
                return {"status": "failed", "error": "Timeout"}
            except Exception as e:
                logger.error(f"‚ùå Exception khi backup: {e}")
                return {"status": "failed", "error": str(e)}

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong backup_database task: {e}", exc_info=True)
        # Kh√¥ng fail task, ch·ªâ log l·ªói
        return {"status": "failed", "error": str(e)}


# T·∫°o DAG duy nh·∫•t v·ªõi schedule c√≥ th·ªÉ config qua Variable
with DAG(**DAG_CONFIG) as dag:

    # ===== CLEANUP TASKS (RUN FIRST) =====
    # These tasks clean up stale data before crawling starts
    # PREVENTIVE cleanup: Better to clean before crawl than after load
    task_cleanup_products = PythonOperator(
        task_id="cleanup_incomplete_products",
        python_callable=cleanup_incomplete_products_wrapper,
        execution_timeout=timedelta(minutes=5),
        pool="crawl_pool",
    )
    
    task_cleanup_categories = PythonOperator(
        task_id="cleanup_orphan_categories",
        python_callable=cleanup_orphan_categories_wrapper,
        execution_timeout=timedelta(minutes=5),
        pool="crawl_pool",
    )

    # TaskGroup: Load v√† Prepare
    with TaskGroup("load_and_prepare") as load_group:
        # (ƒê√É B·ªé) extract_and_load_categories_to_db ƒë·ªÉ gi·∫£m th·ªùi gian pipeline.
        # Task: Load danh s√°ch categories t·ª´ file ƒë·ªÉ crawl
        task_load_categories = PythonOperator(
            task_id="load_categories",
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="crawl_pool",
        )

    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup("crawl_categories") as crawl_group:
        # S·ª≠ d·ª•ng expand ƒë·ªÉ Dynamic Task Mapping
        # C·∫ßn m·ªôt task helper ƒë·ªÉ l·∫•y categories v√† t·∫°o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y categories t·ª´ XCom
            categories = None

            # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

            # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids="load_categories")
                    logger.info(
                        f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                    )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

            # C√°ch 3: Th·ª≠ l·∫•y t·ª´ upstream task (Airflow 3.x compatible)
            if not categories:
                try:
                    # Airflow 3.x: D√πng dag_run.get_task_instance() thay v√¨ TaskInstance constructor
                    dag_run = context["dag_run"]
                    # Th·ª≠ l·∫•y t·ª´ task trong TaskGroup
                    upstream_ti = dag_run.get_task_instance(
                        task_id="load_and_prepare.load_categories"
                    )
                    if upstream_ti:
                        categories = upstream_ti.xcom_pull(key="return_value")
                        logger.info(
                            f"L·∫•y categories t·ª´ dag_run.get_task_instance(): {len(categories) if categories else 0} items"
                        )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ dag_run.get_task_instance(): {e}")

            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []

            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []

            logger.info(
                f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories, t·∫°o {len(categories)} tasks cho Dynamic Task Mapping"
            )

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            return [{"category": cat} for cat in categories]

        def prepare_category_batch_kwargs(**context):
            """
            Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping v·ªõi batch processing

            Chia categories th√†nh batches ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng Airflow tasks v√† t·∫≠n d·ª•ng driver pooling
            """
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # L·∫•y categories t·ª´ XCom (same logic as prepare_crawl_kwargs)
            categories = None

            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids="load_categories")
                    logger.info(
                        f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                    )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

            if not categories:
                try:
                    # Airflow 3.x: D√πng dag_run.get_task_instance() thay v√¨ TaskInstance constructor
                    dag_run = context["dag_run"]
                    # Th·ª≠ l·∫•y t·ª´ task trong TaskGroup
                    upstream_ti = dag_run.get_task_instance(
                        task_id="load_and_prepare.load_categories"
                    )
                    if upstream_ti:
                        categories = upstream_ti.xcom_pull(key="return_value")
                        logger.info(
                            f"L·∫•y categories t·ª´ dag_run.get_task_instance(): {len(categories) if categories else 0} items"
                        )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ dag_run.get_task_instance(): {e}")

            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []

            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []

            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories")

            # Batch Processing: Chia categories th√†nh batches
            batch_size = int(get_variable("TIKI_CATEGORY_BATCH_SIZE", default="10"))
            batches = []
            for i in range(0, len(categories), batch_size):
                batch = categories[i : i + batch_size]
                batches.append(batch)

            logger.info(
                f"üì¶ ƒê√£ chia th√†nh {len(batches)} batches (m·ªói batch {batch_size} categories)"
            )
            logger.info(f"   - Batch ƒë·∫ßu ti√™n: {len(batches[0]) if batches else 0} categories")
            logger.info(f"   - Batch cu·ªëi c√πng: {len(batches[-1]) if batches else 0} categories")

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand (m·ªói dict l√† 1 batch)
            op_kwargs_list = [
                {"category_batch": batch, "batch_index": idx} for idx, batch in enumerate(batches)
            ]

            logger.info(
                f"üî¢ T·∫°o {len(op_kwargs_list)} op_kwargs cho Dynamic Task Mapping (batches)"
            )
            if op_kwargs_list:
                logger.info("üìã Sample batches (first 2):")
                for _i, kwargs in enumerate(op_kwargs_list[:2]):
                    batch = kwargs.get("category_batch", [])
                    batch_idx = kwargs.get("batch_index", -1)
                    category_names = [c.get("name", "Unknown") for c in batch[:3]]
                    logger.info(
                        f"  Batch {batch_idx}: {len(batch)} categories - {category_names}..."
                    )

            return op_kwargs_list

        # Ch·ªâ t·∫°o task prepare ph√π h·ª£p v·ªõi mode ƒëang d√πng (batch ho·∫∑c single-category)
        # ƒê·ªÉ tr√°nh task b·ªã skip v√¨ ch·ªâ 1 trong 2 task prepare ƒë∆∞·ª£c d√πng
        if crawl_category_batch is not None:
            # Batch processing mode (PRODUCTION: Khuy·∫øn ngh·ªã d√πng mode n√†y)
            task_prepare = PythonOperator(
                task_id="prepare_batch_kwargs",
                python_callable=prepare_category_batch_kwargs,
                execution_timeout=timedelta(minutes=1),
            )

            task_crawl_category = PythonOperator.partial(
                task_id="crawl_category",
                python_callable=crawl_category_batch,
                execution_timeout=timedelta(minutes=12),  # T·ªëi ∆∞u: gi·∫£m t·ª´ 15 -> 12 ph√∫t
                pool="crawl_pool",
                retries=1,  # Retry 1 l·∫ßn
                retry_delay=timedelta(seconds=15),  # T·ªëi ∆∞u: th√™m delay ng·∫Øn
            ).expand(op_kwargs=task_prepare.output)
        else:
            # Fallback: single-category processing
            task_prepare = PythonOperator(
                task_id="prepare_crawl_kwargs",
                python_callable=prepare_crawl_kwargs,
                execution_timeout=timedelta(minutes=1),
            )

            task_crawl_category = PythonOperator.partial(
                task_id="crawl_category",
                python_callable=crawl_single_category,
                execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t m·ªói category
                pool="crawl_pool",
                retries=1,
            ).expand(op_kwargs=task_prepare.output)

    # TaskGroup: Process v√† Save
    with TaskGroup("process_and_save") as process_group:
        task_merge_products = PythonOperator(
            task_id="merge_products",
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="crawl_pool",
            trigger_rule="all_done",  # QUAN TR·ªåNG: Ch·∫°y khi t·∫•t c·∫£ upstream tasks done (success ho·∫∑c failed)
        )

        task_save_products = PythonOperator(
            task_id="save_products",
            python_callable=save_products,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="crawl_pool",
        )

    # TaskGroup: Crawl Product Details (Dynamic Task Mapping)
    with TaskGroup("crawl_product_details") as detail_group:

        def prepare_detail_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping detail"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # L·∫•y products t·ª´ prepare_products_for_detail
            # Task n√†y n·∫±m trong TaskGroup 'crawl_product_details', n√™n task_id ƒë·∫ßy ƒë·ªß l√† 'crawl_product_details.prepare_products_for_detail'
            products_to_crawl = None

            # L·∫•y t·ª´ upstream task (prepare_products_for_detail) - c√°ch ƒë√°ng tin c·∫≠y nh·∫•t
            # Th·ª≠ l·∫•y upstream_task_ids t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau (t∆∞∆°ng th√≠ch v·ªõi c√°c phi√™n b·∫£n Airflow)
            upstream_task_ids = []
            try:
                task_instance = context.get("task_instance")
                if task_instance:
                    # Th·ª≠ v·ªõi RuntimeTaskInstance (Airflow SDK m·ªõi)
                    if hasattr(task_instance, "upstream_task_ids"):
                        upstream_task_ids = list(task_instance.upstream_task_ids)
                    # Th·ª≠ v·ªõi ti.task (c√°ch kh√°c)
                    elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                        upstream_task_ids = list(ti.task.upstream_task_ids)
            except (AttributeError, TypeError) as e:
                logger.debug(f"   Kh√¥ng th·ªÉ l·∫•y upstream_task_ids: {e}")

            if upstream_task_ids:
                logger.info(f"üîç Upstream tasks: {upstream_task_ids}")
                # Th·ª≠ l·∫•y t·ª´ t·∫•t c·∫£ upstream tasks
                for task_id in upstream_task_ids:
                    try:
                        products_to_crawl = ti.xcom_pull(task_ids=task_id)
                        if products_to_crawl:
                            logger.debug(f"XCom from upstream: {task_id}")
                            break
                    except Exception as e:
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ {task_id}: {e}")
                        continue

            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ c√°c c√°ch kh√°c
            if not products_to_crawl:
                try:
                    # Th·ª≠ v·ªõi task_id ƒë·∫ßy ƒë·ªß (c√≥ TaskGroup prefix)
                    products_to_crawl = ti.xcom_pull(
                        task_ids="crawl_product_details.prepare_products_for_detail"
                    )
                    logger.info(
                        "‚úÖ L·∫•y XCom t·ª´ task_id: crawl_product_details.prepare_products_for_detail"
                    )
                except Exception as e1:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng l·∫•y ƒë∆∞·ª£c v·ªõi task_id ƒë·∫ßy ƒë·ªß: {e1}")
                    try:
                        # Th·ª≠ v·ªõi task_id kh√¥ng c√≥ prefix (fallback)
                        products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
                        logger.debug("XCom from prepare_products_for_detail")
                    except Exception as e2:
                        logger.error(f"‚ùå Kh√¥ng th·ªÉ l·∫•y XCom v·ªõi c·∫£ 2 c√°ch: {e1}, {e2}")

            if not products_to_crawl:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y products t·ª´ XCom!")
                try:
                    task_instance = context.get("task_instance")
                    upstream_info = []
                    if task_instance:
                        if hasattr(task_instance, "upstream_task_ids"):
                            upstream_info = list(task_instance.upstream_task_ids)
                        elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                            upstream_info = list(ti.task.upstream_task_ids)
                    logger.error(f"   Upstream tasks: {upstream_info}")
                except Exception as e:
                    logger.error(f"   Kh√¥ng th·ªÉ l·∫•y th√¥ng tin upstream tasks: {e}")
                return []

            if not isinstance(products_to_crawl, list):
                logger.error(f"‚ùå Products kh√¥ng ph·∫£i list: {type(products_to_crawl)}")
                logger.error(f"   Value: {products_to_crawl}")
                return []

            logger.info(f"‚úÖ Retrieved {len(products_to_crawl)} products for detail crawl")

            # Dynamic Batch Sizing: T√≠nh optimal batch size d·ª±a tr√™n s·ªë l∆∞·ª£ng products v√† available workers
            def calculate_optimal_batch_size(
                product_count: int,
                available_workers: int = 12,  # Default Airflow worker concurrency
                target_batch_time: int = 45,  # Target: m·ªói batch x·ª≠ l√Ω trong 45-60 gi√¢y
                min_batch: int = 5,
                max_batch: int = 20,
            ) -> int:
                """T√≠nh optimal batch size d·ª±a tr√™n context"""
                # ∆Ø·ªõc t√≠nh: m·ªói product m·∫•t ~3-5s ƒë·ªÉ crawl
                avg_product_time = 4.0  # seconds

                # T√≠nh products per batch ƒë·ªÉ ƒë·∫°t target time
                products_per_batch = target_batch_time / avg_product_time

                # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n s·ªë l∆∞·ª£ng products v√† workers
                # N·∫øu c√≥ nhi·ªÅu products, c√≥ th·ªÉ tƒÉng batch size
                if product_count > 1000:
                    products_per_batch *= 1.2
                elif product_count < 100:
                    products_per_batch *= 0.8

                # ƒê·∫£m b·∫£o trong kho·∫£ng min-max
                optimal = max(min_batch, min(max_batch, int(products_per_batch)))

                logger.info(
                    f"üî¢ Dynamic batch sizing: {product_count} products, "
                    f"{available_workers} workers ‚Üí optimal batch size: {optimal}"
                )
                return optimal

            # L·∫•y batch size t·ª´ config ho·∫∑c t√≠nh dynamic
            try:
                from pipelines.crawl.config import PRODUCT_BATCH_SIZE

                base_batch_size = PRODUCT_BATCH_SIZE
            except Exception:
                base_batch_size = 12  # Default fallback

            # T√≠nh optimal batch size
            optimal_batch_size = calculate_optimal_batch_size(
                len(products_to_crawl),
                available_workers=12,  # C√≥ th·ªÉ l·∫•y t·ª´ Airflow Variable n·∫øu c·∫ßn
            )

            # D√πng gi√° tr·ªã l·ªõn h∆°n gi·ªØa config v√† optimal ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªáu qu·∫£
            batch_size = max(base_batch_size, optimal_batch_size)

            # Batch Processing: Chia products th√†nh batches
            batches = []
            for i in range(0, len(products_to_crawl), batch_size):
                batch = products_to_crawl[i : i + batch_size]
                batches.append(batch)

            logger.info(
                f"üì¶ ƒê√£ chia th√†nh {len(batches)} batches (m·ªói batch {batch_size} products)"
            )
            logger.info(f"   - Batch ƒë·∫ßu ti√™n: {len(batches[0]) if batches else 0} products")
            logger.info(f"   - Batch cu·ªëi c√πng: {len(batches[-1]) if batches else 0} products")

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand (m·ªói dict l√† 1 batch)
            op_kwargs_list = [
                {"product_batch": batch, "batch_index": idx} for idx, batch in enumerate(batches)
            ]

            logger.info(f"üî¢ Created {len(op_kwargs_list)} batches for Dynamic Task Mapping")

            return op_kwargs_list

        task_prepare_detail = PythonOperator(
            task_id="prepare_products_for_detail",
            python_callable=prepare_products_for_detail,
            execution_timeout=timedelta(minutes=5),
        )

        task_prepare_detail_kwargs = PythonOperator(
            task_id="prepare_detail_kwargs",
            python_callable=prepare_detail_kwargs,
            execution_timeout=timedelta(minutes=1),
        )

        # Dynamic Task Mapping cho crawl detail (Batch Processing)
        task_crawl_product_detail = PythonOperator.partial(
            task_id="crawl_product_detail",
            python_callable=crawl_product_batch,  # D√πng batch function thay v√¨ single
            execution_timeout=timedelta(minutes=15),  # T·ªëi ∆∞u: gi·∫£m t·ª´ 20 -> 15 ph√∫t (nhanh h∆°n)
            pool="crawl_pool",
            retries=1,  # T·ªëi ∆∞u: gi·∫£m t·ª´ 2 -> 1 (nhanh h∆°n)
            retry_delay=timedelta(seconds=30),  # T·ªëi ∆∞u: gi·∫£m t·ª´ 2 ph√∫t -> 30 gi√¢y
        ).expand(op_kwargs=task_prepare_detail_kwargs.output)

        task_merge_product_details = PythonOperator(
            task_id="merge_product_details",
            python_callable=merge_product_details,
            execution_timeout=timedelta(minutes=30),  # T·ªëi ∆∞u: gi·∫£m t·ª´ 60 -> 30 ph√∫t
            pool="crawl_pool",
            trigger_rule="all_done",  # Ch·∫°y khi t·∫•t c·∫£ upstream tasks done
            # TƒÉng heartbeat interval ƒë·ªÉ tr√°nh timeout khi x·ª≠ l√Ω nhi·ªÅu d·ªØ li·ªáu
        )

        task_save_products_with_detail = PythonOperator(
            task_id="save_products_with_detail",
            python_callable=save_products_with_detail,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="crawl_pool",
        )

        # Dependencies trong detail group
        (
            task_prepare_detail
            >> task_prepare_detail_kwargs
            >> task_crawl_product_detail
            >> task_merge_product_details
            >> task_save_products_with_detail
        )

    # TaskGroup: Enrich Category Path (th√™m category_path cho products thi·∫øu, d·ª±a v√†o categories file)
    with TaskGroup("enrich_category_path") as enrich_group:

        def enrich_category_path_task(**context):
            """B·ªï sung category_path cho products c√≥ category_id nh∆∞ng ch∆∞a c√≥ breadcrumb.

            Lu·ªìng:
            1. ƒê·ªçc file s·∫£n ph·∫©m chi ti·∫øt (products_with_detail.json)
            2. ƒê·ªçc categories t·ª´ file (categories_recursive_optimized.json) - ƒë√£ c√≥ category_id v√† category_path
            3. X√¢y d·ª±ng lookup map: category_id -> category_path
            4. V·ªõi m·ªói product: n·∫øu c√≥ category_id nh∆∞ng ch∆∞a c√≥ category_path, t√¨m t·ª´ lookup map
            5. Ghi l·∫°i file
            """
            logger = get_logger(context)
            logger.info("=" * 70)
            logger.info("üß© TASK: Enrich Category Path")
            logger.info("=" * 70)

            ti = context["ti"]

            # B∆∞·ªõc 1: L·∫•y file s·∫£n ph·∫©m chi ti·∫øt
            output_file = None
            try:
                output_file = ti.xcom_pull(
                    task_ids="crawl_product_details.save_products_with_detail"
                )
            except Exception:
                pass
            if not output_file:
                output_file = str(OUTPUT_FILE_WITH_DETAIL)

            if not os.path.exists(output_file):
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file detail: {output_file}")

            logger.info(f"üìÇ ƒêang ƒë·ªçc file detail: {output_file}")
            with open(output_file, encoding="utf-8") as f:
                data = json.load(f)
            products = data.get("products", [])
            logger.info(f"üìä S·ªë products tr∆∞·ªõc enrich: {len(products)}")

            # B∆∞·ªõc 2: ƒê·ªçc categories t·ª´ file
            category_path_lookup: dict[str, list] = {}  # category_id -> category_path

            if CATEGORIES_FILE.exists():
                try:
                    logger.info(f"üìñ ƒê·ªçc categories t·ª´ file: {CATEGORIES_FILE}")
                    with open(CATEGORIES_FILE, encoding="utf-8") as cf:
                        raw_categories = json.load(cf)

                    for cat in raw_categories:
                        cat_id = cat.get("category_id")
                        cat_path = cat.get("category_path")

                        # Ch·ªâ th√™m v√†o lookup n·∫øu c√≥ category_id v√† category_path
                        if cat_id and cat_path:
                            category_path_lookup[cat_id] = cat_path

                    logger.info(f"‚úÖ Loaded {len(category_path_lookup)} category_path t·ª´ file")
                except Exception as fe:
                    logger.warning(f"‚ö†Ô∏è L·ªói ƒë·ªçc file categories: {fe}")
            else:
                logger.warning(f"‚ö†Ô∏è File categories kh√¥ng t·ªìn t·∫°i: {CATEGORIES_FILE}")

            # Debug logging for lookup
            if DEBUG_ENRICH_CATEGORY_PATH and category_path_lookup:
                sample_keys = list(category_path_lookup.keys())[:5]
                logger.debug(f"üîç [DEBUG] Sample category_path_lookup keys: {sample_keys}")

                sample_product_ids = [
                    p.get("category_id") for p in products[:5] if p.get("category_id")
                ]
                logger.debug(f"üîç [DEBUG] Sample product category_ids: {sample_product_ids}")

            # B∆∞·ªõc 3: Enrich category_path cho products
            enriched = 0
            without_category_id = 0
            debug_missing_ids = set()

            for p in products:
                # N·∫øu product c√≥ category_id nh∆∞ng ch∆∞a c√≥ category_path
                if p.get("category_id") and not p.get("category_path"):
                    cat_id = str(p["category_id"]).strip()

                    # T√¨m category_path t·ª´ lookup map
                    if cat_id in category_path_lookup:
                        p["category_path"] = category_path_lookup[cat_id]
                        enriched += 1
                    else:
                        # Log: category_id kh√¥ng t√¨m th·∫•y trong file categories
                        without_category_id += 1
                        if DEBUG_ENRICH_CATEGORY_PATH and len(debug_missing_ids) < 10:
                            debug_missing_ids.add(cat_id)

            if DEBUG_ENRICH_CATEGORY_PATH and debug_missing_ids:
                logger.debug(f"‚ö†Ô∏è [DEBUG] Sample missing IDs in lookup: {list(debug_missing_ids)}")

            # B∆∞·ªõc 4: Report
            if enriched > 0:
                logger.info(f"‚úÖ Enriched category_path cho {enriched} products")
            if without_category_id > 0:
                logger.warning(
                    f"‚ö†Ô∏è {without_category_id} products c√≥ category_id nh∆∞ng kh√¥ng t√¨m th·∫•y trong categories"
                )

            # Check: S·ªë products c√≥ category_path
            products_with_path = sum(1 for p in products if p.get("category_path"))
            logger.info(f"üìä T·ªïng products c√≥ category_path: {products_with_path}/{len(products)}")

            # B∆∞·ªõc 5: Ghi l·∫°i file (in-place update)
            data["products"] = products
            atomic_write_file(output_file, data, **context)
            logger.info(f"üíæ ƒê√£ c·∫≠p nh·∫≠t file v·ªõi category_path enrich: {output_file}")

            return {
                "file": output_file,
                "enriched_count": enriched,
                "products_with_path": products_with_path,
            }

        task_enrich_category_path = PythonOperator(
            task_id="enrich_products_category_path",
            python_callable=enrich_category_path_task,
            execution_timeout=timedelta(minutes=10),
            pool="crawl_pool",
        )

    # TaskGroup: Transform and Load
    with TaskGroup("transform_and_load") as transform_load_group:
        task_transform_products = PythonOperator(
            task_id="transform_products",
            python_callable=transform_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="crawl_pool",
        )

        task_load_products = PythonOperator(
            task_id="load_products",
            python_callable=load_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="crawl_pool",
        )

        # Dependencies trong transform_load group
        task_transform_products >> task_load_products

    # TaskGroup: Validate
    with TaskGroup("validate") as validate_group:
        task_validate_data = PythonOperator(
            task_id="validate_data",
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="crawl_pool",
        )

    # TaskGroup: Aggregate and Notify
    with TaskGroup("aggregate_and_notify") as aggregate_group:
        task_aggregate_and_notify = PythonOperator(
            task_id="aggregate_and_notify",
            python_callable=aggregate_and_notify,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="crawl_pool",
            trigger_rule="all_done",  # Ch·∫°y ngay c·∫£ khi c√≥ task upstream fail
        )

        # Cleanup Cache task (no TaskGroup to allow direct reference)
        task_cleanup_cache = PythonOperator(
            task_id="cleanup_redis_cache",
            python_callable=cleanup_redis_cache,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="crawl_pool",
            trigger_rule="all_done",  # Ch·∫°y ngay c·∫£ khi c√≥ task upstream fail
        )

        task_backup_database = PythonOperator(
            task_id="backup_database",
            python_callable=backup_database,
            execution_timeout=timedelta(minutes=15),
            pool="crawl_pool",
            trigger_rule="all_done",
        )

    # Task: Load categories to DB (runs after products are loaded)
    task_load_categories_db = PythonOperator(
        task_id="load_categories_to_db",
        python_callable=load_categories_to_db_wrapper,
        execution_timeout=timedelta(minutes=5),
        pool="crawl_pool",
    )

    # ===== OPTIMIZED DEPENDENCIES =====
    # T·ªëi ∆∞u: Ch·∫°y song song c√°c tasks kh√¥ng ph·ª• thu·ªôc nhau ƒë·ªÉ gi·∫£m th·ªùi gian execution
    
    # Step 1: Cleanup incomplete products before crawling
    # cleanup_categories s·∫Ω ch·∫°y SAU khi load categories v√†o DB (Step 10)
    # Ch·ªâ cleanup products ·ªü ƒë√¢y
    task_cleanup_products >> task_load_categories
    
    # Step 2: Load categories and prepare for crawl
    task_load_categories >> task_prepare

    # Step 3: Prepare crawl kwargs -> crawl category (dynamic mapping)
    task_prepare >> task_crawl_category

    # Step 4: Crawl category -> merge products (merge ch·∫°y khi t·∫•t c·∫£ crawl tasks done)
    task_crawl_category >> task_merge_products

    # Step 5: Merge -> save products
    task_merge_products >> task_save_products

    # Step 6: Save products -> prepare detail -> crawl detail -> merge detail -> save detail
    task_save_products >> task_prepare_detail
    # Dependencies trong detail group ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü d√≤ng 5471-5477
    
    # Step 7: After save_products_with_detail
    # enrich_category_path n√™n ch·∫°y tr∆∞·ªõc transform ƒë·ªÉ transform c√≥ th·ªÉ d√πng category_path ƒë√£ ƒë∆∞·ª£c enrich
    task_save_products_with_detail >> task_enrich_category_path >> task_transform_products
    
    # Step 8: Transform -> Load
    task_transform_products >> task_load_products
    
    # Step 9: After load_products - parallel processing
    # load_categories_db c√≥ th·ªÉ ch·∫°y ngay sau load_products (kh√¥ng c·∫ßn wait enrich)
    # enrich_category_path c≈©ng c√≥ th·ªÉ trigger load_categories_db n·∫øu c·∫ßn
    task_load_products >> task_load_categories_db
    
    # Step 10: Cleanup orphan categories AFTER loading categories to DB
    task_load_categories_db >> task_cleanup_categories
    
    # Step 11: Validate data (c√≥ th·ªÉ ch·∫°y song song v·ªõi cleanup_categories n·∫øu kh√¥ng ph·ª• thu·ªôc)
    # Nh∆∞ng ƒë·ªÉ ƒë·∫£m b·∫£o data consistency, validate sau cleanup
    task_cleanup_categories >> task_validate_data
    
    # Step 12: Aggregate and notify
    task_validate_data >> task_aggregate_and_notify
    
    # Step 13: Final tasks - ch·∫°y song song (cleanup_cache v√† backup_database kh√¥ng ph·ª• thu·ªôc nhau)
    # C·∫£ hai ƒë·ªÅu ph·ª• thu·ªôc v√†o aggregate_and_notify nh∆∞ng kh√¥ng c·∫ßn nhau
    task_aggregate_and_notify >> [task_cleanup_cache, task_backup_database]
