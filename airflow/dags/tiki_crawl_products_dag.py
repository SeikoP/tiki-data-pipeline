"""
DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching
"""
import os
import sys
import json
import time
import hashlib
import tempfile
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk import TaskGroup
from airflow.models import Variable
from airflow.configuration import conf
from airflow.utils.session import provide_session

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o sys.path
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa DAG file
dag_file_dir = os.path.dirname(os.path.abspath(__file__))

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
# Trong Docker, src ƒë∆∞·ª£c mount v√†o /opt/airflow/src
possible_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    '/opt/airflow/src/pipelines/crawl',
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, '..', '..', 'src', 'pipelines', 'crawl')),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, '..', 'src', 'pipelines', 'crawl')),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    '/workspace/src/pipelines/crawl',
    # T·ª´ current working directory
    os.path.join(os.getcwd(), 'src', 'pipelines', 'crawl'),
]

# T√¨m ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá
crawl_module_path = None
crawl_products_path = None

for path in possible_paths:
    test_path = os.path.join(path, 'crawl_products.py')
    if os.path.exists(test_path):
        crawl_module_path = path
        crawl_products_path = test_path
        break

if not crawl_module_path:
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ DAG file
    relative_path = os.path.abspath(os.path.join(dag_file_dir, '..', '..', 'src', 'pipelines', 'crawl'))
    test_path = os.path.join(relative_path, 'crawl_products.py')
    if os.path.exists(test_path):
        crawl_module_path = relative_path
        crawl_products_path = test_path

# Import module crawl_products
if crawl_products_path and os.path.exists(crawl_products_path):
    # S·ª≠ d·ª•ng importlib ƒë·ªÉ import tr·ª±c ti·∫øp t·ª´ file (c√°ch ƒë√°ng tin c·∫≠y nh·∫•t)
    import importlib.util
    spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_path}")
    crawl_products_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(crawl_products_module)
    
    # Extract c√°c functions c·∫ßn thi·∫øt
    crawl_category_products = crawl_products_module.crawl_category_products
    get_page_with_requests = crawl_products_module.get_page_with_requests
    parse_products_from_html = crawl_products_module.parse_products_from_html
    get_total_pages = crawl_products_module.get_total_pages
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng n·∫øu ƒë√£ th√™m v√†o sys.path
    if crawl_module_path and crawl_module_path not in sys.path:
        sys.path.insert(0, crawl_module_path)
    
    try:
        from crawl_products import (
            crawl_category_products,
            get_page_with_requests,
            parse_products_from_html,
            get_total_pages
        )
    except ImportError as e:
        # Debug: ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        debug_info = {
            'dag_file_dir': dag_file_dir,
            'cwd': os.getcwd(),
            'possible_paths': possible_paths,
            'crawl_module_path': crawl_module_path,
            'crawl_products_path': crawl_products_path,
            'sys_path': sys.path[:5]  # Ch·ªâ l·∫•y 5 ƒë·∫ßu ti√™n
        }
        
        # Ki·ªÉm tra xem /opt/airflow/src c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists('/opt/airflow/src'):
            try:
                debug_info['opt_airflow_src_contents'] = os.listdir('/opt/airflow/src')
            except:
                pass
        
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products.\n"
            f"Debug info: {debug_info}\n"
            f"L·ªói g·ªëc: {e}"
        )

# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, 'crawl_products_detail.py')
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    import importlib.util
    spec = importlib.util.spec_from_file_location("crawl_products_detail", crawl_products_detail_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_detail_path}")
    crawl_products_detail_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(crawl_products_detail_module)
    
    # Extract c√°c functions c·∫ßn thi·∫øt
    crawl_product_detail_with_selenium = crawl_products_detail_module.crawl_product_detail_with_selenium
    extract_product_detail = crawl_products_detail_module.extract_product_detail
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng
    try:
        from crawl_products_detail import (
            crawl_product_detail_with_selenium,
            extract_product_detail
        )
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"L·ªói g·ªëc: {e}"
        )

# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,  # Retry 3 l·∫ßn
    'retry_delay': timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    'retry_exponential_backoff': True,  # Exponential backoff
    'max_retry_delay': timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG
DAG_CONFIG = {
    'dag_id': 'tiki_crawl_products',
    'description': 'Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a',
    'default_args': DEFAULT_ARGS,
    'schedule': timedelta(days=1),  # Ch·∫°y h√†ng ng√†y
    'start_date': datetime(2024, 1, 1),
    'catchup': False,
    'tags': ['tiki', 'crawl', 'products', 'data-pipeline'],
    'max_active_runs': 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    'max_active_tasks': 20,  # T·ªëi ƒëa 20 tasks song song
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path('/opt/airflow/data'),  # Docker mount
    Path(__file__).parent.parent.parent / 'data',  # Local development
    Path(os.getcwd()) / 'data',  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    DATA_DIR = Path(__file__).parent.parent.parent / 'data'

CATEGORIES_FILE = DATA_DIR / 'raw' / 'categories_recursive_optimized.json'
OUTPUT_DIR = DATA_DIR / 'raw' / 'products'
CACHE_DIR = OUTPUT_DIR / 'cache'
DETAIL_CACHE_DIR = OUTPUT_DIR / 'detail' / 'cache'
OUTPUT_FILE = OUTPUT_DIR / 'products.json'
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / 'products_with_detail.json'

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()


def get_logger(context):
    """L·∫•y logger t·ª´ context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging
        ti = context.get('task_instance')
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging
        return logging.getLogger("airflow.task")


def load_categories(**context) -> List[Dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file
    
    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üìñ TASK: Load Categories")
    logger.info("="*70)
    
    try:
        categories_file = str(CATEGORIES_FILE)
        logger.info(f"ƒêang ƒë·ªçc file: {categories_file}")
        
        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")
        
        with open(categories_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        
        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")
        
        # L·ªçc danh m·ª•c n·∫øu c·∫ßn (v√≠ d·ª•: ch·ªâ l·∫•y level 2-4)
        # C√≥ th·ªÉ c·∫•u h√¨nh qua Airflow Variable
        try:
            min_level = int(Variable.get('TIKI_MIN_CATEGORY_LEVEL', default_var='2'))
            max_level = int(Variable.get('TIKI_MAX_CATEGORY_LEVEL', default_var='4'))
            categories = [
                cat for cat in categories 
                if min_level <= cat.get('level', 0) <= max_level
            ]
            logger.info(f"‚úì Sau khi l·ªçc level {min_level}-{max_level}: {len(categories)} danh m·ª•c")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l·ªçc theo level: {e}")
        
        # Gi·ªõi h·∫°n s·ªë danh m·ª•c n·∫øu c·∫ßn (ƒë·ªÉ test)
        try:
            max_categories = int(Variable.get('TIKI_MAX_CATEGORIES', default_var='0'))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"‚úì Gi·ªõi h·∫°n: {max_categories} danh m·ª•c")
        except:
            pass
        
        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: Dict[str, Any] = None, **context) -> Dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)
    
    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl
    
    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context
        
    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)
    
    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get('ti')
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, 'op_kwargs', {})
            if op_kwargs:
                category = op_kwargs.get('category')
        
        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get('category') or context.get('op_kwargs', {}).get('category')
    
    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get('ti')
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")
    
    category_url = category.get('url', '')
    category_name = category.get('name', 'Unknown')
    category_id = category.get('id', '')
    
    logger.info("="*70)
    logger.info(f"üõçÔ∏è  TASK: Crawl Category - {category_name}")
    logger.info(f"üîó URL: {category_url}")
    logger.info("="*70)
    
    result = {
        'category_id': category_id,
        'category_name': category_name,
        'category_url': category_url,
        'products': [],
        'status': 'failed',
        'error': None,
        'crawled_at': datetime.now().isoformat(),
        'pages_crawled': 0,
        'products_count': 0
    }
    
    try:
        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(Variable.get('TIKI_MAX_PAGES_PER_CATEGORY', default_var='20'))  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = Variable.get('TIKI_USE_SELENIUM', default_var='false').lower() == 'true'
        timeout = int(Variable.get('TIKI_CRAWL_TIMEOUT', default_var='300'))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(Variable.get('TIKI_RATE_LIMIT_DELAY', default_var='1.0'))  # Delay 1s gi·ªØa c√°c request
        
        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)
        
        # Crawl v·ªõi timeout
        start_time = time.time()
        
        products = crawl_category_products(
            category_url,
            max_pages=max_pages if max_pages > 0 else None,
            use_selenium=use_selenium,
            cache_dir=str(CACHE_DIR)
        )
        
        elapsed = time.time() - start_time
        
        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")
        
        result['products'] = products
        result['status'] = 'success'
        result['products_count'] = len(products)
        result['elapsed_time'] = elapsed
        
        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")
        
    except TimeoutError as e:
        result['error'] = str(e)
        result['status'] = 'timeout'
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
        
    except Exception as e:
        result['error'] = str(e)
        result['status'] = 'failed'
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
    
    return result


def merge_products(**context) -> Dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c
    
    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üîÑ TASK: Merge Products")
    logger.info("="*70)
    
    try:
        from airflow.models import TaskInstance
        from airflow.models.dagrun import DagRun
        
        ti = context['ti']
        dag_run = context['dag_run']
        
        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids='load_and_prepare.load_categories')
            logger.info(f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids='load_categories')
                logger.info(f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")
        
        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")
        
        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")
        
        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            'total_categories': len(categories),
            'success_categories': 0,
            'failed_categories': 0,
            'timeout_categories': 0,
            'total_products': 0,
            'unique_products': 0
        }
        
        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = 'crawl_categories.crawl_category'
        
        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(
                task_ids=task_id,
                key='return_value'
            )
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                for result in all_results:
                    if result and isinstance(result, dict):
                        if result.get('status') == 'success':
                            stats['success_categories'] += 1
                            products = result.get('products', [])
                            all_products.extend(products)
                            stats['total_products'] += len(products)
                        elif result.get('status') == 'timeout':
                            stats['timeout_categories'] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats['failed_categories'] += 1
                            logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get('status') == 'success':
                            stats['success_categories'] += 1
                            products = result.get('products', [])
                            all_products.extend(products)
                            stats['total_products'] += len(products)
                        elif result.get('status') == 'timeout':
                            stats['timeout_categories'] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats['failed_categories'] += 1
                            logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get('status') == 'success':
                    stats['success_categories'] += 1
                    products = all_results.get('products', [])
                    all_products.extend(products)
                    stats['total_products'] += len(products)
                elif all_results.get('status') == 'timeout':
                    stats['timeout_categories'] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats['failed_categories'] += 1
                    logger.warning(f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}")
            
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                logger.info("Th·ª≠ l·∫•y t·ª´ng map_index...")
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id,
                            key='return_value',
                            map_indexes=[map_index]
                        )
                        
                        if result and isinstance(result, dict):
                            if result.get('status') == 'success':
                                stats['success_categories'] += 1
                                products = result.get('products', [])
                                all_products.extend(products)
                                stats['total_products'] += len(products)
                            elif result.get('status') == 'timeout':
                                stats['timeout_categories'] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats['failed_categories'] += 1
                                logger.warning(f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}")
                    except Exception as e:
                        stats['failed_categories'] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")
        
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats['failed_categories'] = len(categories)
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        for product in all_products:
            product_id = product.get('product_id')
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                unique_products.append(product)
        
        stats['unique_products'] = len(unique_products)
        
        logger.info("="*70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("="*70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("="*70)
        
        result = {
            'products': unique_products,
            'stats': stats,
            'merged_at': datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt
    
    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)
    
    filepath = Path(filepath)
    temp_file = filepath.with_suffix('.tmp')
    
    try:
        # Ghi v√†o temporary file
        with open(temp_file, 'w', encoding='utf-8') as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))
        
        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == 'nt':  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))
        
        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")
        
    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)
    
    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üíæ TASK: Save Products")
    logger.info("="*70)
    
    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context['ti']
        merge_result = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids='process_and_save.merge_products')
            logger.info(f"L·∫•y merge_result t·ª´ 'process_and_save.merge_products'")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids='merge_products')
                logger.info(f"L·∫•y merge_result t·ª´ 'merge_products'")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")
        
        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")
        
        products = merge_result.get('products', [])
        stats = merge_result.get('stats', {})
        
        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")
        
        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(Variable.get('TIKI_SAVE_BATCH_SIZE', default_var='10000'))
        
        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i:i + batch_size]
                batch_file = OUTPUT_DIR / f'products_batch_{i // batch_size}.json'
                batch_data = {
                    'batch_index': i // batch_size,
                    'total_batches': (len(products) + batch_size - 1) // batch_size,
                    'products': batch
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            'total_products': len(products),
            'stats': stats,
            'crawled_at': datetime.now().isoformat(),
            'note': 'Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping',
            'products': products
        }
        
        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def prepare_products_for_detail(**context) -> List[Dict[str, Any]]:
    """
    Task: Chu·∫©n b·ªã danh s√°ch products ƒë·ªÉ crawl detail
    
    T·ªëi ∆∞u:
    - Ch·ªâ crawl products ch∆∞a c√≥ detail
    - Chia th√†nh batches ƒë·ªÉ x·ª≠ l√Ω song song
    - Ki·ªÉm tra cache ƒë·ªÉ tr√°nh crawl l·∫°i
    
    Returns:
        List[Dict]: List c√°c dict ch·ª©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üìã TASK: Prepare Products for Detail Crawling")
    logger.info("="*70)
    
    try:
        ti = context['ti']
        
        # L·∫•y products t·ª´ task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids='process_and_save.merge_products')
        except:
            try:
                merge_result = ti.xcom_pull(task_ids='merge_products')
            except:
                pass
        
        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    merge_result = {'products': data.get('products', [])}
        
        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")
        
        products = merge_result.get('products', [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")
        
        # L·ªçc products c·∫ßn crawl detail
        # Ki·ªÉm tra cache ƒë·ªÉ tr√°nh crawl l·∫°i
        products_to_crawl = []
        cache_hits = 0
        
        for product in products:
            product_id = product.get('product_id')
            product_url = product.get('url')
            
            if not product_id or not product_url:
                continue
            
            # Ki·ªÉm tra cache
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            if cache_file.exists():
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_detail = json.load(f)
                        # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß, skip
                        if cached_detail.get('price', {}).get('current_price'):
                            cache_hits += 1
                            continue
                except:
                    pass
            
            products_to_crawl.append({
                'product_id': product_id,
                'url': product_url,
                'name': product.get('name', ''),
                'product': product  # Gi·ªØ nguy√™n product data
            })
        
        logger.info(f"‚úÖ Products c·∫ßn crawl detail: {len(products_to_crawl)}")
        logger.info(f"üì¶ Cache hits: {cache_hits}")
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu c·∫ßn (ƒë·ªÉ test)
        max_products = int(Variable.get('TIKI_MAX_PRODUCTS_FOR_DETAIL', default_var='0'))
        if max_products > 0:
            products_to_crawl = products_to_crawl[:max_products]
            logger.info(f"‚úì Gi·ªõi h·∫°n: {len(products_to_crawl)} products")
        
        # Debug: Log m·ªôt v√†i products ƒë·∫ßu ti√™n
        if products_to_crawl:
            logger.info(f"üìã Sample products (first 3):")
            for i, p in enumerate(products_to_crawl[:3]):
                logger.info(f"  {i+1}. Product ID: {p.get('product_id')}, URL: {p.get('url')[:80]}...")
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o c·∫ßn crawl detail!")
        
        logger.info(f"üî¢ Tr·∫£ v·ªÅ {len(products_to_crawl)} products cho Dynamic Task Mapping")
        
        return products_to_crawl
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi prepare products: {e}", exc_info=True)
        raise


def crawl_single_product_detail(product_info: Dict[str, Any] = None, **context) -> Dict[str, Any]:
    """
    Task: Crawl detail cho m·ªôt product (Dynamic Task Mapping)
    
    T·ªëi ∆∞u:
    - S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Rate limiting
    - Error handling: ti·∫øp t·ª•c v·ªõi product kh√°c khi l·ªói
    - Atomic write cache
    
    Args:
        product_info: Th√¥ng tin product (t·ª´ expand_kwargs)
        context: Airflow context
        
    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi detail v√† metadata
    """
    logger = get_logger(context)
    
    # L·∫•y product_info t·ª´ keyword argument ho·∫∑c context
    if not product_info:
        ti = context.get('ti')
        if ti:
            op_kwargs = getattr(ti, 'op_kwargs', {})
            if op_kwargs:
                product_info = op_kwargs.get('product_info')
        
        if not product_info:
            product_info = context.get('product_info') or context.get('op_kwargs', {}).get('product_info')
    
    if not product_info:
        logger.error(f"Kh√¥ng t√¨m th·∫•y product_info. Context keys: {list(context.keys())}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y product_info")
    
    product_id = product_info.get('product_id', '')
    product_url = product_info.get('url', '')
    product_name = product_info.get('name', 'Unknown')
    
    logger.info("="*70)
    logger.info(f"üîç TASK: Crawl Product Detail - {product_name}")
    logger.info(f"üîó URL: {product_url}")
    logger.info("="*70)
    
    result = {
        'product_id': product_id,
        'url': product_url,
        'status': 'failed',
        'error': None,
        'detail': None,
        'crawled_at': datetime.now().isoformat()
    }
    
    # Ki·ªÉm tra cache tr∆∞·ªõc
    cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_detail = json.load(f)
                # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß, d√πng cache
                if cached_detail.get('price', {}).get('current_price'):
                    logger.info(f"‚úÖ S·ª≠ d·ª•ng cache cho product {product_id}")
                    result['detail'] = cached_detail
                    result['status'] = 'cached'
                    return result
        except Exception as e:
            logger.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c cache: {e}")
    
    try:
        # L·∫•y c·∫•u h√¨nh
        rate_limit_delay = float(Variable.get('TIKI_DETAIL_RATE_LIMIT_DELAY', default_var='2.0'))  # Delay 2s cho detail
        timeout = int(Variable.get('TIKI_DETAIL_CRAWL_TIMEOUT', default_var='60'))  # 1 ph√∫t m·ªói product
        
        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)
        
        # Crawl v·ªõi timeout
        start_time = time.time()
        
        # S·ª≠ d·ª•ng Selenium ƒë·ªÉ crawl detail (c·∫ßn thi·∫øt cho dynamic content)
        html_content = crawl_product_detail_with_selenium(
            product_url,
            save_html=False,
            verbose=False  # Kh√¥ng verbose trong Airflow
        )
        
        # Extract detail
        detail = extract_product_detail(html_content, product_url, verbose=False)
        
        elapsed = time.time() - start_time
        
        if elapsed > timeout:
            raise TimeoutError(f"Crawl detail v∆∞·ª£t qu√° timeout {timeout}s")
        
        result['detail'] = detail
        result['status'] = 'success'
        result['elapsed_time'] = elapsed
        
        # L∆∞u v√†o cache (atomic write)
        try:
            temp_file = cache_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)
            
            if os.name == 'nt':  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))
            
            logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {elapsed:.1f}s, ƒë√£ cache")
        except Exception as e:
            logger.warning(f"Kh√¥ng l∆∞u ƒë∆∞·ª£c cache: {e}")
        
    except TimeoutError as e:
        result['error'] = str(e)
        result['status'] = 'timeout'
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        
    except Exception as e:
        result['error'] = str(e)
        result['status'] = 'failed'
        logger.error(f"‚ùå L·ªói khi crawl detail: {e}", exc_info=True)
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi product kh√°c
    
    return result


def merge_product_details(**context) -> Dict[str, Any]:
    """
    Task: Merge product details v√†o products list
    
    Returns:
        Dict: Products v·ªõi detail ƒë√£ merge
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üîÑ TASK: Merge Product Details")
    logger.info("="*70)
    
    try:
        ti = context['ti']
        
        # L·∫•y products g·ªëc
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids='process_and_save.merge_products')
        except:
            try:
                merge_result = ti.xcom_pull(task_ids='merge_products')
            except:
                pass
        
        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    merge_result = {'products': data.get('products', [])}
        
        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")
        
        products = merge_result.get('products', [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")
        
        # L·∫•y detail results t·ª´ Dynamic Task Mapping
        task_id = 'crawl_product_details.crawl_product_detail'
        all_detail_results = []
        
        # Th·ª≠ l·∫•y t·ª´ XCom
        try:
            detail_results = ti.xcom_pull(task_ids=task_id, key='return_value')
            
            if isinstance(detail_results, list):
                all_detail_results = detail_results
            elif isinstance(detail_results, dict):
                all_detail_results = list(detail_results.values()) if detail_results else []
            elif detail_results:
                all_detail_results = [detail_results]
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ XCom: {e}")
            # Th·ª≠ l·∫•y t·ª´ng map_index
            for map_index in range(len(products)):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id,
                        key='return_value',
                        map_indexes=[map_index]
                    )
                    if result:
                        all_detail_results.append(result)
                except:
                    pass
        
        logger.info(f"L·∫•y ƒë∆∞·ª£c {len(all_detail_results)} detail results")
        
        # T·∫°o dict ƒë·ªÉ lookup nhanh
        detail_dict = {}
        stats = {
            'total_products': len(products),
            'with_detail': 0,
            'cached': 0,
            'failed': 0,
            'timeout': 0
        }
        
        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get('product_id')
                if product_id:
                    detail_dict[product_id] = detail_result
                    status = detail_result.get('status', 'failed')
                    if status == 'success':
                        stats['with_detail'] += 1
                    elif status == 'cached':
                        stats['cached'] += 1
                    elif status == 'timeout':
                        stats['timeout'] += 1
                    else:
                        stats['failed'] += 1
        
        # Merge detail v√†o products
        products_with_detail = []
        for product in products:
            product_id = product.get('product_id')
            detail_result = detail_dict.get(product_id)
            
            if detail_result and detail_result.get('detail'):
                # Merge detail v√†o product
                detail = detail_result['detail']
                product_with_detail = {**product}
                
                # Update c√°c tr∆∞·ªùng t·ª´ detail
                if detail.get('price'):
                    product_with_detail['price'] = detail['price']
                if detail.get('rating'):
                    product_with_detail['rating'] = detail['rating']
                if detail.get('description'):
                    product_with_detail['description'] = detail['description']
                if detail.get('specifications'):
                    product_with_detail['specifications'] = detail['specifications']
                if detail.get('images'):
                    product_with_detail['images'] = detail['images']
                if detail.get('brand'):
                    product_with_detail['brand'] = detail['brand']
                if detail.get('seller'):
                    product_with_detail['seller'] = detail['seller']
                if detail.get('stock'):
                    product_with_detail['stock'] = detail['stock']
                if detail.get('shipping'):
                    product_with_detail['shipping'] = detail['shipping']
                # C·∫≠p nh·∫≠t sales_count t·ª´ detail (n·∫øu c√≥)
                if detail.get('sales_count') is not None:
                    product_with_detail['sales_count'] = detail['sales_count']
                
                # Th√™m metadata
                product_with_detail['detail_crawled_at'] = detail_result.get('crawled_at')
                product_with_detail['detail_status'] = detail_result.get('status')
                
                products_with_detail.append(product_with_detail)
            else:
                # Gi·ªØ nguy√™n product n·∫øu kh√¥ng c√≥ detail
                products_with_detail.append(product)
        
        logger.info("="*70)
        logger.info("üìä TH·ªêNG K√ä MERGE DETAIL")
        logger.info("="*70)
        logger.info(f"üì¶ T·ªïng products: {stats['total_products']}")
        logger.info(f"‚úÖ C√≥ detail: {stats['with_detail']}")
        logger.info(f"üì¶ Cache: {stats['cached']}")
        logger.info(f"‚ùå Failed: {stats['failed']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout']}")
        logger.info("="*70)
        
        result = {
            'products': products_with_detail,
            'stats': stats,
            'merged_at': datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge details: {e}", exc_info=True)
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: L∆∞u products v·ªõi detail v√†o file
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("üíæ TASK: Save Products with Detail")
    logger.info("="*70)
    
    try:
        ti = context['ti']
        
        # L·∫•y k·∫øt qu·∫£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids='crawl_product_details.merge_product_details')
        except:
            try:
                merge_result = ti.xcom_pull(task_ids='merge_product_details')
            except:
                pass
        
        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y merge result t·ª´ XCom")
        
        products = merge_result.get('products', [])
        stats = merge_result.get('stats', {})
        
        logger.info(f"ƒêang l∆∞u {len(products)} products v·ªõi detail...")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        output_data = {
            'total_products': len(products),
            'stats': stats,
            'crawled_at': datetime.now().isoformat(),
            'note': 'Crawl t·ª´ Airflow DAG v·ªõi product details',
            'products': products
        }
        
        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} products v·ªõi detail v√†o: {output_file}")
        
        return output_file
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products with detail: {e}", exc_info=True)
        raise


def validate_data(**context) -> Dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl
    
    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("="*70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("="*70)
    
    try:
        ti = context['ti']
        output_file = None
        
        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids='process_and_save.save_products')
            logger.info(f"L·∫•y output_file t·ª´ 'process_and_save.save_products': {output_file}")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")
        
        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids='save_products')
                logger.info(f"L·∫•y output_file t·ª´ 'save_products': {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")
        
        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")
        
        logger.info(f"ƒêang validate file: {output_file}")
        
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        products = data.get('products', [])
        
        # Validation
        validation_result = {
            'file_exists': True,
            'total_products': len(products),
            'valid_products': 0,
            'invalid_products': 0,
            'errors': []
        }
        
        required_fields = ['product_id', 'name', 'url']
        
        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []
            
            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)
            
            if is_valid:
                validation_result['valid_products'] += 1
            else:
                validation_result['invalid_products'] += 1
                validation_result['errors'].append({
                    'index': i,
                    'product_id': product.get('product_id'),
                    'missing_fields': missing_fields
                })
        
        logger.info("="*70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("="*70)
        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("="*70)
        
        if validation_result['invalid_products'] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning
        
        return validation_result
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


# T·∫°o DAG
with DAG(**DAG_CONFIG) as dag:
    
    # TaskGroup: Load v√† Prepare
    with TaskGroup('load_and_prepare', tooltip='Load categories v√† chu·∫©n b·ªã') as load_group:
        task_load_categories = PythonOperator(
            task_id='load_categories',
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool='default_pool',
        )
    
    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup('crawl_categories', tooltip='Crawl s·∫£n ph·∫©m t·ª´ c√°c danh m·ª•c') as crawl_group:
        # S·ª≠ d·ª•ng expand ƒë·ªÉ Dynamic Task Mapping
        # C·∫ßn m·ªôt task helper ƒë·ªÉ l·∫•y categories v√† t·∫°o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging
            logger = logging.getLogger("airflow.task")
            
            ti = context['ti']
            
            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y categories t·ª´ XCom
            categories = None
            
            # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids='load_and_prepare.load_categories')
                logger.info(f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")
            
            # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids='load_categories')
                    logger.info(f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items")
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")
            
            # C√°ch 3: Th·ª≠ l·∫•y t·ª´ upstream task
            if not categories:
                try:
                    # L·∫•y t·ª´ task trong c√πng DAG run
                    from airflow.models import TaskInstance
                    dag_run = context['dag_run']
                    upstream_ti = TaskInstance(
                        task=dag.get_task('load_and_prepare.load_categories'),
                        run_id=dag_run.run_id
                    )
                    categories = upstream_ti.xcom_pull(key='return_value')
                    logger.info(f"L·∫•y categories t·ª´ TaskInstance: {len(categories) if categories else 0} items")
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ TaskInstance: {e}")
            
            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []
            
            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []
            
            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories, t·∫°o {len(categories)} tasks cho Dynamic Task Mapping")
            
            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            return [{'category': cat} for cat in categories]
        
        task_prepare_crawl = PythonOperator(
            task_id='prepare_crawl_kwargs',
            python_callable=prepare_crawl_kwargs,
            execution_timeout=timedelta(minutes=1),
        )
        
        # Dynamic Task Mapping v·ªõi expand
        # S·ª≠ d·ª•ng expand v·ªõi op_kwargs ƒë·ªÉ tr√°nh l·ªói v·ªõi PythonOperator constructor
        task_crawl_category = PythonOperator.partial(
            task_id='crawl_category',
            python_callable=crawl_single_category,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t m·ªói category
            pool='default_pool',  # C√≥ th·ªÉ t·∫°o pool ri√™ng n·∫øu c·∫ßn
            retries=1,  # Retry 1 l·∫ßn (t·ªïng 2 l·∫ßn th·ª≠: 1 l·∫ßn ƒë·∫ßu + 1 retry)
        ).expand(
            op_kwargs=task_prepare_crawl.output
        )
    
    # TaskGroup: Process v√† Save
    with TaskGroup('process_and_save', tooltip='Merge v√† l∆∞u s·∫£n ph·∫©m') as process_group:
        task_merge_products = PythonOperator(
            task_id='merge_products',
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool='default_pool',
            trigger_rule='all_done',  # QUAN TR·ªåNG: Ch·∫°y khi t·∫•t c·∫£ upstream tasks done (success ho·∫∑c failed)
        )
        
        task_save_products = PythonOperator(
            task_id='save_products',
            python_callable=save_products,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool='default_pool',
        )
    
    # TaskGroup: Crawl Product Details (Dynamic Task Mapping)
    with TaskGroup('crawl_product_details', tooltip='Crawl chi ti·∫øt s·∫£n ph·∫©m') as detail_group:
        def prepare_detail_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping detail"""
            import logging
            logger = logging.getLogger("airflow.task")
            
            ti = context['ti']
            
            # L·∫•y products t·ª´ prepare_products_for_detail
            products_to_crawl = None
            try:
                products_to_crawl = ti.xcom_pull(task_ids='prepare_products_for_detail')
            except:
                try:
                    products_to_crawl = ti.xcom_pull(task_ids='crawl_product_details.prepare_products_for_detail')
                except:
                    pass
            
            if not products_to_crawl:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y products t·ª´ XCom!")
                return []
            
            if not isinstance(products_to_crawl, list):
                logger.error(f"‚ùå Products kh√¥ng ph·∫£i list: {type(products_to_crawl)}")
                logger.error(f"   Value: {products_to_crawl}")
                return []
            
            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(products_to_crawl)} products t·ª´ XCom")
            
            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            op_kwargs_list = [{'product_info': product} for product in products_to_crawl]
            
            logger.info(f"üî¢ T·∫°o {len(op_kwargs_list)} op_kwargs cho Dynamic Task Mapping")
            if op_kwargs_list:
                logger.info(f"üìã Sample op_kwargs (first 2):")
                for i, kwargs in enumerate(op_kwargs_list[:2]):
                    product_info = kwargs.get('product_info', {})
                    logger.info(f"  {i+1}. Product ID: {product_info.get('product_id')}, URL: {product_info.get('url', '')[:60]}...")
            
            return op_kwargs_list
        
        task_prepare_detail = PythonOperator(
            task_id='prepare_products_for_detail',
            python_callable=prepare_products_for_detail,
            execution_timeout=timedelta(minutes=5),
        )
        
        task_prepare_detail_kwargs = PythonOperator(
            task_id='prepare_detail_kwargs',
            python_callable=prepare_detail_kwargs,
            execution_timeout=timedelta(minutes=1),
        )
        
        # Dynamic Task Mapping cho crawl detail
        task_crawl_product_detail = PythonOperator.partial(
            task_id='crawl_product_detail',
            python_callable=crawl_single_product_detail,
            execution_timeout=timedelta(minutes=2),  # Timeout 2 ph√∫t m·ªói product
            pool='default_pool',
            retries=1,  # Retry 1 l·∫ßn
        ).expand(
            op_kwargs=task_prepare_detail_kwargs.output
        )
        
        task_merge_product_details = PythonOperator(
            task_id='merge_product_details',
            python_callable=merge_product_details,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool='default_pool',
            trigger_rule='all_done',  # Ch·∫°y khi t·∫•t c·∫£ upstream tasks done
        )
        
        task_save_products_with_detail = PythonOperator(
            task_id='save_products_with_detail',
            python_callable=save_products_with_detail,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool='default_pool',
        )
        
        # Dependencies trong detail group
        task_prepare_detail >> task_prepare_detail_kwargs >> task_crawl_product_detail >> task_merge_product_details >> task_save_products_with_detail
    
    # TaskGroup: Validate
    with TaskGroup('validate', tooltip='Validate d·ªØ li·ªáu') as validate_group:
        task_validate_data = PythonOperator(
            task_id='validate_data',
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool='default_pool',
        )
    
    # ƒê·ªãnh nghƒ©a dependencies
    # Flow: Load -> Crawl Categories -> Merge & Save -> Prepare Detail -> Crawl Detail -> Merge & Save Detail -> Validate
    task_load_categories >> task_prepare_crawl >> task_crawl_category >> task_merge_products >> task_save_products
    task_save_products >> task_prepare_detail >> task_prepare_detail_kwargs >> task_crawl_product_detail >> task_merge_product_details >> task_save_products_with_detail >> task_validate_data

