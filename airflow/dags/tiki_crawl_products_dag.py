"""
DAG Airflow ƒë·ªÉ crawl s·∫£n ph·∫©m Tiki v·ªõi t·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn

T√≠nh nƒÉng:
- Dynamic Task Mapping: crawl song song nhi·ªÅu danh m·ª•c
- Chia nh·ªè tasks: m·ªói task m·ªôt ch·ª©c nƒÉng ri√™ng
- XCom: chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c tasks
- Retry: t·ª± ƒë·ªông retry khi l·ªói
- Timeout: gi·ªõi h·∫°n th·ªùi gian th·ª±c thi
- Logging: ghi log r√µ r√†ng cho t·ª´ng task
- Error handling: x·ª≠ l√Ω l·ªói v√† ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c
- Atomic writes: ghi file an to√†n, tr√°nh corrupt
- TaskGroup: nh√≥m c√°c tasks li√™n quan
- T·ªëi ∆∞u: batch processing, rate limiting, caching
"""

import json
import os
import shutil
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any

from airflow.providers.standard.operators.python import PythonOperator

from airflow import DAG

# Import Variable v√† TaskGroup v·ªõi suppress warning
try:
    # Th·ª≠ import t·ª´ airflow.sdk (Airflow 3.x)
    from airflow.sdk import TaskGroup, Variable

    _Variable = Variable  # Alias ƒë·ªÉ d√πng wrapper
except ImportError:
    # Fallback: d√πng airflow.models v√† airflow.utils.task_group (Airflow 2.x)
    try:
        from airflow.utils.task_group import TaskGroup
    except ImportError:
        # N·∫øu kh√¥ng c√≥ TaskGroup, t·∫°o dummy class
        class TaskGroup:
            def __init__(self, *args, **kwargs):
                pass

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

    from airflow.models import Variable as _Variable


# Wrapper function ƒë·ªÉ suppress deprecation warning khi g·ªçi Variable.get()
def get_variable(key, default_var=None):
    """Wrapper cho Variable.get() ƒë·ªÉ suppress deprecation warning"""
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore", category=DeprecationWarning, module="airflow.models.variable"
        )
        return _Variable.get(key, default=default_var)


# Alias Variable ƒë·ªÉ code c≈© v·∫´n ho·∫°t ƒë·ªông, nh∆∞ng d√πng wrapper
class VariableWrapper:
    """Wrapper cho Variable ƒë·ªÉ suppress warnings"""

    @staticmethod
    def get(key, default_var=None):
        return get_variable(key, default_var)

    @staticmethod
    def set(key, value):
        return _Variable.set(key, value)


Variable = VariableWrapper

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o sys.path
# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa DAG file
dag_file_dir = os.path.dirname(os.path.abspath(__file__))

# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
# Trong Docker, src ƒë∆∞·ª£c mount v√†o /opt/airflow/src
possible_paths = [
    # T·ª´ /opt/airflow (Docker default - ∆∞u ti√™n)
    "/opt/airflow/src/pipelines/crawl",
    # T·ª´ airflow/dags/ l√™n 2 c·∫•p ƒë·∫øn root (local development)
    os.path.abspath(os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")),
    # T·ª´ airflow/dags/ l√™n 1 c·∫•p (n·∫øu airflow/ l√† root)
    os.path.abspath(os.path.join(dag_file_dir, "..", "src", "pipelines", "crawl")),
    # T·ª´ workspace root (n·∫øu mount v√†o /workspace)
    "/workspace/src/pipelines/crawl",
    # T·ª´ current working directory
    os.path.join(os.getcwd(), "src", "pipelines", "crawl"),
]

# T√¨m ƒë∆∞·ªùng d·∫´n h·ª£p l·ªá
crawl_module_path = None
crawl_products_path = None

for path in possible_paths:
    test_path = os.path.join(path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = path
        crawl_products_path = test_path
        break

if not crawl_module_path:
    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ DAG file
    relative_path = os.path.abspath(
        os.path.join(dag_file_dir, "..", "..", "src", "pipelines", "crawl")
    )
    test_path = os.path.join(relative_path, "crawl_products.py")
    if os.path.exists(test_path):
        crawl_module_path = relative_path
        crawl_products_path = test_path

# Import module utils TR∆Ø·ªöC (c·∫ßn thi·∫øt cho crawl_products v√† crawl_products_detail)
utils_path = None
if crawl_module_path:
    utils_path = os.path.join(crawl_module_path, "utils.py")
    if not os.path.exists(utils_path):
        utils_path = None

if not utils_path:
    # Th·ª≠ t√¨m trong c√°c possible paths
    for path in possible_paths:
        test_path = os.path.join(path, "utils.py")
        if os.path.exists(test_path):
            utils_path = test_path
            break

if utils_path and os.path.exists(utils_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_utils", utils_path)
        if spec and spec.loader:
            utils_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(utils_module)
            # L∆∞u v√†o sys.modules ƒë·ªÉ c√°c module kh√°c c√≥ th·ªÉ import
            sys.modules["crawl_utils"] = utils_module
            # T·∫°o fake package structure ƒë·ªÉ relative import ho·∫°t ƒë·ªông
            if "pipelines.crawl.utils" not in sys.modules:
                sys.modules["pipelines"] = type(sys)("pipelines")
                sys.modules["pipelines.crawl"] = type(sys)("pipelines.crawl")
                sys.modules["pipelines.crawl.utils"] = utils_module
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import utils module: {e}", stacklevel=2)

# Import module crawl_products
if crawl_products_path and os.path.exists(crawl_products_path):
    try:
        # S·ª≠ d·ª•ng importlib ƒë·ªÉ import tr·ª±c ti·∫øp t·ª´ file (c√°ch ƒë√°ng tin c·∫≠y nh·∫•t)
        import importlib.util

        spec = importlib.util.spec_from_file_location("crawl_products", crawl_products_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_path}")
        crawl_products_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_category_products = crawl_products_module.crawl_category_products
        get_page_with_requests = crawl_products_module.get_page_with_requests
        parse_products_from_html = crawl_products_module.parse_products_from_html
        get_total_pages = crawl_products_module.get_total_pages
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_category_products(*args, **kwargs):
            raise ImportError(f"Module crawl_products ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        get_page_with_requests = crawl_category_products
        parse_products_from_html = crawl_category_products
        get_total_pages = crawl_category_products
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng n·∫øu ƒë√£ th√™m v√†o sys.path
    if crawl_module_path and crawl_module_path not in sys.path:
        sys.path.insert(0, crawl_module_path)

    try:
        from crawl_products import crawl_category_products
    except ImportError as e:
        # Debug: ki·ªÉm tra xem th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        debug_info = {
            "dag_file_dir": dag_file_dir,
            "cwd": os.getcwd(),
            "possible_paths": possible_paths,
            "crawl_module_path": crawl_module_path,
            "crawl_products_path": crawl_products_path,
            "sys_path": sys.path[:5],  # Ch·ªâ l·∫•y 5 ƒë·∫ßu ti√™n
        }

        # Ki·ªÉm tra xem /opt/airflow/src c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists("/opt/airflow/src"):
            try:
                debug_info["opt_airflow_src_contents"] = os.listdir("/opt/airflow/src")
            except Exception:
                pass

        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products.\n" f"Debug info: {debug_info}\n" f"L·ªói g·ªëc: {e}"
        ) from e

# Import module crawl_products_detail
crawl_products_detail_path = None
for path in possible_paths:
    test_path = os.path.join(path, "crawl_products_detail.py")
    if os.path.exists(test_path):
        crawl_products_detail_path = test_path
        break

if crawl_products_detail_path and os.path.exists(crawl_products_detail_path):
    try:
        import importlib.util

        spec = importlib.util.spec_from_file_location(
            "crawl_products_detail", crawl_products_detail_path
        )
        if spec is None or spec.loader is None:
            raise ImportError(f"Kh√¥ng th·ªÉ load spec t·ª´ {crawl_products_detail_path}")
        crawl_products_detail_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(crawl_products_detail_module)

        # Extract c√°c functions c·∫ßn thi·∫øt
        crawl_product_detail_with_selenium = (
            crawl_products_detail_module.crawl_product_detail_with_selenium
        )
        extract_product_detail = crawl_products_detail_module.extract_product_detail
    except Exception as e:
        # N·∫øu import l·ªói, log v√† ti·∫øp t·ª•c (s·∫Ω fail khi ch·∫°y task)
        import warnings

        warnings.warn(f"Kh√¥ng th·ªÉ import crawl_products_detail module: {e}", stacklevel=2)

        # T·∫°o dummy functions ƒë·ªÉ tr√°nh NameError
        error_msg = str(e)

        def crawl_product_detail_with_selenium(*args, **kwargs):
            raise ImportError(f"Module crawl_products_detail ch∆∞a ƒë∆∞·ª£c import: {error_msg}")

        extract_product_detail = crawl_product_detail_with_selenium
else:
    # Fallback: th·ª≠ import th√¥ng th∆∞·ªùng
    try:
        from crawl_products_detail import crawl_product_detail_with_selenium, extract_product_detail
    except ImportError as e:
        raise ImportError(
            f"Kh√¥ng t√¨m th·∫•y module crawl_products_detail.\n"
            f"Path: {crawl_products_detail_path}\n"
            f"L·ªói g·ªëc: {e}"
        ) from e

# C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
DEFAULT_ARGS = {
    "owner": "data-team",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,  # Retry 3 l·∫ßn
    "retry_delay": timedelta(minutes=2),  # Delay 2 ph√∫t gi·ªØa c√°c retry
    "retry_exponential_backoff": True,  # Exponential backoff
    "max_retry_delay": timedelta(minutes=10),
}

# C·∫•u h√¨nh DAG - C√≥ th·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa t·ª± ƒë·ªông v√† th·ªß c√¥ng qua Variable
# ƒê·ªçc schedule mode t·ª´ Airflow Variable (m·∫∑c ƒë·ªãnh: 'manual' ƒë·ªÉ test)
# C√≥ th·ªÉ set Variable 'TIKI_DAG_SCHEDULE_MODE' = 'scheduled' ƒë·ªÉ ch·∫°y t·ª± ƒë·ªông
try:
    schedule_mode = Variable.get("TIKI_DAG_SCHEDULE_MODE", default_var="manual")
except Exception:
    schedule_mode = "manual"  # M·∫∑c ƒë·ªãnh l√† manual ƒë·ªÉ test

# X√°c ƒë·ªãnh schedule d·ª±a tr√™n mode
if schedule_mode == "scheduled":
    dag_schedule = timedelta(days=1)  # Ch·∫°y t·ª± ƒë·ªông h√†ng ng√†y
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (T·ª± ƒë·ªông ch·∫°y h√†ng ng√†y)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "scheduled"]
else:
    dag_schedule = None  # Ch·ªâ ch·∫°y khi trigger th·ªß c√¥ng
    dag_description = (
        "Crawl s·∫£n ph·∫©m Tiki v·ªõi Dynamic Task Mapping v√† t·ªëi ∆∞u h√≥a (Ch·∫°y th·ªß c√¥ng - Test mode)"
    )
    dag_tags = ["tiki", "crawl", "products", "data-pipeline", "manual"]

DAG_CONFIG = {
    "dag_id": "tiki_crawl_products",
    "description": dag_description,
    "default_args": DEFAULT_ARGS,
    "schedule": dag_schedule,
    "start_date": datetime(2025, 11, 1),  # Ng√†y c·ªë ƒë·ªãnh trong qu√° kh·ª©
    "catchup": False,  # Kh√¥ng ch·∫°y l·∫°i c√°c task ƒë√£ b·ªè l·ª°
    "tags": dag_tags,
    "max_active_runs": 1,  # Ch·ªâ ch·∫°y 1 DAG instance t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    "max_active_tasks": 20,  # T·ªëi ƒëa 20 tasks song song
}

# Th∆∞ m·ª•c d·ªØ li·ªáu
# Trong Docker, data ƒë∆∞·ª£c mount v√†o /opt/airflow/data
# Th·ª≠ nhi·ªÅu ƒë∆∞·ªùng d·∫´n
possible_data_dirs = [
    Path("/opt/airflow/data"),  # Docker mount
    Path(__file__).parent.parent.parent / "data",  # Local development
    Path(os.getcwd()) / "data",  # Current working directory
]

DATA_DIR = None
for data_dir in possible_data_dirs:
    if data_dir.exists():
        DATA_DIR = data_dir
        break

if not DATA_DIR:
    # Fallback: d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    DATA_DIR = Path(__file__).parent.parent.parent / "data"

CATEGORIES_FILE = DATA_DIR / "raw" / "categories_recursive_optimized.json"
OUTPUT_DIR = DATA_DIR / "raw" / "products"
CACHE_DIR = OUTPUT_DIR / "cache"
DETAIL_CACHE_DIR = OUTPUT_DIR / "detail" / "cache"
OUTPUT_FILE = OUTPUT_DIR / "products.json"
OUTPUT_FILE_WITH_DETAIL = OUTPUT_DIR / "products_with_detail.json"
# Progress tracking cho multi-day crawling
PROGRESS_FILE = OUTPUT_DIR / "crawl_progress.json"

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Thread-safe lock cho atomic writes
write_lock = Lock()


def get_logger(context):
    """L·∫•y logger t·ª´ context (Airflow 3.x compatible)"""
    try:
        # Airflow 3.x: s·ª≠ d·ª•ng logging module
        import logging

        ti = context.get("task_instance")
        if ti:
            # T·∫°o logger v·ªõi task_id v√† dag_id
            logger_name = f"airflow.task.{ti.dag_id}.{ti.task_id}"
            return logging.getLogger(logger_name)
        else:
            # Fallback: d√πng root logger
            return logging.getLogger("airflow.task")
    except Exception:
        # Fallback: d√πng root logger
        import logging

        return logging.getLogger("airflow.task")


def load_categories(**context) -> list[dict[str, Any]]:
    """
    Task 1: Load danh s√°ch danh m·ª•c t·ª´ file

    Returns:
        List[Dict]: Danh s√°ch danh m·ª•c
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìñ TASK: Load Categories")
    logger.info("=" * 70)

    try:
        categories_file = str(CATEGORIES_FILE)
        logger.info(f"ƒêang ƒë·ªçc file: {categories_file}")

        if not os.path.exists(categories_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {categories_file}")

        with open(categories_file, encoding="utf-8") as f:
            categories = json.load(f)

        logger.info(f"‚úÖ ƒê√£ load {len(categories)} danh m·ª•c")

        # L·ªçc danh m·ª•c n·∫øu c·∫ßn (v√≠ d·ª•: ch·ªâ l·∫•y level 2-4)
        # C√≥ th·ªÉ c·∫•u h√¨nh qua Airflow Variable
        try:
            min_level = int(Variable.get("TIKI_MIN_CATEGORY_LEVEL", default_var="2"))
            max_level = int(Variable.get("TIKI_MAX_CATEGORY_LEVEL", default_var="4"))
            categories = [
                cat for cat in categories if min_level <= cat.get("level", 0) <= max_level
            ]
            logger.info(f"‚úì Sau khi l·ªçc level {min_level}-{max_level}: {len(categories)} danh m·ª•c")
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l·ªçc theo level: {e}")

        # Gi·ªõi h·∫°n s·ªë danh m·ª•c n·∫øu c·∫ßn (ƒë·ªÉ test)
        try:
            max_categories = int(Variable.get("TIKI_MAX_CATEGORIES", default_var="0"))
            if max_categories > 0:
                categories = categories[:max_categories]
                logger.info(f"‚úì Gi·ªõi h·∫°n: {max_categories} danh m·ª•c")
        except Exception:
            pass

        # Push categories l√™n XCom ƒë·ªÉ c√°c task kh√°c d√πng
        return categories

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi load categories: {e}", exc_info=True)
        raise


def crawl_single_category(category: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task 2: Crawl s·∫£n ph·∫©m t·ª´ m·ªôt danh m·ª•c (Dynamic Task Mapping)

    T·ªëi ∆∞u h√≥a:
    - Rate limiting: delay gi·ªØa c√°c request
    - Caching: s·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Error handling: ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c khi l·ªói
    - Timeout: gi·ªõi h·∫°n th·ªùi gian crawl

    Args:
        category: Th√¥ng tin danh m·ª•c (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi products v√† metadata
    """
    logger = get_logger(context)

    # L·∫•y category t·ª´ keyword argument ho·∫∑c t·ª´ op_kwargs trong context
    # Khi s·ª≠ d·ª•ng expand v·ªõi op_kwargs, category s·∫Ω ƒë∆∞·ª£c truy·ªÅn qua op_kwargs
    if not category:
        # Th·ª≠ l·∫•y t·ª´ ti.op_kwargs (c√°ch ch√≠nh x√°c nh·∫•t)
        ti = context.get("ti")
        if ti:
            # op_kwargs ƒë∆∞·ª£c truy·ªÅn v√†o function th√¥ng qua ti
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                category = op_kwargs.get("category")

        # Fallback: th·ª≠ l·∫•y t·ª´ context tr·ª±c ti·∫øp
        if not category:
            category = context.get("category") or context.get("op_kwargs", {}).get("category")

    if not category:
        # Debug: log context ƒë·ªÉ t√¨m l·ªói
        logger.error(f"Kh√¥ng t√¨m th·∫•y category. Context keys: {list(context.keys())}")
        ti = context.get("ti")
        if ti:
            logger.error(f"ti.op_kwargs: {getattr(ti, 'op_kwargs', 'N/A')}")
        raise ValueError("Kh√¥ng t√¨m th·∫•y category. Ki·ªÉm tra expand v·ªõi op_kwargs.")

    category_url = category.get("url", "")
    category_name = category.get("name", "Unknown")
    category_id = category.get("id", "")

    logger.info("=" * 70)
    logger.info(f"üõçÔ∏è  TASK: Crawl Category - {category_name}")
    logger.info(f"üîó URL: {category_url}")
    logger.info("=" * 70)

    result = {
        "category_id": category_id,
        "category_name": category_name,
        "category_url": category_url,
        "products": [],
        "status": "failed",
        "error": None,
        "crawled_at": datetime.now().isoformat(),
        "pages_crawled": 0,
        "products_count": 0,
    }

    try:
        # L·∫•y c·∫•u h√¨nh t·ª´ Airflow Variables
        max_pages = int(
            Variable.get("TIKI_MAX_PAGES_PER_CATEGORY", default_var="20")
        )  # M·∫∑c ƒë·ªãnh 20 trang ƒë·ªÉ tr√°nh timeout
        use_selenium = Variable.get("TIKI_USE_SELENIUM", default_var="false").lower() == "true"
        timeout = int(Variable.get("TIKI_CRAWL_TIMEOUT", default_var="300"))  # 5 ph√∫t m·∫∑c ƒë·ªãnh
        rate_limit_delay = float(
            Variable.get("TIKI_RATE_LIMIT_DELAY", default_var="1.0")
        )  # Delay 1s gi·ªØa c√°c request

        # Rate limiting: delay tr∆∞·ªõc khi crawl
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout
        start_time = time.time()

        products = crawl_category_products(
            category_url,
            max_pages=max_pages if max_pages > 0 else None,
            use_selenium=use_selenium,
            cache_dir=str(CACHE_DIR),
            use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
            use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
        )

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(f"Crawl v∆∞·ª£t qu√° timeout {timeout}s")

        result["products"] = products
        result["status"] = "success"
        result["products_count"] = len(products)
        result["elapsed_time"] = elapsed

        logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {len(products)} s·∫£n ph·∫©m trong {elapsed:.1f}s")

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        logger.error(f"‚è±Ô∏è  Timeout: {e}")
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        logger.error(f"‚ùå L·ªói khi crawl category {category_name}: {e}", exc_info=True)
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi danh m·ª•c kh√°c

    return result


def merge_products(**context) -> dict[str, Any]:
    """
    Task 3: Merge s·∫£n ph·∫©m t·ª´ t·∫•t c·∫£ c√°c danh m·ª•c

    Returns:
        Dict: T·ªïng h·ª£p s·∫£n ph·∫©m v√† th·ªëng k√™
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Products")
    logger.info("=" * 70)

    try:

        ti = context["ti"]

        # L·∫•y categories t·ª´ task load_categories (trong TaskGroup load_and_prepare)
        # Th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ l·∫•y categories
        categories = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
            logger.info(
                f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
            )
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not categories:
            try:
                categories = ti.xcom_pull(task_ids="load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

        if not categories:
            raise ValueError("Kh√¥ng t√¨m th·∫•y categories t·ª´ XCom")

        logger.info(f"ƒêang merge k·∫øt qu·∫£ t·ª´ {len(categories)} danh m·ª•c...")

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        all_products = []
        stats = {
            "total_categories": len(categories),
            "success_categories": 0,
            "failed_categories": 0,
            "timeout_categories": 0,
            "total_products": 0,
            "unique_products": 0,
        }

        # L·∫•y k·∫øt qu·∫£ t·ª´ c√°c task crawl (Dynamic Task Mapping)
        # V·ªõi Dynamic Task Mapping trong Airflow 2.x, c·∫ßn l·∫•y t·ª´ task_id v·ªõi map_index
        task_id = "crawl_categories.crawl_category"

        # L·∫•y t·ª´ XCom - th·ª≠ nhi·ªÅu c√°ch
        try:
            # C√°ch 1: L·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ XCom (Airflow 2.x c√≥ th·ªÉ tr·∫£ v·ªÅ list)
            all_results = ti.xcom_pull(task_ids=task_id, key="return_value")

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if isinstance(all_results, list):
                # N·∫øu l√† list, x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠
                for result in all_results:
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif isinstance(all_results, dict):
                # N·∫øu l√† dict, c√≥ th·ªÉ key l√† map_index ho·∫∑c category_id
                for result in all_results.values():
                    if result and isinstance(result, dict):
                        if result.get("status") == "success":
                            stats["success_categories"] += 1
                            products = result.get("products", [])
                            all_products.extend(products)
                            stats["total_products"] += len(products)
                        elif result.get("status") == "timeout":
                            stats["timeout_categories"] += 1
                            logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                        else:
                            stats["failed_categories"] += 1
                            logger.warning(
                                f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                            )
            elif all_results and isinstance(all_results, dict):
                # N·∫øu ch·ªâ c√≥ 1 k·∫øt qu·∫£ (dict)
                if all_results.get("status") == "success":
                    stats["success_categories"] += 1
                    products = all_results.get("products", [])
                    all_products.extend(products)
                    stats["total_products"] += len(products)
                elif all_results.get("status") == "timeout":
                    stats["timeout_categories"] += 1
                    logger.warning(f"‚è±Ô∏è  Category {all_results.get('category_name')} timeout")
                else:
                    stats["failed_categories"] += 1
                    logger.warning(
                        f"‚ùå Category {all_results.get('category_name')} failed: {all_results.get('error')}"
                    )

            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ l·∫•y t·ª´ng map_index
            if not all_results or (isinstance(all_results, (list, dict)) and len(all_results) == 0):
                logger.info("Th·ª≠ l·∫•y t·ª´ng map_index...")
                for map_index in range(len(categories)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )

                        if result and isinstance(result, dict):
                            if result.get("status") == "success":
                                stats["success_categories"] += 1
                                products = result.get("products", [])
                                all_products.extend(products)
                                stats["total_products"] += len(products)
                            elif result.get("status") == "timeout":
                                stats["timeout_categories"] += 1
                                logger.warning(f"‚è±Ô∏è  Category {result.get('category_name')} timeout")
                            else:
                                stats["failed_categories"] += 1
                                logger.warning(
                                    f"‚ùå Category {result.get('category_name')} failed: {result.get('error')}"
                                )
                    except Exception as e:
                        stats["failed_categories"] += 1
                        logger.warning(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ map_index {map_index}: {e}")

        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ l·∫•y k·∫øt qu·∫£ t·ª´ XCom: {e}", exc_info=True)
            # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c, ƒë√°nh d·∫•u t·∫•t c·∫£ l√† failed
            stats["failed_categories"] = len(categories)

        # Lo·∫°i b·ªè tr√πng l·∫∑p theo product_id
        seen_ids = set()
        unique_products = []
        products_with_sales_count = 0
        for product in all_products:
            product_id = product.get("product_id")
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                # ƒê·∫£m b·∫£o sales_count lu√¥n c√≥ trong product (k·ªÉ c·∫£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                elif product.get("sales_count") is not None:
                    products_with_sales_count += 1
                unique_products.append(product)

        # Log th·ªëng k√™ sales_count
        logger.info(
            f"üìä Products c√≥ sales_count: {products_with_sales_count}/{len(unique_products)} ({products_with_sales_count/len(unique_products)*100:.1f}%)"
            if unique_products
            else "üìä Products c√≥ sales_count: 0/0"
        )

        stats["unique_products"] = len(unique_products)

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä")
        logger.info("=" * 70)
        logger.info(f"üìÅ T·ªïng danh m·ª•c: {stats['total_categories']}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {stats['success_categories']}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {stats['failed_categories']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout_categories']}")
        logger.info(f"üì¶ T·ªïng s·∫£n ph·∫©m (tr∆∞·ªõc dedup): {stats['total_products']}")
        logger.info(f"üì¶ S·∫£n ph·∫©m unique: {stats['unique_products']}")
        logger.info("=" * 70)

        result = {
            "products": unique_products,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge products: {e}", exc_info=True)
        raise


def atomic_write_file(filepath: str, data: Any, **context):
    """
    Ghi file an to√†n (atomic write) ƒë·ªÉ tr√°nh corrupt

    S·ª≠ d·ª•ng temporary file v√† rename ƒë·ªÉ ƒë·∫£m b·∫£o atomicity
    """
    logger = get_logger(context)

    filepath = Path(filepath)
    temp_file = filepath.with_suffix(".tmp")

    try:
        # Ghi v√†o temporary file
        with open(temp_file, "w", encoding="utf-8") as f:
            if isinstance(data, dict):
                json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                f.write(str(data))

        # Atomic rename (tr√™n Unix) ho·∫∑c move (tr√™n Windows)
        if os.name == "nt":  # Windows
            # Tr√™n Windows, c·∫ßn x√≥a file c≈© tr∆∞·ªõc
            if filepath.exists():
                filepath.unlink()
            shutil.move(str(temp_file), str(filepath))
        else:  # Unix/Linux
            os.rename(str(temp_file), str(filepath))

        logger.info(f"‚úÖ ƒê√£ ghi file atomic: {filepath}")

    except Exception as e:
        # X√≥a temp file n·∫øu c√≥ l·ªói
        if temp_file.exists():
            temp_file.unlink()
        logger.error(f"‚ùå L·ªói khi ghi file: {e}", exc_info=True)
        raise


def save_products(**context) -> str:
    """
    Task 4: L∆∞u s·∫£n ph·∫©m v√†o file (atomic write)

    T·ªëi ∆∞u h√≥a cho d·ªØ li·ªáu l·ªõn:
    - Batch processing: chia nh·ªè v√† l∆∞u t·ª´ng batch
    - Atomic write: tr√°nh corrupt file
    - Compression: c√≥ th·ªÉ n√©n file n·∫øu c·∫ßn

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products")
    logger.info("=" * 70)

    try:
        # L·∫•y k·∫øt qu·∫£ t·ª´ task merge_products (trong TaskGroup process_and_save)
        ti = context["ti"]
        merge_result = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
            logger.info("L·∫•y merge_result t·ª´ 'process_and_save.merge_products'")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.merge_products': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not merge_result:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
                logger.info("L·∫•y merge_result t·ª´ 'merge_products'")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'merge_products': {e}")

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ merge t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"ƒêang l∆∞u {len(products)} s·∫£n ph·∫©m...")

        # Batch processing cho d·ªØ li·ªáu l·ªõn
        batch_size = int(Variable.get("TIKI_SAVE_BATCH_SIZE", default_var="10000"))

        if len(products) > batch_size:
            logger.info(f"Chia nh·ªè th√†nh batches (m·ªói batch {batch_size} s·∫£n ph·∫©m)...")
            # L∆∞u t·ª´ng batch v√†o file ri√™ng, sau ƒë√≥ merge
            batch_files = []
            for i in range(0, len(products), batch_size):
                batch = products[i : i + batch_size]
                batch_file = OUTPUT_DIR / f"products_batch_{i // batch_size}.json"
                batch_data = {
                    "batch_index": i // batch_size,
                    "total_batches": (len(products) + batch_size - 1) // batch_size,
                    "products": batch,
                }
                atomic_write_file(str(batch_file), batch_data, **context)
                batch_files.append(batch_file)
                logger.info(f"‚úì ƒê√£ l∆∞u batch {i // batch_size + 1}: {len(batch)} s·∫£n ph·∫©m")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl t·ª´ Airflow DAG v·ªõi Dynamic Task Mapping",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products: {e}", exc_info=True)
        raise


def prepare_products_for_detail(**context) -> list[dict[str, Any]]:
    """
    Task: Chu·∫©n b·ªã danh s√°ch products ƒë·ªÉ crawl detail

    T·ªëi ∆∞u cho multi-day crawling:
    - Ch·ªâ crawl products ch∆∞a c√≥ detail
    - Chia th√†nh batches theo ng√†y (c√≥ th·ªÉ crawl trong nhi·ªÅu ng√†y)
    - Ki·ªÉm tra cache v√† progress ƒë·ªÉ tr√°nh crawl l·∫°i
    - Track progress ƒë·ªÉ resume t·ª´ ƒëi·ªÉm d·ª´ng

    Returns:
        List[Dict]: List c√°c dict ch·ª©a product info cho Dynamic Task Mapping
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üìã TASK: Prepare Products for Detail Crawling (Multi-Day Support)")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products t·ª´ task save_products
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file output
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"üìä T·ªïng s·ªë products: {len(products)}")

        # ƒê·ªçc progress file ƒë·ªÉ bi·∫øt ƒë√£ crawl ƒë·∫øn ƒë√¢u
        progress = {
            "crawled_product_ids": set(),
            "last_crawled_index": 0,
            "total_crawled": 0,
            "last_updated": None,
        }

        if PROGRESS_FILE.exists():
            try:
                with open(PROGRESS_FILE, encoding="utf-8") as f:
                    saved_progress = json.load(f)
                    progress["crawled_product_ids"] = set(
                        saved_progress.get("crawled_product_ids", [])
                    )
                    progress["last_crawled_index"] = saved_progress.get("last_crawled_index", 0)
                    progress["total_crawled"] = saved_progress.get("total_crawled", 0)
                    progress["last_updated"] = saved_progress.get("last_updated")
                    logger.info(
                        f"üìÇ ƒê√£ load progress: {len(progress['crawled_product_ids'])} products ƒë√£ crawl"
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c progress file: {e}")

        # L·ªçc products c·∫ßn crawl detail
        products_to_crawl = []
        cache_hits = 0
        already_crawled = 0

        # L·∫•y c·∫•u h√¨nh cho multi-day crawling
        # T√≠nh to√°n: 500 products ~ 52.75 ph√∫t -> 280 products ~ 30 ph√∫t
        products_per_day = int(
            Variable.get("TIKI_PRODUCTS_PER_DAY", default_var="280")
        )  # M·∫∑c ƒë·ªãnh 280 products/ng√†y (~30 ph√∫t)
        max_products = int(
            Variable.get("TIKI_MAX_PRODUCTS_FOR_DETAIL", default_var="0")
        )  # 0 = kh√¥ng gi·ªõi h·∫°n

        logger.info(
            f"‚öôÔ∏è  C·∫•u h√¨nh: {products_per_day} products/ng√†y, max: {max_products if max_products > 0 else 'kh√¥ng gi·ªõi h·∫°n'}"
        )

        # B·∫Øt ƒë·∫ßu t·ª´ index ƒë√£ crawl
        start_index = progress["last_crawled_index"]
        products_to_check = products[start_index:]

        logger.info(
            f"üîÑ B·∫Øt ƒë·∫ßu t·ª´ index {start_index} (ƒë√£ crawl {progress['total_crawled']} products)"
        )

        for idx, product in enumerate(products_to_check):
            product_id = product.get("product_id")
            product_url = product.get("url")

            if not product_id or not product_url:
                continue

            # Ki·ªÉm tra xem ƒë√£ crawl ch∆∞a (t·ª´ progress)
            if product_id in progress["crawled_product_ids"]:
                already_crawled += 1
                continue

            # Ki·ªÉm tra cache
            cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
            has_valid_cache = False
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        cached_detail = json.load(f)
                        # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                        has_price = cached_detail.get("price", {}).get("current_price")
                        has_sales_count = cached_detail.get("sales_count") is not None

                        # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), ƒë√°nh d·∫•u ƒë√£ crawl
                        if has_price and has_sales_count:
                            cache_hits += 1
                            progress["crawled_product_ids"].add(product_id)
                            already_crawled += 1
                            has_valid_cache = True
                        # N·∫øu cache thi·∫øu sales_count, v·∫´n c·∫ßn crawl l·∫°i
                except Exception:
                    pass

            # N·∫øu ch∆∞a c√≥ cache h·ª£p l·ªá, th√™m v√†o danh s√°ch crawl
            if not has_valid_cache:
                products_to_crawl.append(
                    {
                        "product_id": product_id,
                        "url": product_url,
                        "name": product.get("name", ""),
                        "product": product,  # Gi·ªØ nguy√™n product data
                        "index": start_index + idx,  # L∆∞u index ƒë·ªÉ track progress
                    }
                )

            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products crawl trong ng√†y n√†y
            if len(products_to_crawl) >= products_per_day:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {products_per_day} products cho ng√†y h√¥m nay")
                break

            # Gi·ªõi h·∫°n t·ªïng s·ªë (n·∫øu c√≥)
            if max_products > 0 and len(products_to_crawl) >= max_products:
                logger.info(f"‚úì ƒê√£ ƒë·∫°t gi·ªõi h·∫°n t·ªïng {max_products} products")
                break

        logger.info(f"‚úÖ Products c·∫ßn crawl h√¥m nay: {len(products_to_crawl)}")
        logger.info(f"üì¶ Cache hits: {cache_hits}")
        logger.info(f"‚úì ƒê√£ crawl tr∆∞·ªõc ƒë√≥: {already_crawled}")
        logger.info(f"üìà T·ªïng ƒë√£ crawl: {progress['total_crawled'] + already_crawled}")
        logger.info(
            f"üìâ C√≤n l·∫°i: {len(products) - (progress['total_crawled'] + already_crawled + len(products_to_crawl))}"
        )

        # L∆∞u progress (s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau khi crawl xong)
        if products_to_crawl:
            # L∆∞u index c·ªßa product cu·ªëi c√πng s·∫Ω ƒë∆∞·ª£c crawl
            last_index = products_to_crawl[-1]["index"]
            progress["last_crawled_index"] = last_index + 1
            progress["last_updated"] = datetime.now().isoformat()

            # L∆∞u progress v√†o file
            try:
                with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "crawled_product_ids": list(progress["crawled_product_ids"]),
                            "last_crawled_index": progress["last_crawled_index"],
                            "total_crawled": progress["total_crawled"] + already_crawled,
                            "last_updated": progress["last_updated"],
                        },
                        f,
                        ensure_ascii=False,
                        indent=2,
                    )
                logger.info(f"üíæ ƒê√£ l∆∞u progress: index {progress['last_crawled_index']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng l∆∞u ƒë∆∞·ª£c progress: {e}")

        # Debug: Log m·ªôt v√†i products ƒë·∫ßu ti√™n
        if products_to_crawl:
            logger.info("üìã Sample products (first 3):")
            for i, p in enumerate(products_to_crawl[:3]):
                logger.info(
                    f"  {i+1}. Product ID: {p.get('product_id')}, URL: {p.get('url')[:80]}..."
                )
        else:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o c·∫ßn crawl detail h√¥m nay!")
            logger.info("üí° T·∫•t c·∫£ products ƒë√£ ƒë∆∞·ª£c crawl ho·∫∑c c√≥ cache h·ª£p l·ªá")

        logger.info(f"üî¢ Tr·∫£ v·ªÅ {len(products_to_crawl)} products cho Dynamic Task Mapping")

        return products_to_crawl

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi prepare products: {e}", exc_info=True)
        raise


def crawl_single_product_detail(product_info: dict[str, Any] = None, **context) -> dict[str, Any]:
    """
    Task: Crawl detail cho m·ªôt product (Dynamic Task Mapping)

    T·ªëi ∆∞u:
    - S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh crawl l·∫°i
    - Rate limiting
    - Error handling: ti·∫øp t·ª•c v·ªõi product kh√°c khi l·ªói
    - Atomic write cache

    Args:
        product_info: Th√¥ng tin product (t·ª´ expand_kwargs)
        context: Airflow context

    Returns:
        Dict: K·∫øt qu·∫£ crawl v·ªõi detail v√† metadata
    """
    # Kh·ªüi t·∫°o result m·∫∑c ƒë·ªãnh
    default_result = {
        "product_id": "unknown",
        "url": "",
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    try:
        logger = get_logger(context)
    except Exception as e:
        # N·∫øu kh√¥ng th·ªÉ t·∫°o logger, v·∫´n ti·∫øp t·ª•c v·ªõi default result
        import logging

        logger = logging.getLogger("airflow.task")
        logger.error(f"Kh√¥ng th·ªÉ t·∫°o logger t·ª´ context: {e}")

    # L·∫•y product_info t·ª´ keyword argument ho·∫∑c context
    if not product_info:
        ti = context.get("ti")
        if ti:
            op_kwargs = getattr(ti, "op_kwargs", {})
            if op_kwargs:
                product_info = op_kwargs.get("product_info")

        if not product_info:
            product_info = context.get("product_info") or context.get("op_kwargs", {}).get(
                "product_info"
            )

    if not product_info:
        logger.error(f"Kh√¥ng t√¨m th·∫•y product_info. Context keys: {list(context.keys())}")
        # Return result v·ªõi status failed thay v√¨ raise exception
        return {
            "product_id": "unknown",
            "url": "",
            "status": "failed",
            "error": "Kh√¥ng t√¨m th·∫•y product_info trong context",
            "detail": None,
            "crawled_at": datetime.now().isoformat(),
        }

    product_id = product_info.get("product_id", "")
    product_url = product_info.get("url", "")
    product_name = product_info.get("name", "Unknown")

    logger.info("=" * 70)
    logger.info(f"üîç TASK: Crawl Product Detail - {product_name}")
    logger.info(f"üîó URL: {product_url}")
    logger.info("=" * 70)

    result = {
        "product_id": product_id,
        "url": product_url,
        "status": "failed",
        "error": None,
        "detail": None,
        "crawled_at": datetime.now().isoformat(),
    }

    # Ki·ªÉm tra cache tr∆∞·ªõc - ∆∞u ti√™n Redis, fallback v·ªÅ file
    # Th·ª≠ Redis cache tr∆∞·ªõc (nhanh h∆°n, distributed)
    redis_cache = None
    try:
        from pipelines.crawl.storage.redis_cache import get_redis_cache

        redis_cache = get_redis_cache("redis://redis:6379/1")
        if redis_cache:
            cached_detail = redis_cache.get_cached_product_detail(product_id)
            if cached_detail:
                # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                has_price = cached_detail.get("price", {}).get("current_price")
                has_sales_count = cached_detail.get("sales_count") is not None

                # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), d√πng cache
                if has_price and has_sales_count:
                    logger.info(
                        f"[Redis Cache] ‚úÖ Hit cache cho product {product_id} (c√≥ price v√† sales_count)"
                    )
                    result["detail"] = cached_detail
                    result["status"] = "cached"
                    return result
                elif has_price:
                    # Cache c√≥ price nh∆∞ng thi·∫øu sales_count ‚Üí crawl l·∫°i ƒë·ªÉ l·∫•y sales_count
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache thi·∫øu sales_count cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
                else:
                    # Cache kh√¥ng ƒë·∫ßy ƒë·ªß ‚Üí crawl l·∫°i
                    logger.info(
                        f"[Redis Cache] ‚ö†Ô∏è  Cache kh√¥ng ƒë·∫ßy ƒë·ªß cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
    except Exception:
        # Redis kh√¥ng available, fallback v·ªÅ file cache
        pass

    # Fallback: Ki·ªÉm tra file cache
    cache_file = DETAIL_CACHE_DIR / f"{product_id}.json"
    if cache_file.exists():
        try:
            with open(cache_file, encoding="utf-8") as f:
                cached_detail = json.load(f)
                # Ki·ªÉm tra cache c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng: c·∫ßn c√≥ price v√† sales_count
                has_price = cached_detail.get("price", {}).get("current_price")
                has_sales_count = cached_detail.get("sales_count") is not None

                # N·∫øu ƒë√£ c√≥ detail ƒë·∫ßy ƒë·ªß (c√≥ price v√† sales_count), d√πng cache
                if has_price and has_sales_count:
                    logger.info(
                        f"[File Cache] ‚úÖ Hit cache cho product {product_id} (c√≥ price v√† sales_count)"
                    )
                    result["detail"] = cached_detail
                    result["status"] = "cached"
                    return result
                elif has_price:
                    # Cache c√≥ price nh∆∞ng thi·∫øu sales_count ‚Üí crawl l·∫°i ƒë·ªÉ l·∫•y sales_count
                    logger.info(
                        f"[File Cache] ‚ö†Ô∏è  Cache thi·∫øu sales_count cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
                else:
                    # Cache kh√¥ng ƒë·∫ßy ƒë·ªß ‚Üí crawl l·∫°i
                    logger.info(
                        f"[File Cache] ‚ö†Ô∏è  Cache kh√¥ng ƒë·∫ßy ƒë·ªß cho product {product_id}, s·∫Ω crawl l·∫°i"
                    )
        except Exception as e:
            logger.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c cache: {e}")

    try:
        # Validate URL
        if not product_url or not product_url.startswith("http"):
            raise ValueError(f"URL kh√¥ng h·ª£p l·ªá: {product_url}")

        # L·∫•y c·∫•u h√¨nh
        rate_limit_delay = float(
            Variable.get("TIKI_DETAIL_RATE_LIMIT_DELAY", default_var="2.0")
        )  # Delay 2s cho detail
        timeout = int(
            Variable.get("TIKI_DETAIL_CRAWL_TIMEOUT", default_var="120")
        )  # 2 ph√∫t m·ªói product (tƒÉng t·ª´ 60s)

        # Rate limiting
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)

        # Crawl v·ªõi timeout
        start_time = time.time()

        # S·ª≠ d·ª•ng Selenium ƒë·ªÉ crawl detail (c·∫ßn thi·∫øt cho dynamic content)
        html_content = None
        try:
            # Th·ª≠ crawl v·ªõi retry v√† timeout ng·∫Øn h∆°n
            html_content = crawl_product_detail_with_selenium(
                product_url,
                save_html=False,
                verbose=False,  # Kh√¥ng verbose trong Airflow
                max_retries=2,  # Retry 2 l·∫ßn
                timeout=25,  # Timeout 25s (ng·∫Øn h∆°n ƒë·ªÉ fail nhanh h∆°n)
                use_redis_cache=True,  # S·ª≠ d·ª•ng Redis cache
                use_rate_limiting=True,  # S·ª≠ d·ª•ng rate limiting
            )

            if not html_content or len(html_content) < 100:
                raise ValueError(
                    f"HTML content qu√° ng·∫Øn ho·∫∑c r·ªóng: {len(html_content) if html_content else 0} k√Ω t·ª±"
                )

        except Exception as selenium_error:
            # Log l·ªói Selenium chi ti·∫øt
            error_type = type(selenium_error).__name__
            error_msg = str(selenium_error)

            # R√∫t g·ªçn error message n·∫øu qu√° d√†i
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."

            logger.error(f"‚ùå L·ªói Selenium ({error_type}): {error_msg}")

            # Ki·ªÉm tra c√°c l·ªói ph·ªï bi·∫øn v√† ph√¢n lo·∫°i
            error_msg_lower = error_msg.lower()
            if (
                "chrome" in error_msg_lower
                or "driver" in error_msg_lower
                or "webdriver" in error_msg_lower
            ):
                result["error"] = f"Chrome/Driver error: {error_msg}"
                result["status"] = "selenium_error"
            elif (
                "timeout" in error_msg_lower
                or "timed out" in error_msg_lower
                or "time-out" in error_msg_lower
            ):
                result["error"] = f"Timeout: {error_msg}"
                result["status"] = "timeout"
            elif (
                "connection" in error_msg_lower
                or "network" in error_msg_lower
                or "refused" in error_msg_lower
            ):
                result["error"] = f"Network error: {error_msg}"
                result["status"] = "network_error"
            elif "memory" in error_msg_lower or "out of memory" in error_msg_lower:
                result["error"] = f"Memory error: {error_msg}"
                result["status"] = "memory_error"
            else:
                result["error"] = f"Selenium error: {error_msg}"
                result["status"] = "failed"

            # Kh√¥ng raise, return result v·ªõi status failed
            return result

        # Extract detail
        try:
            detail = extract_product_detail(html_content, product_url, verbose=False)

            if not detail:
                raise ValueError("Kh√¥ng extract ƒë∆∞·ª£c detail t·ª´ HTML")

        except Exception as extract_error:
            error_type = type(extract_error).__name__
            error_msg = str(extract_error)
            logger.error(f"‚ùå L·ªói khi extract detail ({error_type}): {error_msg}")
            result["error"] = f"Extract error: {error_msg}"
            result["status"] = "extract_error"
            return result

        elapsed = time.time() - start_time

        if elapsed > timeout:
            raise TimeoutError(
                f"Crawl detail v∆∞·ª£t qu√° timeout {timeout}s (elapsed: {elapsed:.1f}s)"
            )

        result["detail"] = detail
        result["status"] = "success"
        result["elapsed_time"] = elapsed

        # L∆∞u v√†o cache - ∆∞u ti√™n Redis, fallback v·ªÅ file
        # Redis cache (nhanh, distributed)
        if redis_cache:
            try:
                redis_cache.cache_product_detail(product_id, detail, ttl=604800)  # 7 ng√†y
                logger.info(f"[Redis Cache] ‚úÖ ƒê√£ cache detail cho product {product_id}")
            except Exception as e:
                logger.warning(f"[Redis Cache] ‚ö†Ô∏è  L·ªói khi cache v√†o Redis: {e}")

        # File cache (fallback)
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            DETAIL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

            temp_file = cache_file.with_suffix(".tmp")
            logger.debug(f"üíæ ƒêang l∆∞u cache v√†o: {cache_file}")

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)

            # Atomic move
            if os.name == "nt":  # Windows
                if cache_file.exists():
                    cache_file.unlink()
                shutil.move(str(temp_file), str(cache_file))
            else:  # Unix/Linux
                os.rename(str(temp_file), str(cache_file))

            # Verify cache file was created
            if cache_file.exists():
                logger.info(f"‚úÖ Crawl th√†nh c√¥ng: {elapsed:.1f}s, ƒë√£ cache v√†o {cache_file}")
                # Log sales_count n·∫øu c√≥
                if detail.get("sales_count") is not None:
                    logger.info(f"   üìä sales_count: {detail.get('sales_count')}")
                else:
                    logger.warning("   ‚ö†Ô∏è  sales_count: None (kh√¥ng t√¨m th·∫•y)")
            else:
                logger.error(f"‚ùå Cache file kh√¥ng ƒë∆∞·ª£c t·∫°o: {cache_file}")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng l∆∞u ƒë∆∞·ª£c cache: {e}", exc_info=True)
            # Kh√¥ng fail task v√¨ ƒë√£ crawl th√†nh c√¥ng, ch·ªâ kh√¥ng l∆∞u ƒë∆∞·ª£c cache

    except TimeoutError as e:
        result["error"] = str(e)
        result["status"] = "timeout"
        logger.error(f"‚è±Ô∏è  Timeout: {e}")

    except ValueError as e:
        result["error"] = str(e)
        result["status"] = "validation_error"
        logger.error(f"‚ùå Validation error: {e}")

    except Exception as e:
        result["error"] = str(e)
        result["status"] = "failed"
        error_type = type(e).__name__
        logger.error(f"‚ùå L·ªói khi crawl detail ({error_type}): {e}", exc_info=True)
        # Kh√¥ng raise ƒë·ªÉ ti·∫øp t·ª•c v·ªõi product kh√°c

    # ƒê·∫£m b·∫£o lu√¥n return result, kh√¥ng bao gi·ªù raise exception
    try:
        return result
    except Exception as e:
        # N·∫øu c√≥ l·ªói khi return (kh√¥ng th·ªÉ x·∫£y ra nh∆∞ng ƒë·ªÉ an to√†n)
        logger.error(f"‚ùå L·ªói khi return result: {e}", exc_info=True)
        default_result["error"] = f"L·ªói khi return result: {str(e)}"
        return default_result


def merge_product_details(**context) -> dict[str, Any]:
    """
    Task: Merge product details v√†o products list

    Returns:
        Dict: Products v·ªõi detail ƒë√£ merge
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üîÑ TASK: Merge Product Details")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y products g·ªëc
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="process_and_save.merge_products")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_products")
            except Exception:
                pass

        if not merge_result:
            # Th·ª≠ l·∫•y t·ª´ file
            if OUTPUT_FILE.exists():
                with open(OUTPUT_FILE, encoding="utf-8") as f:
                    data = json.load(f)
                    merge_result = {"products": data.get("products", [])}

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y products t·ª´ XCom ho·∫∑c file")

        products = merge_result.get("products", [])
        logger.info(f"T·ªïng s·ªë products: {len(products)}")

        # L·∫•y s·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl t·ª´ prepare_products_for_detail
        # ƒê√¢y l√† s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø, kh√¥ng ph·∫£i t·ªïng s·ªë products
        products_to_crawl = None
        try:
            products_to_crawl = ti.xcom_pull(
                task_ids="crawl_product_details.prepare_products_for_detail"
            )
        except Exception:
            try:
                products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
            except Exception:
                pass

        # S·ªë l∆∞·ª£ng products th·ª±c t·∫ø ƒë∆∞·ª£c crawl (map_index count)
        expected_crawl_count = len(products_to_crawl) if products_to_crawl else 0
        logger.info(
            f"üìä S·ªë products d·ª± ki·∫øn ƒë∆∞·ª£c crawl (t·ª´ prepare_products_for_detail): {expected_crawl_count}"
        )

        # T·ª± ƒë·ªông ph√°t hi·ªán s·ªë l∆∞·ª£ng map_index th·ª±c t·∫ø c√≥ s·∫µn b·∫±ng c√°ch th·ª≠ l·∫•y XCom
        # ƒêi·ªÅu n√†y gi√∫p x·ª≠ l√Ω tr∆∞·ªùng h·ª£p m·ªôt s·ªë tasks ƒë√£ fail ho·∫∑c ch∆∞a ch·∫°y xong
        actual_crawl_count = expected_crawl_count
        if expected_crawl_count > 0:
            # Th·ª≠ l·∫•y XCom t·ª´ map_index cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø
            # T√¨m map_index cao nh·∫•t c√≥ XCom
            task_id = "crawl_product_details.crawl_product_detail"
            max_found_index = -1

            # Binary search ƒë·ªÉ t√¨m map_index cao nh·∫•t c√≥ XCom (t·ªëi ∆∞u h∆°n linear search)
            # Nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n, th·ª≠ t·ª´ cu·ªëi v·ªÅ ƒë·∫ßu v·ªõi step size l·ªõn
            test_indices = []
            if expected_crawl_count > 1000:
                # V·ªõi s·ªë l∆∞·ª£ng l·ªõn, test m·ªôt s·ªë ƒëi·ªÉm
                step = max(100, expected_crawl_count // 20)
                test_indices = list(range(0, expected_crawl_count, step))
                test_indices.append(expected_crawl_count - 1)
            else:
                # V·ªõi s·ªë l∆∞·ª£ng nh·ªè, test t·∫•t c·∫£
                test_indices = list(range(expected_crawl_count))

            for test_idx in reversed(test_indices):
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[test_idx]
                    )
                    if result:
                        max_found_index = test_idx
                        break
                except Exception:
                    pass

            if max_found_index >= 0:
                # T√¨m ch√≠nh x√°c map_index cao nh·∫•t b·∫±ng c√°ch t√¨m t·ª´ max_found_index
                # Th·ª≠ t·ª´ max_found_index ƒë·∫øn expected_crawl_count
                for idx in range(max_found_index, min(max_found_index + 200, expected_crawl_count)):
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[idx]
                        )
                        if result:
                            max_found_index = idx
                    except Exception:
                        break

                actual_crawl_count = max_found_index + 1
                logger.info(
                    f"‚úÖ Ph√°t hi·ªán {actual_crawl_count} map_index th·ª±c t·∫ø c√≥ XCom (d·ª± ki·∫øn: {expected_crawl_count})"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y XCom n√†o, s·ª≠ d·ª•ng expected_crawl_count: {expected_crawl_count}"
                )

        if actual_crawl_count == 0:
            logger.warning("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o ƒë∆∞·ª£c crawl detail, b·ªè qua merge detail")
            # Tr·∫£ v·ªÅ products g·ªëc kh√¥ng c√≥ detail
            return {
                "products": products,
                "stats": {
                    "total_products": len(products),
                    "with_detail": 0,
                    "cached": 0,
                    "failed": 0,
                    "timeout": 0,
                },
                "merged_at": datetime.now().isoformat(),
            }

        # L·∫•y detail results t·ª´ Dynamic Task Mapping
        task_id = "crawl_product_details.crawl_product_detail"
        all_detail_results = []

        # L·∫•y t·∫•t c·∫£ results b·∫±ng c√°ch l·∫•y t·ª´ng map_index ƒë·ªÉ tr√°nh gi·ªõi h·∫°n XCom
        # CH·ªà l·∫•y t·ª´ map_index 0 ƒë·∫øn actual_crawl_count - 1 (kh√¥ng ph·∫£i len(products))
        logger.info(f"B·∫Øt ƒë·∫ßu l·∫•y detail results t·ª´ {actual_crawl_count} crawled products...")

        # L·∫•y theo batch ƒë·ªÉ t·ªëi ∆∞u
        batch_size = 100
        for start_idx in range(0, actual_crawl_count, batch_size):
            end_idx = min(start_idx + batch_size, actual_crawl_count)
            batch_map_indexes = list(range(start_idx, end_idx))

            try:
                batch_results = ti.xcom_pull(
                    task_ids=task_id, key="return_value", map_indexes=batch_map_indexes
                )

                if batch_results:
                    if isinstance(batch_results, list):
                        # List results theo th·ª© t·ª± map_indexes
                        all_detail_results.extend([r for r in batch_results if r])
                    elif isinstance(batch_results, dict):
                        # Dict v·ªõi key l√† map_index ho·∫∑c string
                        # L·∫•y t·∫•t c·∫£ values, s·∫Øp x·∫øp theo map_index n·∫øu c√≥ th·ªÉ
                        values = [v for v in batch_results.values() if v]
                        all_detail_results.extend(values)
                    else:
                        # Single result
                        all_detail_results.append(batch_results)

                if (start_idx // batch_size + 1) % 10 == 0:
                    logger.info(f"ƒê√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count} results...")
            except Exception as e:
                logger.warning(f"L·ªói khi l·∫•y batch {start_idx}-{end_idx}: {e}")
                # Th·ª≠ l·∫•y t·ª´ng map_index ri√™ng l·∫ª
                for map_index in batch_map_indexes:
                    try:
                        result = ti.xcom_pull(
                            task_ids=task_id, key="return_value", map_indexes=[map_index]
                        )
                        if result:
                            if isinstance(result, list):
                                all_detail_results.extend([r for r in result if r])
                            elif isinstance(result, dict):
                                all_detail_results.append(result)
                            else:
                                all_detail_results.append(result)
                    except Exception as e2:
                        # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                        logger.debug(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e2}")
                        pass

        logger.info(
            f"L·∫•y ƒë∆∞·ª£c {len(all_detail_results)} detail results (mong ƒë·ª£i {actual_crawl_count})"
        )

        # N·∫øu kh√¥ng l·∫•y ƒë·ªß, th·ª≠ l·∫•y t·ª´ng map_index m·ªôt (ch·ªâ trong ph·∫°m vi actual_crawl_count)
        if len(all_detail_results) < actual_crawl_count * 0.8:  # N·∫øu thi·∫øu h∆°n 20%
            logger.warning(
                f"Ch·ªâ l·∫•y ƒë∆∞·ª£c {len(all_detail_results)}/{actual_crawl_count} results, th·ª≠ l·∫•y t·ª´ng map_index..."
            )
            all_detail_results = []  # Reset v√† l·∫•y l·∫°i
            for map_index in range(actual_crawl_count):  # CH·ªà l·∫•y t·ª´ 0 ƒë·∫øn actual_crawl_count - 1
                try:
                    result = ti.xcom_pull(
                        task_ids=task_id, key="return_value", map_indexes=[map_index]
                    )
                    if result:
                        if isinstance(result, list):
                            all_detail_results.extend([r for r in result if r])
                        elif isinstance(result, dict):
                            # N·∫øu l√† dict, c√≥ th·ªÉ l√† dict ch·ª©a result
                            all_detail_results.append(result)
                        else:
                            all_detail_results.append(result)

                    if (map_index + 1) % 500 == 0:
                        logger.info(
                            f"ƒê√£ l·∫•y {len(all_detail_results)}/{actual_crawl_count} results (t·ª´ng map_index)..."
                        )
                except Exception as e:
                    # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c (c√≥ th·ªÉ task ch∆∞a ch·∫°y xong ho·∫∑c failed)
                    logger.debug(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c map_index {map_index}: {e}")
                    pass

            logger.info(f"Sau khi l·∫•y t·ª´ng map_index: {len(all_detail_results)} detail results")

        # T·∫°o dict ƒë·ªÉ lookup nhanh
        detail_dict = {}
        stats = {
            "total_products": len(products),
            "with_detail": 0,
            "cached": 0,
            "failed": 0,
            "timeout": 0,
        }

        for detail_result in all_detail_results:
            if detail_result and isinstance(detail_result, dict):
                product_id = detail_result.get("product_id")
                if product_id:
                    detail_dict[product_id] = detail_result
                    status = detail_result.get("status", "failed")
                    if status == "success":
                        stats["with_detail"] += 1
                    elif status == "cached":
                        stats["cached"] += 1
                    elif status == "timeout":
                        stats["timeout"] += 1
                    else:
                        stats["failed"] += 1

        # Merge detail v√†o products
        products_with_detail = []
        for product in products:
            product_id = product.get("product_id")
            detail_result = detail_dict.get(product_id)

            if detail_result and detail_result.get("detail"):
                # Merge detail v√†o product
                detail = detail_result["detail"]
                product_with_detail = {**product}

                # Update c√°c tr∆∞·ªùng t·ª´ detail
                if detail.get("price"):
                    product_with_detail["price"] = detail["price"]
                if detail.get("rating"):
                    product_with_detail["rating"] = detail["rating"]
                if detail.get("description"):
                    product_with_detail["description"] = detail["description"]
                if detail.get("specifications"):
                    product_with_detail["specifications"] = detail["specifications"]
                if detail.get("images"):
                    product_with_detail["images"] = detail["images"]
                if detail.get("brand"):
                    product_with_detail["brand"] = detail["brand"]
                if detail.get("seller"):
                    product_with_detail["seller"] = detail["seller"]
                if detail.get("stock"):
                    product_with_detail["stock"] = detail["stock"]
                if detail.get("shipping"):
                    product_with_detail["shipping"] = detail["shipping"]
                # C·∫≠p nh·∫≠t sales_count: ∆∞u ti√™n t·ª´ detail, n·∫øu kh√¥ng c√≥ th√¨ d√πng t·ª´ product g·ªëc
                # Ch·ªâ c·∫ßn c√≥ trong m·ªôt trong hai l√† ƒë·ªß
                if detail.get("sales_count") is not None:
                    product_with_detail["sales_count"] = detail["sales_count"]
                elif product.get("sales_count") is not None:
                    product_with_detail["sales_count"] = product["sales_count"]
                # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng c√≥, gi·ªØ None (ƒë√£ c√≥ trong product g·ªëc)

                # Th√™m metadata
                product_with_detail["detail_crawled_at"] = detail_result.get("crawled_at")
                product_with_detail["detail_status"] = detail_result.get("status")

                products_with_detail.append(product_with_detail)
            else:
                # Gi·ªØ nguy√™n product n·∫øu kh√¥ng c√≥ detail
                # ƒê·∫£m b·∫£o sales_count c√≥ trong product (k·ªÉ c·∫£ None)
                if "sales_count" not in product:
                    product["sales_count"] = None
                products_with_detail.append(product)

        logger.info("=" * 70)
        logger.info("üìä TH·ªêNG K√ä MERGE DETAIL")
        logger.info("=" * 70)
        logger.info(f"üì¶ T·ªïng products: {stats['total_products']}")
        logger.info(f"‚úÖ C√≥ detail: {stats['with_detail']}")
        logger.info(f"üì¶ Cache: {stats['cached']}")
        logger.info(f"‚ùå Failed: {stats['failed']}")
        logger.info(f"‚è±Ô∏è  Timeout: {stats['timeout']}")
        logger.info("=" * 70)

        result = {
            "products": products_with_detail,
            "stats": stats,
            "merged_at": datetime.now().isoformat(),
        }

        return result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi merge details: {e}", exc_info=True)
        raise


def save_products_with_detail(**context) -> str:
    """
    Task: L∆∞u products v·ªõi detail v√†o file

    Returns:
        str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("üíæ TASK: Save Products with Detail")
    logger.info("=" * 70)

    try:
        ti = context["ti"]

        # L·∫•y k·∫øt qu·∫£ merge
        merge_result = None
        try:
            merge_result = ti.xcom_pull(task_ids="crawl_product_details.merge_product_details")
        except Exception:
            try:
                merge_result = ti.xcom_pull(task_ids="merge_product_details")
            except Exception:
                pass

        if not merge_result:
            raise ValueError("Kh√¥ng t√¨m th·∫•y merge result t·ª´ XCom")

        products = merge_result.get("products", [])
        stats = merge_result.get("stats", {})

        logger.info(f"ƒêang l∆∞u {len(products)} products v·ªõi detail...")

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        output_data = {
            "total_products": len(products),
            "stats": stats,
            "crawled_at": datetime.now().isoformat(),
            "note": "Crawl t·ª´ Airflow DAG v·ªõi product details",
            "products": products,
        }

        # Atomic write
        output_file = str(OUTPUT_FILE_WITH_DETAIL)
        atomic_write_file(output_file, output_data, **context)

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(products)} products v·ªõi detail v√†o: {output_file}")

        return output_file

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi save products with detail: {e}", exc_info=True)
        raise


def validate_data(**context) -> dict[str, Any]:
    """
    Task 5: Validate d·ªØ li·ªáu ƒë√£ crawl

    Returns:
        Dict: K·∫øt qu·∫£ validation
    """
    logger = get_logger(context)
    logger.info("=" * 70)
    logger.info("‚úÖ TASK: Validate Data")
    logger.info("=" * 70)

    try:
        ti = context["ti"]
        output_file = None

        # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
        try:
            output_file = ti.xcom_pull(task_ids="process_and_save.save_products")
            logger.info(f"L·∫•y output_file t·ª´ 'process_and_save.save_products': {output_file}")
        except Exception as e:
            logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'process_and_save.save_products': {e}")

        # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
        if not output_file:
            try:
                output_file = ti.xcom_pull(task_ids="save_products")
                logger.info(f"L·∫•y output_file t·ª´ 'save_products': {output_file}")
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'save_products': {e}")

        if not output_file or not os.path.exists(output_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file output: {output_file}")

        logger.info(f"ƒêang validate file: {output_file}")

        with open(output_file, encoding="utf-8") as f:
            data = json.load(f)

        products = data.get("products", [])

        # Validation
        validation_result = {
            "file_exists": True,
            "total_products": len(products),
            "valid_products": 0,
            "invalid_products": 0,
            "errors": [],
        }

        required_fields = ["product_id", "name", "url"]

        for i, product in enumerate(products):
            is_valid = True
            missing_fields = []

            for field in required_fields:
                if not product.get(field):
                    is_valid = False
                    missing_fields.append(field)

            if is_valid:
                validation_result["valid_products"] += 1
            else:
                validation_result["invalid_products"] += 1
                validation_result["errors"].append(
                    {
                        "index": i,
                        "product_id": product.get("product_id"),
                        "missing_fields": missing_fields,
                    }
                )

        logger.info("=" * 70)
        logger.info("üìä VALIDATION RESULTS")
        logger.info("=" * 70)
        logger.info(f"‚úÖ Valid products: {validation_result['valid_products']}")
        logger.info(f"‚ùå Invalid products: {validation_result['invalid_products']}")
        logger.info("=" * 70)

        if validation_result["invalid_products"] > 0:
            logger.warning(f"C√≥ {validation_result['invalid_products']} s·∫£n ph·∫©m kh√¥ng h·ª£p l·ªá")
            # Kh√¥ng fail task, ch·ªâ warning

        return validation_result

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi validate data: {e}", exc_info=True)
        raise


# T·∫°o DAG duy nh·∫•t v·ªõi schedule c√≥ th·ªÉ config qua Variable
with DAG(**DAG_CONFIG) as dag:

    # TaskGroup: Load v√† Prepare
    with TaskGroup("load_and_prepare", tooltip="Load categories v√† chu·∫©n b·ªã") as load_group:
        task_load_categories = PythonOperator(
            task_id="load_categories",
            python_callable=load_categories,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="default_pool",
        )

    # TaskGroup: Crawl Categories (Dynamic Task Mapping)
    with TaskGroup("crawl_categories", tooltip="Crawl s·∫£n ph·∫©m t·ª´ c√°c danh m·ª•c") as crawl_group:
        # S·ª≠ d·ª•ng expand ƒë·ªÉ Dynamic Task Mapping
        # C·∫ßn m·ªôt task helper ƒë·ªÉ l·∫•y categories v√† t·∫°o list op_kwargs
        def prepare_crawl_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y categories t·ª´ XCom
            categories = None

            # C√°ch 1: L·∫•y t·ª´ task_id v·ªõi TaskGroup prefix
            try:
                categories = ti.xcom_pull(task_ids="load_and_prepare.load_categories")
                logger.info(
                    f"L·∫•y categories t·ª´ 'load_and_prepare.load_categories': {len(categories) if categories else 0} items"
                )
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_and_prepare.load_categories': {e}")

            # C√°ch 2: Th·ª≠ kh√¥ng c√≥ prefix
            if not categories:
                try:
                    categories = ti.xcom_pull(task_ids="load_categories")
                    logger.info(
                        f"L·∫•y categories t·ª´ 'load_categories': {len(categories) if categories else 0} items"
                    )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ 'load_categories': {e}")

            # C√°ch 3: Th·ª≠ l·∫•y t·ª´ upstream task (ƒë∆°n gi·∫£n h√≥a ƒë·ªÉ tr√°nh timeout)
            if not categories:
                try:
                    # L·∫•y t·ª´ task trong c√πng DAG run - ƒë∆°n gi·∫£n h√≥a
                    from airflow.models import TaskInstance

                    dag_run = context["dag_run"]
                    # L·∫•y DAG t·ª´ context thay v√¨ d√πng bi·∫øn global
                    dag_obj = context.get("dag")
                    if dag_obj:
                        upstream_task = dag_obj.get_task("load_and_prepare.load_categories")
                        upstream_ti = TaskInstance(task=upstream_task, run_id=dag_run.run_id)
                        categories = upstream_ti.xcom_pull(key="return_value")
                        logger.info(
                            f"L·∫•y categories t·ª´ TaskInstance: {len(categories) if categories else 0} items"
                        )
                except Exception as e:
                    logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ TaskInstance: {e}")

            if not categories:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y categories t·ª´ XCom!")
                return []

            if not isinstance(categories, list):
                logger.error(f"‚ùå Categories kh√¥ng ph·∫£i list: {type(categories)}")
                return []

            logger.info(
                f"‚úÖ ƒê√£ l·∫•y {len(categories)} categories, t·∫°o {len(categories)} tasks cho Dynamic Task Mapping"
            )

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            return [{"category": cat} for cat in categories]

        task_prepare_crawl = PythonOperator(
            task_id="prepare_crawl_kwargs",
            python_callable=prepare_crawl_kwargs,
            execution_timeout=timedelta(minutes=1),
        )

        # Dynamic Task Mapping v·ªõi expand
        # S·ª≠ d·ª•ng expand v·ªõi op_kwargs ƒë·ªÉ tr√°nh l·ªói v·ªõi PythonOperator constructor
        task_crawl_category = PythonOperator.partial(
            task_id="crawl_category",
            python_callable=crawl_single_category,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t m·ªói category
            pool="default_pool",  # C√≥ th·ªÉ t·∫°o pool ri√™ng n·∫øu c·∫ßn
            retries=1,  # Retry 1 l·∫ßn (t·ªïng 2 l·∫ßn th·ª≠: 1 l·∫ßn ƒë·∫ßu + 1 retry)
        ).expand(op_kwargs=task_prepare_crawl.output)

    # TaskGroup: Process v√† Save
    with TaskGroup("process_and_save", tooltip="Merge v√† l∆∞u s·∫£n ph·∫©m") as process_group:
        task_merge_products = PythonOperator(
            task_id="merge_products",
            python_callable=merge_products,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="default_pool",
            trigger_rule="all_done",  # QUAN TR·ªåNG: Ch·∫°y khi t·∫•t c·∫£ upstream tasks done (success ho·∫∑c failed)
        )

        task_save_products = PythonOperator(
            task_id="save_products",
            python_callable=save_products,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
        )

    # TaskGroup: Crawl Product Details (Dynamic Task Mapping)
    with TaskGroup("crawl_product_details", tooltip="Crawl chi ti·∫øt s·∫£n ph·∫©m") as detail_group:

        def prepare_detail_kwargs(**context):
            """Helper function ƒë·ªÉ prepare op_kwargs cho Dynamic Task Mapping detail"""
            import logging

            logger = logging.getLogger("airflow.task")

            ti = context["ti"]

            # L·∫•y products t·ª´ prepare_products_for_detail
            # Task n√†y n·∫±m trong TaskGroup 'crawl_product_details', n√™n task_id ƒë·∫ßy ƒë·ªß l√† 'crawl_product_details.prepare_products_for_detail'
            products_to_crawl = None

            # L·∫•y t·ª´ upstream task (prepare_products_for_detail) - c√°ch ƒë√°ng tin c·∫≠y nh·∫•t
            # Th·ª≠ l·∫•y upstream_task_ids t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau (t∆∞∆°ng th√≠ch v·ªõi c√°c phi√™n b·∫£n Airflow)
            upstream_task_ids = []
            try:
                task_instance = context.get("task_instance")
                if task_instance:
                    # Th·ª≠ v·ªõi RuntimeTaskInstance (Airflow SDK m·ªõi)
                    if hasattr(task_instance, "upstream_task_ids"):
                        upstream_task_ids = list(task_instance.upstream_task_ids)
                    # Th·ª≠ v·ªõi ti.task (c√°ch kh√°c)
                    elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                        upstream_task_ids = list(ti.task.upstream_task_ids)
            except (AttributeError, TypeError) as e:
                logger.debug(f"   Kh√¥ng th·ªÉ l·∫•y upstream_task_ids: {e}")

            if upstream_task_ids:
                logger.info(f"üîç Upstream tasks: {upstream_task_ids}")
                # Th·ª≠ l·∫•y t·ª´ t·∫•t c·∫£ upstream tasks
                for task_id in upstream_task_ids:
                    try:
                        products_to_crawl = ti.xcom_pull(task_ids=task_id)
                        if products_to_crawl:
                            logger.info(f"‚úÖ L·∫•y XCom t·ª´ upstream task: {task_id}")
                            break
                    except Exception as e:
                        logger.debug(f"   Kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ {task_id}: {e}")
                        continue

            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c, th·ª≠ c√°c c√°ch kh√°c
            if not products_to_crawl:
                try:
                    # Th·ª≠ v·ªõi task_id ƒë·∫ßy ƒë·ªß (c√≥ TaskGroup prefix)
                    products_to_crawl = ti.xcom_pull(
                        task_ids="crawl_product_details.prepare_products_for_detail"
                    )
                    logger.info(
                        "‚úÖ L·∫•y XCom t·ª´ task_id: crawl_product_details.prepare_products_for_detail"
                    )
                except Exception as e1:
                    logger.warning(f"‚ö†Ô∏è  Kh√¥ng l·∫•y ƒë∆∞·ª£c v·ªõi task_id ƒë·∫ßy ƒë·ªß: {e1}")
                    try:
                        # Th·ª≠ v·ªõi task_id kh√¥ng c√≥ prefix (fallback)
                        products_to_crawl = ti.xcom_pull(task_ids="prepare_products_for_detail")
                        logger.info("‚úÖ L·∫•y XCom t·ª´ task_id: prepare_products_for_detail")
                    except Exception as e2:
                        logger.error(f"‚ùå Kh√¥ng th·ªÉ l·∫•y XCom v·ªõi c·∫£ 2 c√°ch: {e1}, {e2}")

            if not products_to_crawl:
                logger.error("‚ùå Kh√¥ng th·ªÉ l·∫•y products t·ª´ XCom!")
                try:
                    task_instance = context.get("task_instance")
                    upstream_info = []
                    if task_instance:
                        if hasattr(task_instance, "upstream_task_ids"):
                            upstream_info = list(task_instance.upstream_task_ids)
                        elif hasattr(ti, "task") and hasattr(ti.task, "upstream_task_ids"):
                            upstream_info = list(ti.task.upstream_task_ids)
                    logger.error(f"   Upstream tasks: {upstream_info}")
                except Exception as e:
                    logger.error(f"   Kh√¥ng th·ªÉ l·∫•y th√¥ng tin upstream tasks: {e}")
                return []

            if not isinstance(products_to_crawl, list):
                logger.error(f"‚ùå Products kh√¥ng ph·∫£i list: {type(products_to_crawl)}")
                logger.error(f"   Value: {products_to_crawl}")
                return []

            logger.info(f"‚úÖ ƒê√£ l·∫•y {len(products_to_crawl)} products t·ª´ XCom")

            # Tr·∫£ v·ªÅ list c√°c dict ƒë·ªÉ expand
            op_kwargs_list = [{"product_info": product} for product in products_to_crawl]

            logger.info(f"üî¢ T·∫°o {len(op_kwargs_list)} op_kwargs cho Dynamic Task Mapping")
            if op_kwargs_list:
                logger.info("üìã Sample op_kwargs (first 2):")
                for i, kwargs in enumerate(op_kwargs_list[:2]):
                    product_info = kwargs.get("product_info", {})
                    logger.info(
                        f"  {i+1}. Product ID: {product_info.get('product_id')}, URL: {product_info.get('url', '')[:60]}..."
                    )

            return op_kwargs_list

        task_prepare_detail = PythonOperator(
            task_id="prepare_products_for_detail",
            python_callable=prepare_products_for_detail,
            execution_timeout=timedelta(minutes=5),
        )

        task_prepare_detail_kwargs = PythonOperator(
            task_id="prepare_detail_kwargs",
            python_callable=prepare_detail_kwargs,
            execution_timeout=timedelta(minutes=1),
        )

        # Dynamic Task Mapping cho crawl detail
        task_crawl_product_detail = PythonOperator.partial(
            task_id="crawl_product_detail",
            python_callable=crawl_single_product_detail,
            execution_timeout=timedelta(
                minutes=7
            ),  # TƒÉng timeout l√™n 7 ph√∫t ƒë·ªÉ ƒë·ªß th·ªùi gian cho Selenium driver kh·ªüi ƒë·ªông
            pool="default_pool",
            retries=2,  # TƒÉng retry l√™n 2 l·∫ßn ƒë·ªÉ gi·∫£m failed tasks
            retry_delay=timedelta(seconds=30),  # Delay 30s gi·ªØa c√°c retry
        ).expand(op_kwargs=task_prepare_detail_kwargs.output)

        task_merge_product_details = PythonOperator(
            task_id="merge_product_details",
            python_callable=merge_product_details,
            execution_timeout=timedelta(minutes=30),  # Timeout 30 ph√∫t
            pool="default_pool",
            trigger_rule="all_done",  # Ch·∫°y khi t·∫•t c·∫£ upstream tasks done
        )

        task_save_products_with_detail = PythonOperator(
            task_id="save_products_with_detail",
            python_callable=save_products_with_detail,
            execution_timeout=timedelta(minutes=10),  # Timeout 10 ph√∫t
            pool="default_pool",
        )

        # Dependencies trong detail group
        (
            task_prepare_detail
            >> task_prepare_detail_kwargs
            >> task_crawl_product_detail
            >> task_merge_product_details
            >> task_save_products_with_detail
        )

    # TaskGroup: Validate
    with TaskGroup("validate", tooltip="Validate d·ªØ li·ªáu") as validate_group:
        task_validate_data = PythonOperator(
            task_id="validate_data",
            python_callable=validate_data,
            execution_timeout=timedelta(minutes=5),  # Timeout 5 ph√∫t
            pool="default_pool",
        )

    # ƒê·ªãnh nghƒ©a dependencies
    # Flow: Load -> Crawl Categories -> Merge & Save -> Prepare Detail -> Crawl Detail -> Merge & Save Detail -> Validate

    # Dependencies gi·ªØa c√°c TaskGroup
    # Load categories tr∆∞·ªõc, sau ƒë√≥ prepare crawl kwargs
    task_load_categories >> task_prepare_crawl

    # Prepare crawl kwargs -> crawl category (dynamic mapping)
    task_prepare_crawl >> task_crawl_category

    # Crawl category -> merge products (merge ch·∫°y khi t·∫•t c·∫£ crawl tasks done)
    task_crawl_category >> task_merge_products

    # Merge -> save products
    task_merge_products >> task_save_products

    # Save products -> prepare detail -> crawl detail -> merge detail -> save detail -> validate
    task_save_products >> task_prepare_detail
    # Dependencies trong detail group ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü d√≤ng 1800
    # Ch·ªâ c·∫ßn th√™m dependency t·ª´ save_products -> prepare_detail (ƒë√£ c√≥ ·ªü tr√™n)
    # v√† t·ª´ save_products_with_detail -> validate
    task_save_products_with_detail >> task_validate_data
