<!-- SEO -->
<!-- Keywords: Tiki Data Pipeline, Airflow, Selenium, Docker, Data Engineering, ETL, Web Scraping, Tiki.vn -->

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=0,0A192F,172A45,64FFDA&height=200&section=header&text=Tiki%20Data%20Pipeline&fontSize=60&fontColor=fff&animation=twinkling&fontAlignY=35&desc=Automated%20Tiki.vn%20Product%20Crawling%20with%20Airflow%20%2B%20Selenium&descAlignY=55&descAlign=50"/>
</div>

<p align="center">
  <img src="https://img.shields.io/badge/Version-1.0.0-blue?style=for-the-badge&logo=github&logoColor=white"/>
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge&logo=opensourceinitiative&logoColor=white"/>
  <img src="https://img.shields.io/badge/Status-Active-success?style=for-the-badge&logo=checkmarx&logoColor=white"/>
  <img src="https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white"/>
</p>

<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=24&duration=3000&pause=1000&color=64FFDA&center=true&vCenter=true&width=700&lines=ğŸ›ï¸+Tiki+Product+Crawler;âš¡+Airflow+%2B+Selenium+Automation;ğŸ“Š+Category+%26+Product+Details;ğŸ”„+Dynamic+Task+Mapping+Optimized" alt="Typing SVG" />
</p>

---

## ğŸ“– Giá»›i thiá»‡u

**Tiki Data Pipeline** lÃ  má»™t há»‡ thá»‘ng ETL (Extract, Transform, Load) hoÃ n chá»‰nh Ä‘á»ƒ crawl, xá»­ lÃ½ vÃ  lÆ°u trá»¯ dá»¯ liá»‡u sáº£n pháº©m tá»« Tiki.vn. Dá»± Ã¡n cung cáº¥p:

- âœ… **Extract**: Crawl danh má»¥c, danh sÃ¡ch sáº£n pháº©m vÃ  chi tiáº¿t tá»« Tiki.vn vá»›i Selenium + Async
- âœ… **Transform**: Normalize, validate vÃ  tÃ­nh toÃ¡n cÃ¡c trÆ°á»ng dá»¯ liá»‡u
- âœ… **Load**: LÆ°u dá»¯ liá»‡u vÃ o PostgreSQL database vá»›i batch processing
- âœ… **Orchestration**: Tá»± Ä‘á»™ng hÃ³a workflow vá»›i Apache Airflow + Dynamic Task Mapping
- âœ… **Performance**: Driver pooling, async crawling, multi-level caching, rate limiting
- âœ… **Resilience**: Circuit breaker, retry patterns, graceful degradation
- âœ… **AI Integration**: Groq AI summarization, Discord notifications
- âœ… **Security**: TruffleHog scanning, secrets management, .env protection

---

## âœ¨ TÃ­nh nÄƒng ná»•i báº­t

<div align="center">

| ğŸ¯ Feature | ğŸ“ Description |
|:---------:|:-------------|
| ğŸ›ï¸ **Product Crawler** | Selenium + Async vá»›i driver pooling vÃ  multi-level caching |
| ğŸ”„ **Data Transformer** | Normalize, validate, computed fields vá»›i comprehensive error handling |
| ğŸ’¾ **Data Loader** | PostgreSQL batch upserts vá»›i connection pooling |
| âš¡ **Airflow DAG** | Dynamic Task Mapping vá»›i batch processing tá»‘i Æ°u |
| ğŸ¤– **AI Integration** | Groq AI summarization + Discord notifications |
| ğŸ›¡ï¸ **Resilience Patterns** | Circuit breaker, retry, graceful degradation, DLQ |
| ğŸš€ **Performance** | Driver pooling, async crawling, Redis + file caching |
| ğŸ” **Data Quality** | Multi-stage validation, deduplication, data integrity checks |
| ğŸ”’ **Security** | TruffleHog scanning, secrets management, environment protection |

</div>

---

## ğŸ› ï¸ Tech Stack

<p align="center">
  <img src="https://skillicons.dev/icons?i=docker,postgres,redis,python,airflow,git,github&theme=dark&perline=7"/>
</p>

<div align="center">
  
| Category | Technologies |
|:--------:|:-----------:|
| **Orchestration** | Apache Airflow 3.1.3, Celery Executor, Dynamic Task Mapping |
| **Web Scraping** | Selenium WebDriver 4.0+, aiohttp, BeautifulSoup4, Async |
| **Databases** | PostgreSQL 16, Redis 7.2 (cache + message broker) |
| **AI/ML** | Groq AI API, Discord Webhooks |
| **Containerization** | Docker, Docker Compose |
| **Languages** | Python 3.8+ (asyncio, typing, dataclasses) |
| **Security** | TruffleHog, dotenv, secrets management |
| **Tools** | Git, GitHub, psycopg2, webdriver-manager |

</div>

---

## ğŸš€ Quick Start

### Prerequisites

```bash
âœ… Docker >= 20.10
âœ… Docker Compose >= 2.0
âœ… RAM: 4GB+ (8GB recommended)
âœ… CPU: 2+ cores
âœ… Disk: 10GB+ free space
âœ… Chrome/Chromium (Ä‘Æ°á»£c cÃ i tá»± Ä‘á»™ng trong Docker)
```

### Installation

#### 1. Clone Repository

```bash
git clone https://github.com/your-username/tiki-data-pipeline.git
cd tiki-data-pipeline
```

#### 2. Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng (Báº¯t buá»™c)

```bash
# Copy file .env.example thÃ nh .env vÃ  Ä‘iá»n cÃ¡c giÃ¡ trá»‹ thá»±c táº¿
cp .env.example .env

# Chá»‰nh sá»­a file .env vá»›i cÃ¡c giÃ¡ trá»‹ cá»§a báº¡n:
nano .env  # hoáº·c sá»­ dá»¥ng text editor yÃªu thÃ­ch

# Cáº¦N THIáº¾T Láº¬P:
# - POSTGRES_USER: TÃªn ngÆ°á»i dÃ¹ng PostgreSQL
# - POSTGRES_PASSWORD: Máº­t kháº©u PostgreSQL (Sá»¬ Dá»¤NG Máº¬T KHáº¨U Máº NH!)
# - _AIRFLOW_WWW_USER_USERNAME: TÃªn ngÆ°á»i dÃ¹ng Airflow Web UI
# - _AIRFLOW_WWW_USER_PASSWORD: Máº­t kháº©u Airflow Web UI (Sá»¬ Dá»¤NG Máº¬T KHáº¨U Máº NH!)

# TÃ™Y CHá»ŒN (cho AI features):
# - GROQ_API_KEY: API key tá»« https://console.groq.com/
# - DISCORD_WEBHOOK_URL: Webhook URL tá»« Discord Server Settings

# âš ï¸ QUAN TRá»ŒNG Báº¢O Máº¬T:
# - File .env Ä‘Ã£ Ä‘Æ°á»£c gitignored - KHÃ”NG BAO GIá»œ commit file nÃ y!
# - Sá»­ dá»¥ng máº­t kháº©u máº¡nh (12+ kÃ½ tá»±, chá»¯ hoa, sá»‘, kÃ½ tá»± Ä‘áº·c biá»‡t)
# - KhÃ´ng share credentials trong chat, email hay public repos
# - Rotate passwords Ä‘á»‹nh ká»³
# - Xem thÃªm: docs/SECURITY.md
```

#### 3. Khá»Ÿi Ä‘á»™ng Airflow Services

```bash
# Build vÃ  khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d --build

# Xem logs
docker-compose logs -f

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps
```

#### 4. Truy cáº­p Airflow Web UI

- **URL**: http://localhost:8080
- **Username**: `airflow`
- **Password**: `airflow`

#### 5. Cháº¡y Pipeline

**CÃ¡ch 1: Sá»­ dá»¥ng Airflow DAG (Khuyáº¿n nghá»‹)**

1. Má»Ÿ Airflow Web UI: http://localhost:8080
2. TÃ¬m DAG `tiki_crawl_products`
3. Click "Play" Ä‘á»ƒ trigger DAG
4. Xem progress trong Graph View

**CÃ¡ch 2: Cháº¡y script trá»±c tiáº¿p**

```bash
# Crawl categories
python src/pipelines/crawl/crawl_categories_recursive.py

# Crawl products tá»« categories
python src/pipelines/crawl/crawl_products.py

# Crawl product details (test)
python src/pipelines/crawl/crawl_products_detail.py
```

---

## ğŸ“Š Architecture

<div align="center">

```mermaid
graph TB
    subgraph "ğŸŒ External Source"
        TIKI[Tiki.vn<br/>Website]
    end
    
    subgraph "â˜ï¸ Airflow Orchestration"
        SCHEDULER[Airflow Scheduler<br/>Schedule & Trigger]
        API[Airflow API Server<br/>Web UI :8080]
        WORKER[Airflow Worker<br/>Execute Tasks]
        PROCESSOR[DAG Processor<br/>Parse DAGs]
    end
    
    subgraph "ğŸ’¾ Storage Layer"
        POSTGRES[(PostgreSQL<br/>Metadata + Products Data)]
        REDIS[(Redis<br/>Cache + Message Broker)]
    end
    
    subgraph "ğŸ“¥ Extract Pipeline"
        CRAWL_CAT[Crawl Categories<br/>Recursive]
        CRAWL_PROD[Crawl Products<br/>Dynamic Task Mapping]
        CRAWL_DETAIL[Crawl Details<br/>Selenium]
    end
    
    subgraph "ğŸ”„ Transform Pipeline"
        TRANSFORM[Transform Products<br/>Normalize + Validate]
        COMPUTE[Compute Fields<br/>Revenue, Popularity]
    end
    
    subgraph "ğŸ“¤ Load Pipeline"
        LOAD[Load to Database<br/>PostgreSQL + JSON]
    end
    
    subgraph "ğŸ“Š Data Storage"
        RAW_JSON[Raw Data<br/>JSON Files]
        PROCESSED_JSON[Processed Data<br/>JSON Files]
        DB_TABLE[(Products Table<br/>PostgreSQL)]
    end
    
    subgraph "ğŸ“ˆ Asset Tracking"
        ASSET_RAW[tiki://products/raw<br/>Dataset]
        ASSET_DETAIL[tiki://products/with_detail<br/>Dataset]
        ASSET_TRANS[tiki://products/transformed<br/>Dataset]
        ASSET_FINAL[tiki://products/final<br/>Dataset]
    end
    
    subgraph "âœ… Validation & Analytics"
        VALIDATE[Validate Data]
        AGGREGATE[Aggregate & Notify]
    end
    
    %% External to Crawl
    TIKI -->|HTTP/HTTPS| CRAWL_CAT
    TIKI -->|HTTP/HTTPS| CRAWL_PROD
    TIKI -->|Selenium| CRAWL_DETAIL
    
    %% Airflow Orchestration
    SCHEDULER --> POSTGRES
    SCHEDULER --> REDIS
    API --> POSTGRES
    WORKER --> REDIS
    WORKER --> POSTGRES
    PROCESSOR --> SCHEDULER
    
    %% Extract Flow
    SCHEDULER --> CRAWL_CAT
    CRAWL_CAT --> CRAWL_PROD
    CRAWL_PROD --> RAW_JSON
    CRAWL_PROD --> ASSET_RAW
    CRAWL_PROD --> CRAWL_DETAIL
    CRAWL_DETAIL --> RAW_JSON
    CRAWL_DETAIL --> ASSET_DETAIL
    
    %% Transform Flow
    ASSET_DETAIL -.->|Asset Trigger| TRANSFORM
    TRANSFORM --> COMPUTE
    COMPUTE --> PROCESSED_JSON
    COMPUTE --> ASSET_TRANS
    
    %% Load Flow
    ASSET_TRANS -.->|Asset Trigger| LOAD
    LOAD --> DB_TABLE
    LOAD --> PROCESSED_JSON
    LOAD --> ASSET_FINAL
    
    %% Validation
    ASSET_FINAL -.->|Asset Trigger| VALIDATE
    VALIDATE --> AGGREGATE
    
    %% Cache
    REDIS -.->|Cache| CRAWL_DETAIL
    REDIS -.->|Cache| CRAWL_PROD
    
    %% Styling
    classDef external fill:#FF6B6B,stroke:#C92A2A,stroke-width:2px,color:#fff
    classDef airflow fill:#017CEE,stroke:#0056B3,stroke-width:2px,color:#fff
    classDef storage fill:#336791,stroke:#1E4A6B,stroke-width:2px,color:#fff
    classDef extract fill:#51CF66,stroke:#2F9E44,stroke-width:2px,color:#fff
    classDef transform fill:#FFD43B,stroke:#F59F00,stroke-width:2px,color:#000
    classDef load fill:#74C0FC,stroke:#1971C2,stroke-width:2px,color:#fff
    classDef data fill:#845EF7,stroke:#5F3DC4,stroke-width:2px,color:#fff
    classDef asset fill:#90EE90,stroke:#2F9E44,stroke-width:2px,color:#000
    classDef validate fill:#FF8787,stroke:#C92A2A,stroke-width:2px,color:#fff
    
    class TIKI external
    class SCHEDULER,API,WORKER,PROCESSOR airflow
    class POSTGRES,REDIS storage
    class CRAWL_CAT,CRAWL_PROD,CRAWL_DETAIL extract
    class TRANSFORM,COMPUTE transform
    class LOAD load
    class RAW_JSON,PROCESSED_JSON,DB_TABLE data
    class ASSET_RAW,ASSET_DETAIL,ASSET_TRANS,ASSET_FINAL asset
    class VALIDATE,AGGREGATE validate
```

**ğŸ“¥ Download diagram files Ä‘á»ƒ import vÃ o cÃ¡c tool:**
- [Mermaid format](docs/architecture.mmd) - Import vÃ o [Mermaid Live Editor](https://mermaid.live), VS Code, hoáº·c GitHub
- [PlantUML format](docs/architecture.puml) - Import vÃ o [PlantUML Online](http://www.plantuml.com/plantuml/uml/), IntelliJ IDEA, hoáº·c VS Code
- [Draw.io format](docs/architecture.drawio.xml) - Import vÃ o [Draw.io](https://app.diagrams.net/) hoáº·c [diagrams.net](https://app.diagrams.net/)

Xem thÃªm: [Architecture Documentation](docs/ARCHITECTURE.md)

</div>

### ETL Pipeline Flow

```
1. Extract (Crawl)
   â”œâ”€â”€ Categories â†’ Products â†’ Product Details
   â””â”€â”€ Output: Raw JSON files + Asset: tiki://products/raw, tiki://products/with_detail

2. Transform
   â”œâ”€â”€ Normalize, Validate, Compute Fields
   â””â”€â”€ Output: Transformed JSON + Asset: tiki://products/transformed

3. Load
   â”œâ”€â”€ PostgreSQL Database + JSON Backup
   â””â”€â”€ Output: Final Data + Asset: tiki://products/final
```

### Services Overview

| Service | Purpose | Port |
|:-------:|:--------|:----:|
| **PostgreSQL** | Airflow metadata + Products data | 5432 (internal) |
| **Redis** | Celery message broker + Cache | 6379 (internal) |
| **Airflow API Server** | Web UI vÃ  REST API | 8080 |
| **Airflow Scheduler** | Schedule vÃ  trigger DAGs | - |
| **Airflow Worker** | Execute tasks | - |
| **Airflow DAG Processor** | Parse vÃ  load DAGs | - |
| **Airflow Triggerer** | Handle deferrable tasks | - |

---

## ğŸ“ Project Structure

```
tiki-data-pipeline/
â”œâ”€â”€ ğŸ“„ README.md                    # File nÃ y
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ³ docker-compose.yaml          # Docker Compose configuration
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“š docs/                        # Documentation
â”œâ”€â”€ ğŸ”§ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ setup/                     # Setup scripts
â”‚   â”œâ”€â”€ utils/                     # Utility scripts
â”‚   â””â”€â”€ shell/                     # Shell scripts
â”œâ”€â”€ â˜ï¸ airflow/                     # Airflow configuration
â”‚   â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â”‚   â””â”€â”€ tiki_crawl_products_dag.py
â”‚   â”œâ”€â”€ logs/                      # Airflow logs
â”‚   â”œâ”€â”€ config/                    # Airflow config
â”‚   â”œâ”€â”€ plugins/                   # Airflow plugins
â”‚   â”œâ”€â”€ setup/                     # Setup scripts
â”‚   â”‚   â””â”€â”€ init-airflow-db.sh     # Database init script
â”‚   â””â”€â”€ Dockerfile                 # Custom Airflow image vá»›i Chrome
â”œâ”€â”€ ğŸ’» src/                         # Source code
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ crawl/                 # Crawling pipelines
â”‚       â”‚   â”œâ”€â”€ crawl_categories_recursive.py    # Crawl categories Ä‘á»‡ quy
â”‚       â”‚   â”œâ”€â”€ crawl_products.py                 # Crawl danh sÃ¡ch sáº£n pháº©m
â”‚       â”‚   â”œâ”€â”€ crawl_products_detail.py          # Crawl chi tiáº¿t sáº£n pháº©m
â”‚       â”‚   â””â”€â”€ config.py                         # Configuration
â”‚       â”œâ”€â”€ transform/             # Transform pipeline
â”‚       â”‚   â””â”€â”€ transformer.py                    # Data transformer
â”‚       â””â”€â”€ load/                  # Load pipeline
â”‚           â””â”€â”€ loader.py                         # Data loader
â”œâ”€â”€ ğŸ“Š data/                        # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                        # Raw data (tá»« crawl)
â”‚   â”‚   â”œâ”€â”€ categories_recursive_optimized.json
â”‚   â”‚   â””â”€â”€ products/
â”‚   â”‚       â”œâ”€â”€ products.json
â”‚   â”‚       â””â”€â”€ products_with_detail.json
â”‚   â””â”€â”€ processed/                 # Processed data (sau transform)
â”‚       â”œâ”€â”€ products_transformed.json
â”‚       â””â”€â”€ products_final.json
â””â”€â”€ ğŸ“š demos/                       # Demo files
    â”œâ”€â”€ demo_step1_crawl.py         # Demo crawl
    â”œâ”€â”€ demo_step2_transform.py     # Demo transform
    â”œâ”€â”€ demo_step3_load.py           # Demo load
    â””â”€â”€ demo_e2e_full.py            # Demo full pipeline
```

---

## ğŸ”§ Pipeline Workflow

### 1. Crawl Categories

Crawl danh má»¥c sáº£n pháº©m Ä‘á»‡ quy tá»« Tiki.vn:

```bash
python src/pipelines/crawl/crawl_categories_recursive.py
```

**Output**: `data/raw/categories_recursive_optimized.json`

**Cáº¥u trÃºc dá»¯ liá»‡u**:
```json
{
  "name": "TÃªn danh má»¥c",
  "slug": "ten-danh-muc",
  "url": "https://tiki.vn/...",
  "image_url": "...",
  "parent_url": "...",
  "level": 1
}
```

### 2. Crawl Products

Crawl danh sÃ¡ch sáº£n pháº©m tá»« cÃ¡c danh má»¥c:

```bash
python src/pipelines/crawl/crawl_products.py
```

**Output**: `data/raw/products/products.json`

**Cáº¥u trÃºc dá»¯ liá»‡u**:
```json
{
  "product_id": "123456789",
  "name": "TÃªn sáº£n pháº©m",
  "url": "https://tiki.vn/...",
  "image_url": "...",
  "sales_count": 2000,
  "category_url": "...",
  "crawled_at": "2024-01-01 12:00:00"
}
```

### 3. Crawl Product Details

Crawl chi tiáº¿t sáº£n pháº©m (giÃ¡, Ä‘Ã¡nh giÃ¡, mÃ´ táº£, thÃ´ng sá»‘ ká»¹ thuáº­t, v.v.):

```bash
python src/pipelines/crawl/crawl_products_detail.py
```

**Output**: `data/raw/products/products_with_detail.json`

**Cáº¥u trÃºc dá»¯ liá»‡u**:
```json
{
  "product_id": "123456789",
  "name": "TÃªn sáº£n pháº©m",
  "price": {
    "current_price": 100000,
    "original_price": 150000,
    "discount_percent": 33.3,
    "currency": "VND"
  },
  "rating": {
    "average": 4.5,
    "total_reviews": 100
  },
  "description": "...",
  "specifications": {...},
  "images": [...],
  "brand": {...},
  "seller": {...},
  "stock": {...},
  "shipping": {...}
}
```

### 4. Transform Products

Transform dá»¯ liá»‡u sáº£n pháº©m Ä‘Ã£ crawl:

```bash
python src/pipelines/transform/transformer.py
```

**Chá»©c nÄƒng**:
- Normalize fields (trim, parse numbers, format)
- Flatten nested structures (price, rating, seller)
- Validate dá»¯ liá»‡u
- TÃ­nh computed fields (revenue, popularity score, value score)

**Output**: `data/processed/products_transformed.json`

### 5. Load Products

Load dá»¯ liá»‡u Ä‘Ã£ transform vÃ o database:

```bash
python src/pipelines/load/loader.py
```

**Chá»©c nÄƒng**:
- Load vÃ o PostgreSQL database
- LÆ°u vÃ o file JSON (backup)
- Batch processing
- Upsert (update náº¿u Ä‘Ã£ tá»“n táº¡i)

**Output**: `data/processed/products_final.json`

### 6. Airflow DAG (Full Pipeline)

DAG tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ quy trÃ¬nh ETL vá»›i **Asset-aware Scheduling**:

1. **Load Categories**: Load danh sÃ¡ch categories tá»« file
2. **Crawl Products**: Crawl products tá»« categories (Dynamic Task Mapping)
3. **Merge Products**: Merge vÃ  lÆ°u danh sÃ¡ch products
   - ğŸ“Š Táº¡o Asset: `tiki://products/raw`
4. **Crawl Product Details**: Crawl chi tiáº¿t products (Dynamic Task Mapping)
5. **Merge Details**: Merge details vÃ o products
   - ğŸ“Š Táº¡o Asset: `tiki://products/with_detail`
6. **Transform Products**: Normalize, validate vÃ  tÃ­nh computed fields
   - ğŸ“Š Táº¡o Asset: `tiki://products/transformed`
7. **Load Products**: Load vÃ o PostgreSQL database
   - ğŸ“Š Táº¡o Asset: `tiki://products/final`
8. **Validate Data**: Validate dá»¯ liá»‡u Ä‘Ã£ load

**Truy cáº­p**: http://localhost:8080  
**DAG ID**: `tiki_crawl_products`

**Asset Tracking**: DAG sá»­ dá»¥ng Dataset/Asset Ä‘á»ƒ track data dependencies. Xem thÃªm: [docs/ASSET_SCHEDULING.md](docs/ASSET_SCHEDULING.md)

### 7. Demo Files (Quick Start)

Cháº¡y tá»«ng bÆ°á»›c hoáº·c toÃ n bá»™ pipeline:

```bash
# Cháº¡y tá»«ng bÆ°á»›c
python demos/demo_step1_crawl.py      # Crawl
python demos/demo_step2_transform.py   # Transform
python demos/demo_step3_load.py        # Load

# Hoáº·c cháº¡y toÃ n bá»™
python demos/demo_e2e_full.py
```

Xem thÃªm: [demos/README.md](demos/README.md)

---

## âš™ï¸ Configuration

### Airflow Variables

Cáº¥u hÃ¬nh cÃ¡c biáº¿n sau trong Airflow UI (Admin â†’ Variables):

| Variable | Default | Description |
|:--------:|:------:|:-----------|
| `TIKI_MAX_CATEGORIES` | `0` | Sá»‘ danh má»¥c tá»‘i Ä‘a (0 = táº¥t cáº£) |
| `TIKI_MAX_PAGES_PER_CATEGORY` | `20` | Sá»‘ trang tá»‘i Ä‘a má»—i danh má»¥c |
| `TIKI_MIN_CATEGORY_LEVEL` | `2` | Level danh má»¥c tá»‘i thiá»ƒu |
| `TIKI_MAX_CATEGORY_LEVEL` | `4` | Level danh má»¥c tá»‘i Ä‘a |
| `TIKI_USE_SELENIUM` | `false` | CÃ³ dÃ¹ng Selenium cho category crawl |
| `TIKI_CRAWL_TIMEOUT` | `300` | Timeout crawl category (giÃ¢y) |
| `TIKI_RATE_LIMIT_DELAY` | `1.0` | Delay giá»¯a requests (giÃ¢y) |
| `TIKI_MAX_PRODUCTS_FOR_DETAIL` | `0` | Sá»‘ products tá»‘i Ä‘a crawl detail (0 = táº¥t cáº£) |
| `TIKI_DETAIL_RATE_LIMIT_DELAY` | `2.0` | Delay cho detail crawl (giÃ¢y) |
| `TIKI_DETAIL_CRAWL_TIMEOUT` | `60` | Timeout crawl detail (giÃ¢y) |
| `TIKI_SAVE_BATCH_SIZE` | `10000` | Sá»‘ sáº£n pháº©m má»—i batch khi save |
| `POSTGRES_HOST` | `postgres` | PostgreSQL host |
| `POSTGRES_PORT` | `5432` | PostgreSQL port |
| `POSTGRES_DB` | `crawl_data` | Database name |
| `POSTGRES_USER` | `airflow` | Database user |
| `POSTGRES_PASSWORD` | `airflow` | Database password |
| `TIKI_USE_ASSET_SCHEDULING` | `false` | Enable Asset-aware scheduling |

### Environment Variables

CÃ¡c biáº¿n mÃ´i trÆ°á»ng cÃ³ thá»ƒ Ä‘Æ°á»£c set trong `.env` hoáº·c `docker-compose.yaml`:

```bash
# Airflow
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=.

# Database (cho Transform & Load)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=crawl_data
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Python packages (sáº½ Ä‘Æ°á»£c cÃ i tá»± Ä‘á»™ng)
_PIP_ADDITIONAL_REQUIREMENTS=selenium>=4.0.0 beautifulsoup4>=4.12.0 requests>=2.31.0 lxml>=4.9.0 tqdm>=4.65.0 webdriver-manager>=4.0.0
```

---

## ğŸ¯ Use Cases

<div align="center">

| Use Case | Description | Example |
|:--------:|:-----------|:--------|
| ğŸ›ï¸ **Product Monitoring** | Theo dÃµi sáº£n pháº©m Tiki | Price tracking, Stock monitoring, Sales trends |
| ğŸ“Š **Market Analysis** | PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng | Category trends, Sales analysis, Popularity metrics |
| ğŸ’° **Price Comparison** | So sÃ¡nh giÃ¡ sáº£n pháº©m | Competitor analysis, Discount tracking |
| ğŸ“ˆ **Data Analytics** | PhÃ¢n tÃ­ch dá»¯ liá»‡u sáº£n pháº©m | Product performance, Reviews analysis, Revenue estimation |
| ğŸ”„ **Automated ETL** | Thu tháº­p vÃ  xá»­ lÃ½ dá»¯ liá»‡u tá»± Ä‘á»™ng | Daily product updates, Data transformation, Database loading |
| ğŸ“Š **Business Intelligence** | BÃ¡o cÃ¡o vÃ  dashboard | Product insights, Market trends, Performance metrics |

</div>

---

## ğŸ† Best Practices

<div align="center">

âœ… **Driver Pooling** - Reuse Selenium drivers Ä‘á»ƒ tá»‘i Æ°u performance  
âœ… **Async Crawling** - Crawl parallel vá»›i aiohttp + asyncio  
âœ… **Multi-level Caching** - Redis + File caching vá»›i TTL vÃ  invalidation  
âœ… **Rate Limiting** - Intelligent delay vÃ  request throttling  
âœ… **Resilience Patterns** - Circuit breaker, retry, graceful degradation  
âœ… **Batch Processing** - Dynamic batch sizing Ä‘á»ƒ optimize memory  
âœ… **Error Handling** - Dead letter queue vÃ  comprehensive logging  
âœ… **Data Validation** - Multi-stage validation pipeline  
âœ… **Atomic Operations** - Safe file writes vÃ  database transactions  
âœ… **Security First** - Secrets management, TruffleHog scanning, .env protection  

</div>

---

## ğŸ”’ Security

Dá»± Ã¡n nÃ y tuÃ¢n thá»§ cÃ¡c best practices vá» báº£o máº­t:

- âœ… **Secrets Management**: Táº¥t cáº£ credentials trong `.env` files (gitignored)
- âœ… **TruffleHog Scanning**: Automated secrets detection trong codebase
- âœ… **No Hardcoded Secrets**: Environment variables cho táº¥t cáº£ sensitive data
- âœ… **Security Documentation**: Comprehensive security guidelines
- âœ… **Incident Response**: Documented procedures cho security incidents

**Xem thÃªm**: [docs/SECURITY.md](docs/SECURITY.md) - Complete security guidelines

---

## ğŸ“ˆ Performance & Resources

<div align="center">

| Component | CPU Limit | Memory Limit | Performance Notes |
|:---------:|:---------:|:------------:|:------------------|
| **PostgreSQL** | 2 cores | 2GB | Connection pooling, indexed queries |
| **Redis** | 1 core | 1GB | In-memory caching + message broker |
| **Airflow Scheduler** | 1 core | 2GB | DAG parsing + task scheduling |
| **Airflow Worker** | 2 cores | 2GB | Celery worker vá»›i task execution |
| **Airflow Webserver** | 0.5 core | 512MB | Web UI + REST API |

**Total Estimated**: ~6-8 CPU cores, ~8-10GB RAM

**Optimizations**:
- âš¡ Driver pooling giáº£m 70% overhead
- âš¡ Async crawling tÄƒng 5x throughput
- âš¡ Multi-level caching giáº£m 80% redundant requests
- âš¡ Batch processing giáº£m 60% memory usage

</div>

---

## âš ï¸ Important Notes

<div align="center">

> âš ï¸ **Rate Limiting**: Intelligent throttling vá»›i configurable delays (1-2s default)  
> ğŸ”’ **Security**: NEVER commit `.env` files - táº¥t cáº£ secrets pháº£i trong environment variables  
> ğŸš€ **Performance**: Driver pooling + async crawling cho throughput tá»‘i Æ°u  
> ğŸ“Š **Data Volume**: 10GB+ disk space recommended cho full product catalog  
> ğŸ³ **Docker Resources**: Minimum 8GB RAM, 4+ CPU cores cho production  
> ğŸ’¾ **Caching Strategy**: Multi-level (Redis + File) vá»›i intelligent TTL  
> ğŸ›¡ï¸ **Resilience**: Circuit breaker + retry patterns cho fault tolerance  
> ğŸ”„ **Batch Processing**: Dynamic batching (10 products/batch) cho memory optimization  
> ğŸ¤– **AI Integration**: Optional Groq AI + Discord notifications  
> ğŸ” **Monitoring**: Comprehensive logging + error tracking vá»›i DLQ  

</div>

---

## ğŸ“š Documentation

### Core Documentation
- ğŸ“– [Architecture Overview](docs/ARCHITECTURE.md) - System architecture vÃ  design patterns
- ğŸ”’ [Security Guidelines](docs/SECURITY.md) - Comprehensive security practices
- ğŸš¨ [Security Incidents](docs/SECURITY_INCIDENT_2025-11-18.md) - Security incident reports
- âš¡ [Performance Analysis](docs/PERFORMANCE_ANALYSIS.md) - Performance benchmarks vÃ  optimizations
- ğŸ¯ [Optimization Guide](docs/OPTIMIZATION_GUIDE.md) - Detailed optimization strategies

### Technical Documentation  
- ğŸ“Š [Redis Usage](docs/REDIS_USAGE.md) - Redis caching strategies
- ğŸ”„ [DAG Data Flow](docs/DAG_DATA_FLOW_ANALYSIS.md) - Airflow DAG analysis
- ğŸ§ª [Test DAG Guide](docs/TEST_DAG_GUIDE.md) - Testing guidelines
- ğŸ“¦ [Category Batch Integration](docs/CATEGORY_BATCH_INTEGRATION.md) - Batch processing patterns

### External References
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Selenium WebDriver Documentation](https://www.selenium.dev/documentation/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)
- [TruffleHog Security Scanner](https://github.com/trufflesecurity/trufflehog)

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. Create a **feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit** your changes: `git commit -m 'Add amazing feature'`
4. **Push** to the branch: `git push origin feature/amazing-feature`
5. Open a **Pull Request**

**Before submitting**:
- âœ… Run security scan: `docker run --rm -v "${PWD}:/scan" ghcr.io/trufflesecurity/trufflehog:latest git file:///scan --only-verified`
- âœ… Run linting: `make lint` (Unix) or `.\scripts\ci.ps1 lint` (Windows)
- âœ… Run tests: `make test` (Unix) or `.\scripts\ci.ps1 test` (Windows)
- âœ… Update documentation if needed

---

## ğŸ“ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**SeikoP**
- GitHub: [@SeikoP](https://github.com/SeikoP)
- Repository: [tiki-data-pipeline](https://github.com/SeikoP/tiki-data-pipeline)

---

## ğŸ™ Acknowledgments

- **Apache Airflow** - Workflow orchestration platform
- **Selenium** - Web automation framework  
- **PostgreSQL** - Robust relational database
- **Redis** - In-memory data structure store
- **Groq AI** - Fast AI inference
- **TruffleHog** - Secrets scanning tool
- **Tiki.vn** - Data source

---

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=0,0A192F,172A45,64FFDA&height=100&section=footer"/>
</div>

<div align="center">
  <p>Made with â¤ï¸ and â˜• by SeikoP</p>
  <p>â­ Star this repo if you find it helpful!</p>
</div>

