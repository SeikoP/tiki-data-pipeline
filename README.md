# ðŸš€ Data Pipeline Template

Má»™t **template repository** hoÃ n chá»‰nh Ä‘á»ƒ cháº¡y **Apache Airflow** káº¿t há»£p vá»›i **Firecrawl Self-Host** cho cÃ¡c dá»± Ã¡n data pipeline.

> ðŸ’¡ **Template nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng láº¡i cho nhiá»u dá»± Ã¡n khÃ¡c nhau!**

## ðŸ“Œ Sá»­ dá»¥ng nhÆ° Template

Xem file [TEMPLATE.md](docs/TEMPLATE.md) Ä‘á»ƒ biáº¿t cÃ¡ch sá»­ dá»¥ng repository nÃ y nhÆ° má»™t template cho dá»± Ã¡n má»›i.

## ðŸŽ¯ TÃ­nh nÄƒng

- âœ… **Apache Airflow 3.1.2** - Workflow orchestration
- âœ… **Firecrawl Self-Host** - Web scraping vÃ  crawling
- âœ… **Shared Databases** - Tá»‘i Æ°u tÃ i nguyÃªn vá»›i 1 Redis + 1 Postgres
- âœ… **Docker Compose** - Dá»… dÃ ng deploy vÃ  quáº£n lÃ½
- âœ… **Resource Limits** - Quáº£n lÃ½ tÃ i nguyÃªn hiá»‡u quáº£
- âœ… **Health Checks** - Tá»± Ä‘á»™ng kiá»ƒm tra sá»©c khá»e services

## ðŸ“‹ YÃªu cáº§u

- **Docker** >= 20.10
- **Docker Compose** >= 2.0
- **RAM**: Tá»‘i thiá»ƒu 4GB (khuyáº¿n nghá»‹ 8GB+)
- **CPU**: Tá»‘i thiá»ƒu 2 cores
- **Disk**: Tá»‘i thiá»ƒu 10GB trá»‘ng

## ðŸš€ Quick Start

### 1. Clone repository

```bash
git clone [<repository-url>](https://github.com/SeikoP/airflow-firecrawl-data-pipeline)
cd airflow-firecrawl-data-pipeline
```

### 2. Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng

```bash
# Copy file máº«u
cp .env.example .env

# Chá»‰nh sá»­a cÃ¡c biáº¿n mÃ´i trÆ°á»ng cáº§n thiáº¿t
# Äáº·c biá»‡t lÃ : OPENAI_API_KEY, BULL_AUTH_KEY, TEST_API_KEY
nano .env  # hoáº·c dÃ¹ng editor khÃ¡c
```

### 3. Khá»Ÿi Ä‘á»™ng services

```bash
# Build vÃ  khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Xem logs
docker-compose logs -f

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps
```

### 4. Truy cáº­p services

- **Airflow Web UI**: http://localhost:8080
  - Username: `airflow` (máº·c Ä‘á»‹nh)
  - Password: `airflow` (máº·c Ä‘á»‹nh)
  
- **Firecrawl API**: http://localhost:3002
  - API documentation: http://localhost:3002/docs

## ðŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
tiki-data-pipeline/
â”œâ”€â”€ docker-compose.yaml          # Cáº¥u hÃ¬nh chÃ­nh (shared databases)
â”œâ”€â”€ docker-compose.separate-db.yaml  # Backup: tÃ¡ch riÃªng databases
â”œâ”€â”€ .env.example                 # Template biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init-multiple-databases.sh   # Script táº¡o databases
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                    # Airflow DAGs cá»§a báº¡n
â”‚   â”œâ”€â”€ logs/                    # Airflow logs (gitignored)
â”‚   â”œâ”€â”€ config/                  # Airflow config
â”‚   â””â”€â”€ plugins/                 # Airflow plugins
â”œâ”€â”€ firecrawl/                   # Firecrawl source code
â””â”€â”€ src/                        # Source code dá»± Ã¡n cá»§a báº¡n
    â”œâ”€â”€ pipelines/
    â”œâ”€â”€ models/
    â””â”€â”€ utils/
```

## ðŸ—„ï¸ Cáº¥u trÃºc Databases

### Redis (Shared)
- **Database 0**: Airflow Celery broker
- **Database 1**: Firecrawl queue & rate limiting

### Postgres (Shared)
- **Database `airflow`**: User `airflow`, password `airflow`
- **Database `nuq`**: User `postgres`, password `postgres`

> ðŸ’¡ **LÆ°u Ã½**: Cáº¥u hÃ¬nh nÃ y tá»‘i Æ°u cho development/staging. Production nÃªn cÃ¢n nháº¯c tÃ¡ch riÃªng databases.

## âš™ï¸ Cáº¥u hÃ¬nh

### Biáº¿n mÃ´i trÆ°á»ng quan trá»ng

Xem file `.env.example` Ä‘á»ƒ biáº¿t danh sÃ¡ch Ä‘áº§y Ä‘á»§. CÃ¡c biáº¿n quan trá»ng nháº¥t:

```bash
# Báº¯t buá»™c
OPENAI_API_KEY=your_key_here
BULL_AUTH_KEY=your_key_here
TEST_API_KEY=your_key_here

# TÃ¹y chá»n nhÆ°ng khuyáº¿n nghá»‹
AIRFLOW_UID=50000  # Linux: id -u
```

### Resource Limits

CÃ¡c services Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh resource limits Ä‘á»ƒ trÃ¡nh chiáº¿m quÃ¡ nhiá»u tÃ i nguyÃªn:

- **Postgres**: 1 CPU, 1GB RAM
- **Redis**: 0.5 CPU, 512MB RAM
- **Airflow Services**: 0.5-2 CPU, 256MB-2GB RAM
- **Firecrawl Services**: 0.5-2 CPU, 512MB-2GB RAM

Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong `docker-compose.yaml` náº¿u cáº§n.

## ðŸ”§ Sá»­ dá»¥ng

### Táº¡o DAG má»›i

```bash
# Táº¡o file DAG trong airflow/dags/
nano airflow/dags/my_dag.py
```

VÃ­ dá»¥ DAG Ä‘Æ¡n giáº£n:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def hello_world():
    print("Hello from Airflow!")

with DAG(
    'my_first_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:
    task = PythonOperator(
        task_id='hello',
        python_callable=hello_world
    )
```

### Sá»­ dá»¥ng Firecrawl API

```python
import requests

# Scrape má»™t website
response = requests.post(
    'http://localhost:3002/v0/scrape',
    json={
        'url': 'https://example.com',
        'formats': ['markdown']
    },
    headers={'Authorization': f'Bearer {TEST_API_KEY}'}
)
print(response.json())
```

### Backup dá»¯ liá»‡u

```bash
# Backup Airflow database
docker-compose exec postgres pg_dump -U airflow airflow > airflow_backup.sql

# Backup Firecrawl database
docker-compose exec postgres pg_dump -U postgres nuq > nuq_backup.sql
```

### Restore dá»¯ liá»‡u

```bash
# Restore Airflow
docker-compose exec -T postgres psql -U airflow -d airflow < airflow_backup.sql

# Restore Firecrawl
docker-compose exec -T postgres psql -U postgres -d nuq < nuq_backup.sql
```

## ðŸ› Troubleshooting

### Services khÃ´ng khá»Ÿi Ä‘á»™ng

```bash
# Xem logs chi tiáº¿t
docker-compose logs [service-name]

# Kiá»ƒm tra health status
docker-compose ps

# Restart service
docker-compose restart [service-name]
```

### Lá»—i database connection

```bash
# Kiá»ƒm tra Postgres Ä‘Ã£ sáºµn sÃ ng
docker-compose exec postgres pg_isready -U postgres

# Kiá»ƒm tra databases Ä‘Ã£ Ä‘Æ°á»£c táº¡o
docker-compose exec postgres psql -U postgres -c "\l"
```

### Lá»—i permissions (Linux)

```bash
# Set AIRFLOW_UID
export AIRFLOW_UID=$(id -u)
echo "AIRFLOW_UID=$AIRFLOW_UID" >> .env
```

### XÃ³a vÃ  khá»Ÿi Ä‘á»™ng láº¡i

```bash
# Dá»«ng vÃ  xÃ³a containers, volumes
docker-compose down -v

# Khá»Ÿi Ä‘á»™ng láº¡i tá»« Ä‘áº§u
docker-compose up -d
```

## ðŸ“š TÃ i liá»‡u tham kháº£o

### TÃ i liá»‡u trong repository
- [Documentation Index](docs/README.md) - Tá»•ng quan tÃ i liá»‡u
- [QUICK_START.md](docs/QUICK_START.md) - HÆ°á»›ng dáº«n nhanh
- [TEMPLATE.md](docs/TEMPLATE.md) - CÃ¡ch sá»­ dá»¥ng template
- [SETUP_GITHUB.md](docs/SETUP_GITHUB.md) - Setup GitHub

### TÃ i liá»‡u bÃªn ngoÃ i
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Firecrawl Self-Host Guide](https://docs.firecrawl.dev/self-hosting)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

## ðŸ”„ Migration tá»« Separate Databases

Náº¿u báº¡n Ä‘ang dÃ¹ng `docker-compose.separate-db.yaml` vÃ  muá»‘n chuyá»ƒn sang shared databases:

```bash
# 1. Backup dá»¯ liá»‡u
docker-compose -f docker-compose.separate-db.yaml exec postgres pg_dump -U airflow airflow > airflow_backup.sql
docker-compose -f docker-compose.separate-db.yaml exec nuq-postgres pg_dump -U postgres postgres > nuq_backup.sql

# 2. Dá»«ng containers cÅ©
docker-compose -f docker-compose.separate-db.yaml down

# 3. Khá»Ÿi Ä‘á»™ng vá»›i cáº¥u hÃ¬nh má»›i
docker-compose up -d

# 4. Restore dá»¯ liá»‡u
docker-compose exec -T postgres psql -U airflow -d airflow < airflow_backup.sql
docker-compose exec -T postgres psql -U postgres -d nuq < nuq_backup.sql
```

## ðŸ“ License

Xem file LICENSE trong repository.

## ðŸ“š TÃ i liá»‡u

Xem thÆ° má»¥c [docs/](docs/) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t:
- [QUICK_START.md](docs/QUICK_START.md) - HÆ°á»›ng dáº«n nhanh push template
- [TEMPLATE.md](docs/TEMPLATE.md) - CÃ¡ch sá»­ dá»¥ng template
- [SETUP_GITHUB.md](docs/SETUP_GITHUB.md) - Setup GitHub template
- [CONTRIBUTING.md](docs/CONTRIBUTING.md) - Contributing guidelines

## ðŸ¤ Contributing

Contributions are welcome! Vui lÃ²ng táº¡o issue hoáº·c pull request. Xem [CONTRIBUTING.md](docs/CONTRIBUTING.md) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## âš ï¸ LÆ°u Ã½

- File `.env` chá»©a thÃ´ng tin nháº¡y cáº£m, **KHÃ”NG** commit lÃªn Git
- Production: NÃªn thay Ä‘á»•i máº­t kháº©u máº·c Ä‘á»‹nh vÃ  sá»­ dá»¥ng secrets management
- Production: CÃ¢n nháº¯c tÃ¡ch riÃªng databases náº¿u cáº§n isolation cao

## ðŸ“ž Support

Náº¿u gáº·p váº¥n Ä‘á», vui lÃ²ng táº¡o issue trÃªn GitHub repository.

