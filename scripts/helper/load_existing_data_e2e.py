"""
Script E2E ƒë·ªÉ load d·ªØ li·ªáu ƒë√£ crawl tr∆∞·ªõc ƒë√≥ v√†o database
- Extract v√† load categories t·ª´ categories_tree.json
- Load products t·ª´ products.json
- ƒê·∫£m b·∫£o li√™n k·∫øt gi·ªØa products v√† categories
"""

import json
import os
import sys
from pathlib import Path
from typing import Any

# Th√™m src v√†o path
# File n√†y ·ªü scripts/helper/, n√™n c·∫ßn l√™n 2 c·∫•p ƒë·ªÉ ƒë·∫øn project root
project_root = Path(__file__).parent.parent.parent
src_path = project_root / "src"

# Ki·ªÉm tra src_path c√≥ t·ªìn t·∫°i kh√¥ng
if not src_path.exists():
    # Th·ª≠ c√°ch kh√°c: t√¨m project root b·∫±ng c√°ch t√¨m th∆∞ m·ª•c c√≥ src/
    current = Path(__file__).parent
    found = False
    while current != current.parent:  # D·ª´ng khi ƒë·∫øn root c·ªßa filesystem
        if (current / "src").exists():
            project_root = current
            src_path = project_root / "src"
            found = True
            break
        current = current.parent

    if not found:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c src. ƒê√£ th·ª≠: {src_path}")

sys.path.insert(0, str(src_path))

from pipelines.extract.extract_categories import extract_categories_from_tree_file
from pipelines.load.loader import DataLoader
from pipelines.transform.transformer import DataTransformer


def load_categories_e2e(loader: DataLoader, tree_file: Path) -> dict[str, Any]:
    """Load categories t·ª´ categories_tree.json v√†o database"""
    print("\n" + "=" * 70)
    print("üìÅ B∆Ø·ªöC 1: EXTRACT & LOAD CATEGORIES")
    print("=" * 70)

    if not tree_file.exists():
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file: {tree_file}")
        print("   B·ªè qua b∆∞·ªõc n√†y...")
        return {"skipped": True, "total_loaded": 0, "db_loaded": 0}

    try:
        # Extract categories
        print(f"üìñ ƒêang extract categories t·ª´: {tree_file}")
        categories = extract_categories_from_tree_file(tree_file)
        print(f"‚úÖ ƒê√£ extract {len(categories)} categories")

        # Load v√†o database
        print("üíæ ƒêang load categories v√†o database...")
        stats = loader.load_categories(
            categories,
            save_to_file=None,
            upsert=True,
            validate_before_load=True,
        )

        print(f"‚úÖ ƒê√£ load {stats['db_loaded']} categories v√†o database")
        print(f"   - T·ªïng s·ªë: {stats['total_loaded']}")
        print(f"   - Th√†nh c√¥ng: {stats['success_count']}")
        print(f"   - Th·∫•t b·∫°i: {stats['failed_count']}")

        if stats.get("errors"):
            print(f"‚ö†Ô∏è  C√≥ {len(stats['errors'])} l·ªói (hi·ªÉn th·ªã 5 ƒë·∫ßu ti√™n):")
            for error in stats["errors"][:5]:
                print(f"   - {error}")

        return stats

    except Exception as e:
        print(f"‚ùå L·ªói khi load categories: {e}")
        import traceback

        traceback.print_exc()
        return {"error": str(e), "total_loaded": 0, "db_loaded": 0}


def load_products_from_cache(cache_dir: Path) -> dict[str, dict[str, Any]]:
    """Load products t·ª´ cache folder (detail/cache)"""
    cache_products = {}

    if not cache_dir.exists():
        return cache_products

    print(f"üìÇ ƒêang qu√©t cache folder: {cache_dir}")
    cache_files = list(cache_dir.glob("*.json"))
    print(f"   T√¨m th·∫•y {len(cache_files)} file cache")

    loaded_count = 0
    error_count = 0

    for cache_file in cache_files:
        try:
            with open(cache_file, encoding="utf-8") as f:
                product_detail = json.load(f)

            product_id = product_detail.get("product_id")
            if not product_id:
                # Th·ª≠ extract t·ª´ t√™n file
                product_id = cache_file.stem

            if product_id:
                cache_products[product_id] = product_detail
                loaded_count += 1
        except Exception as e:
            error_count += 1
            if error_count <= 5:  # Ch·ªâ log 5 l·ªói ƒë·∫ßu ti√™n
                print(f"   ‚ö†Ô∏è  L·ªói khi ƒë·ªçc {cache_file.name}: {e}")

    print(f"‚úÖ ƒê√£ load {loaded_count} products t·ª´ cache")
    if error_count > 5:
        print(f"   ‚ö†Ô∏è  C√≥ th√™m {error_count - 5} l·ªói kh√°c")

    return cache_products


def load_products_e2e(
    loader: DataLoader,
    cache_dir: Path | None = None,
    products_with_detail_file: Path | None = None,
    products_file: Path | None = None,
) -> dict[str, Any]:
    """Load products c√≥ detail ƒë·∫ßy ƒë·ªß v√†o database (t·ª´ cache ho·∫∑c products_with_detail.json)"""
    print("\n" + "=" * 70)
    print("üì¶ B∆Ø·ªöC 2: LOAD PRODUCTS (CH·ªà LOAD D·ªÆ LI·ªÜU C√ì DETAIL)")
    print("=" * 70)

    # B∆∞·ªõc 0: Load category_url mapping t·ª´ products.json (n·∫øu c√≥) - ƒë·ªÉ b·ªï sung category_url
    category_url_mapping = {}  # product_id -> category_url
    if products_file and products_file.exists():
        print(f"üìñ ƒêang ƒë·ªçc category_url mapping t·ª´: {products_file}")
        try:
            with open(products_file, encoding="utf-8") as f:
                data = json.load(f)

            products_list = []
            if isinstance(data, list):
                products_list = data
            elif isinstance(data, dict):
                if "products" in data:
                    products_list = data["products"]
                elif "data" in data and isinstance(data["data"], dict):
                    products_list = data["data"].get("products", [])

            for product in products_list:
                product_id = product.get("product_id")
                category_url = product.get("category_url")
                if product_id and category_url:
                    category_url_mapping[product_id] = category_url

            print(f"‚úÖ ƒê√£ load {len(category_url_mapping)} category_url mappings t·ª´ products.json")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc products.json: {e}")

    # B∆∞·ªõc 1: Load t·ª´ cache folder (n·∫øu c√≥) - ƒë·∫ßy ƒë·ªß nh·∫•t
    cache_products = {}
    if cache_dir and cache_dir.exists():
        cache_products = load_products_from_cache(cache_dir)

    # B∆∞·ªõc 2: Load t·ª´ products_with_detail.json (n·∫øu c√≥) - ƒë·∫ßy ƒë·ªß
    products_with_detail = []
    if products_with_detail_file and products_with_detail_file.exists():
        print(f"üìñ ƒêang ƒë·ªçc products_with_detail t·ª´: {products_with_detail_file}")
        try:
            with open(products_with_detail_file, encoding="utf-8") as f:
                data = json.load(f)

            if isinstance(data, list):
                products_with_detail = data
            elif isinstance(data, dict) and "products" in data:
                products_with_detail = data["products"]

            print(f"‚úÖ ƒê√£ ƒë·ªçc {len(products_with_detail)} products t·ª´ products_with_detail.json")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc products_with_detail.json: {e}")

    # B∆∞·ªõc 3: Merge v√† lo·∫°i b·ªè duplicate (∆∞u ti√™n cache > products_with_detail)
    print("\nüîÑ ƒêang merge v√† lo·∫°i b·ªè duplicate...")
    merged_products = {}
    duplicate_count = 0
    seen_in_detail = set()  # ƒê·∫øm products unique t·ª´ products_with_detail

    # ∆Øu ti√™n 1: Cache (ƒë·∫ßy ƒë·ªß nh·∫•t) - m·ªói file cache l√† unique product_id
    for product_id, product in cache_products.items():
        # B·ªï sung category_url t·ª´ mapping n·∫øu ch∆∞a c√≥
        if not product.get("category_url") and product_id in category_url_mapping:
            product["category_url"] = category_url_mapping[product_id]
        merged_products[product_id] = product

    # ∆Øu ti√™n 2: products_with_detail (n·∫øu ch∆∞a c√≥ trong cache)
    # Lo·∫°i b·ªè duplicate trong c√πng list products_with_detail
    for product in products_with_detail:
        product_id = product.get("product_id")
        if not product_id:
            continue

        # N·∫øu ƒë√£ c√≥ trong cache, b·ªè qua
        if product_id in merged_products:
            duplicate_count += 1
            continue

        # N·∫øu ƒë√£ th·∫•y trong products_with_detail list, b·ªè qua (duplicate trong c√πng file)
        if product_id in seen_in_detail:
            duplicate_count += 1
            continue

        seen_in_detail.add(product_id)
        # B·ªï sung category_url t·ª´ mapping n·∫øu ch∆∞a c√≥
        if not product.get("category_url") and product_id in category_url_mapping:
            product["category_url"] = category_url_mapping[product_id]
        merged_products[product_id] = product

    products = list(merged_products.values())

    if duplicate_count > 0:
        print(f"   ‚ö†Ô∏è  ƒê√£ lo·∫°i b·ªè {duplicate_count} products duplicate")

    if not products:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y products n√†o c√≥ detail ƒë·ªÉ load")
        print("   üí° L∆∞u √Ω: products.json ch·ªâ ch·ª©a danh s√°ch c∆° b·∫£n, kh√¥ng c√≥ detail")
        print("   üí° C·∫ßn c√≥ d·ªØ li·ªáu t·ª´ cache folder ho·∫∑c products_with_detail.json")
        return {"skipped": True, "total_loaded": 0, "db_loaded": 0}

    print(f"‚úÖ T·ªïng h·ª£p: {len(products)} products unique c√≥ detail")
    print(f"   - T·ª´ cache: {len(cache_products)}")
    print(f"   - T·ª´ products_with_detail: {len(seen_in_detail)}")
    if duplicate_count > 0:
        print(f"   - ƒê√£ lo·∫°i b·ªè duplicate: {duplicate_count}")

    try:
        # Validate v√† chu·∫©n b·ªã products
        print("üîç ƒêang validate products...")
        valid_products = []
        invalid_count = 0

        for product in products:
            # Ki·ªÉm tra required fields
            if not product.get("product_id") and not product.get("url"):
                invalid_count += 1
                continue

            # Extract product_id t·ª´ URL n·∫øu ch∆∞a c√≥
            if not product.get("product_id") and product.get("url"):
                try:
                    from pipelines.crawl.utils import extract_product_id_from_url

                    product_id = extract_product_id_from_url(product["url"])
                    if product_id:
                        product["product_id"] = product_id
                    else:
                        invalid_count += 1
                        continue
                except Exception:
                    invalid_count += 1
                    continue

            # ƒê·∫£m b·∫£o c√≥ category_url (c√≥ th·ªÉ None)
            # N·∫øu ch∆∞a c√≥, th·ª≠ l·∫•y t·ª´ mapping ho·∫∑c ƒë·ªÉ None
            if "category_url" not in product or not product.get("category_url"):
                product_id = product.get("product_id")
                if product_id and product_id in category_url_mapping:
                    product["category_url"] = category_url_mapping[product_id]
                else:
                    product["category_url"] = None

            # Extract category_id t·ª´ category_url n·∫øu ch∆∞a c√≥
            if not product.get("category_id") and product.get("category_url"):
                try:
                    from pipelines.crawl.utils import extract_category_id_from_url

                    category_id = extract_category_id_from_url(product["category_url"])
                    if category_id:
                        product["category_id"] = category_id
                except Exception:
                    pass  # N·∫øu kh√¥ng import ƒë∆∞·ª£c, b·ªè qua

            # ƒê·∫£m b·∫£o category_path ƒë∆∞·ª£c gi·ªØ l·∫°i (n·∫øu c√≥ trong cache)
            # category_path ƒë√£ c√≥ s·∫µn t·ª´ cache, kh√¥ng c·∫ßn x·ª≠ l√Ω th√™m

            valid_products.append(product)

        print(f"‚úÖ ƒê√£ validate: {len(valid_products)} valid, {invalid_count} invalid")

        # Transform products t·ª´ nested format sang flat format cho database
        print("\nüîÑ ƒêang transform products (nested ‚Üí flat format)...")
        transformer = DataTransformer()
        transformed_products = []
        transform_failed = 0

        for product in valid_products:
            try:
                # Transform product (flatten nested dicts: price, rating, seller, stock)
                transformed = transformer.transform_product(product)
                if transformed:
                    transformed_products.append(transformed)
                else:
                    transform_failed += 1
            except Exception as e:
                transform_failed += 1
                if transform_failed <= 5:  # Ch·ªâ log 5 l·ªói ƒë·∫ßu ti√™n
                    print(
                        f"   ‚ö†Ô∏è  L·ªói transform product {product.get('product_id', 'unknown')}: {e}"
                    )

        if transform_failed > 0:
            print(f"   ‚ö†Ô∏è  C√≥ {transform_failed} products transform th·∫•t b·∫°i")
        print(f"‚úÖ ƒê√£ transform {len(transformed_products)} products th√†nh c√¥ng")

        if not transformed_products:
            print("‚ö†Ô∏è  Kh√¥ng c√≥ products n√†o ƒë·ªÉ load sau khi transform")
            return {"skipped": True, "total_loaded": 0, "db_loaded": 0}

        # Load v√†o database
        print("\nüíæ ƒêang load products v√†o database...")
        print("   üìå ƒê·∫£m b·∫£o kh√¥ng duplicate:")
        print("      - ƒê√£ lo·∫°i b·ªè duplicate trong memory (d·ª±a tr√™n product_id)")
        print("      - Database c√≥ UNIQUE constraint tr√™n product_id")
        print("      - S·ª≠ d·ª•ng UPSERT (ON CONFLICT UPDATE) ƒë·ªÉ update n·∫øu ƒë√£ t·ªìn t·∫°i")
        stats = loader.load_products(
            transformed_products,
            save_to_file=None,
            upsert=True,  # UPDATE n·∫øu product_id ƒë√£ t·ªìn t·∫°i (kh√¥ng t·∫°o duplicate)
            validate_before_load=False,  # ƒê√£ validate v√† transform ·ªü tr√™n
        )

        print(f"‚úÖ ƒê√£ load {stats['db_loaded']} products v√†o database")
        print(f"   - T·ªïng s·ªë: {stats['total_loaded']}")
        print(f"   - Th√†nh c√¥ng: {stats['success_count']}")
        print(f"   - Th·∫•t b·∫°i: {stats['failed_count']}")

        if stats.get("errors"):
            print(f"‚ö†Ô∏è  C√≥ {len(stats['errors'])} l·ªói (hi·ªÉn th·ªã 5 ƒë·∫ßu ti√™n):")
            for error in stats["errors"][:5]:
                print(f"   - {error}")

        return stats

    except Exception as e:
        print(f"‚ùå L·ªói khi load products: {e}")
        import traceback

        traceback.print_exc()
        return {"error": str(e), "total_loaded": 0, "db_loaded": 0}


def verify_data_links(loader: DataLoader) -> dict[str, Any]:
    """Verify li√™n k·∫øt gi·ªØa products v√† categories"""
    print("\n" + "=" * 70)
    print("üîó B∆Ø·ªöC 3: VERIFY DATA LINKS")
    print("=" * 70)

    try:
        # Ki·ªÉm tra connection
        if not loader.enable_db or not loader.db_storage:
            print("‚ö†Ô∏è  Database kh√¥ng available, b·ªè qua verification")
            return {"skipped": True}

        from pipelines.crawl.storage.postgres_storage import PostgresStorage

        storage: PostgresStorage = loader.db_storage

        # ƒê·∫øm categories
        with storage.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM categories")
                category_count = cur.fetchone()[0]

                # ƒê·∫øm products
                cur.execute("SELECT COUNT(*) FROM products")
                product_count = cur.fetchone()[0]

                # ƒê·∫øm products c√≥ category_url
                cur.execute("SELECT COUNT(*) FROM products WHERE category_url IS NOT NULL")
                products_with_category = cur.fetchone()[0]

                # ƒê·∫øm products c√≥ category_url match v·ªõi categories
                cur.execute(
                    """
                    SELECT COUNT(DISTINCT p.id)
                    FROM products p
                    INNER JOIN categories c ON p.category_url = c.url
                """
                )
                products_linked = cur.fetchone()[0]

                # ƒê·∫øm products c√≥ category_url nh∆∞ng kh√¥ng match
                cur.execute(
                    """
                    SELECT COUNT(DISTINCT p.id)
                    FROM products p
                    LEFT JOIN categories c ON p.category_url = c.url
                    WHERE p.category_url IS NOT NULL AND c.url IS NULL
                """
                )
                products_unlinked = cur.fetchone()[0]

                # L·∫•y sample c√°c category_url kh√¥ng match (ƒë·ªÉ debug)
                cur.execute(
                    """
                    SELECT DISTINCT p.category_url
                    FROM products p
                    LEFT JOIN categories c ON p.category_url = c.url
                    WHERE p.category_url IS NOT NULL AND c.url IS NULL
                    LIMIT 10
                """
                )
                unlinked_urls = [row[0] for row in cur.fetchall()]

                # Th·ªëng k√™ theo level categories
                cur.execute(
                    """
                    SELECT level, COUNT(*) as count
                    FROM categories
                    GROUP BY level
                    ORDER BY level
                """
                )
                categories_by_level = {row[0]: row[1] for row in cur.fetchall()}

        print("üìä Th·ªëng k√™:")
        print(f"   - T·ªïng s·ªë categories: {category_count}")
        if categories_by_level:
            print("   - Categories theo level:")
            for level in sorted(categories_by_level.keys()):
                print(f"     Level {level}: {categories_by_level[level]} categories")
        print(f"   - T·ªïng s·ªë products: {product_count}")
        print(f"   - Products c√≥ category_url: {products_with_category}")
        print(f"   - Products ƒë√£ link v·ªõi categories: {products_linked}")
        print(f"   - Products ch∆∞a link (category_url kh√¥ng t·ªìn t·∫°i): {products_unlinked}")

        if products_unlinked > 0:
            print(
                f"\n‚ö†Ô∏è  C√≥ {products_unlinked} products c√≥ category_url nh∆∞ng kh√¥ng t√¨m th·∫•y category t∆∞∆°ng ·ª©ng"
            )
            if unlinked_urls:
                print("   Sample category_urls kh√¥ng t√¨m th·∫•y (10 ƒë·∫ßu ti√™n):")
                for url in unlinked_urls[:5]:
                    print(f"     - {url}")
            print("   C√≥ th·ªÉ do:")
            print("   - Category ch∆∞a ƒë∆∞·ª£c load v√†o database")
            print("   - URL kh√¥ng kh·ªõp (c√≥ th·ªÉ do format kh√°c)")

        # T√≠nh t·ª∑ l·ªá link
        if products_with_category > 0:
            link_rate = (products_linked / products_with_category) * 100
            print(
                f"\n   üìà T·ª∑ l·ªá link: {link_rate:.2f}% ({products_linked}/{products_with_category})"
            )

        return {
            "category_count": category_count,
            "product_count": product_count,
            "products_with_category": products_with_category,
            "products_linked": products_linked,
            "products_unlinked": products_unlinked,
            "categories_by_level": categories_by_level,
            "unlinked_urls_sample": unlinked_urls,
        }

    except Exception as e:
        print(f"‚ùå L·ªói khi verify: {e}")
        import traceback

        traceback.print_exc()
        return {"error": str(e)}


def check_required_files() -> tuple[bool, list[str]]:
    """Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt c√≥ t·ªìn t·∫°i kh√¥ng"""
    missing_files = []

    # Ki·ªÉm tra folder data/raw
    data_raw = project_root / "data" / "raw"
    if not data_raw.exists():
        missing_files.append(f"Folder: {data_raw}")
        return False, missing_files

    # Ki·ªÉm tra categories_tree.json (kh√¥ng b·∫Øt bu·ªôc)
    tree_file = data_raw / "categories_tree.json"
    if not tree_file.exists():
        missing_files.append(f"File (optional): {tree_file}")

    # Ki·ªÉm tra folder products
    products_dir = data_raw / "products"
    if not products_dir.exists():
        missing_files.append(f"Folder: {products_dir}")
        return False, missing_files

    # Ki·ªÉm tra c√°c ngu·ªìn d·ªØ li·ªáu c√≥ detail (√≠t nh·∫•t 1 trong 2 ph·∫£i c√≥)
    cache_dir = products_dir / "detail" / "cache"
    products_with_detail_file = products_dir / "products_with_detail.json"

    has_cache = cache_dir.exists() and any(cache_dir.glob("*.json"))
    has_products_with_detail = products_with_detail_file.exists()

    if not (has_cache or has_products_with_detail):
        missing_files.append("√çt nh·∫•t m·ªôt trong c√°c ngu·ªìn sau (c√≥ detail ƒë·∫ßy ƒë·ªß):")
        missing_files.append(f"  - Cache folder: {cache_dir} (c√≥ file .json)")
        missing_files.append(f"  - File: {products_with_detail_file}")
        missing_files.append("")
        missing_files.append("  ‚ö†Ô∏è  L∆∞u √Ω: products.json ch·ªâ ch·ª©a danh s√°ch c∆° b·∫£n, kh√¥ng c√≥ detail")

    return (
        len(
            [
                f
                for f in missing_files
                if not f.startswith("  -") and not f.startswith("File (optional)")
            ]
        )
        == 0,
        missing_files,
    )


def main():
    """Main function E2E"""
    print("=" * 70)
    print("üöÄ E2E: LOAD EXISTING DATA TO DATABASE")
    print("=" * 70)
    print("\nScript n√†y s·∫Ω:")
    print("  1. Extract v√† load categories t·ª´ categories_tree.json")
    print("  2. Load products c√≥ detail ƒë·∫ßy ƒë·ªß (t·ª´ cache folder ho·∫∑c products_with_detail.json)")
    print("  3. Verify li√™n k·∫øt gi·ªØa products v√† categories")
    print("\nüìå L∆∞u √Ω:")
    print("   - Ch·ªâ load d·ªØ li·ªáu c√≥ detail ƒë·∫ßy ƒë·ªß (t·ª´ cache ho·∫∑c products_with_detail.json)")
    print("   - products.json ch·ªâ ch·ª©a danh s√°ch c∆° b·∫£n, kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng")
    print("   - D·ªØ li·ªáu t·ª´ cache folder s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n n·∫øu c√≥")

    # Ki·ªÉm tra files c·∫ßn thi·∫øt
    print("\nüîç Ki·ªÉm tra files c·∫ßn thi·∫øt...")
    files_ok, missing_files = check_required_files()

    if not files_ok:
        print("‚ùå Thi·∫øu c√°c file/folder sau:")
        for file in missing_files:
            print(f"   - {file}")
        print("\nüí° T·∫°o c√°c folder c·∫ßn thi·∫øt...")

        # T·∫°o c√°c folder n·∫øu ch∆∞a c√≥
        (project_root / "data" / "raw").mkdir(parents=True, exist_ok=True)
        (project_root / "data" / "raw" / "products").mkdir(parents=True, exist_ok=True)

        print("‚úÖ ƒê√£ t·∫°o c√°c folder c·∫ßn thi·∫øt")
        print("‚ö†Ô∏è  Vui l√≤ng ƒë·∫£m b·∫£o √≠t nh·∫•t m·ªôt trong c√°c ngu·ªìn sau t·ªìn t·∫°i:")
        print("   - data/raw/products/detail/cache/*.json (c√≥ file .json)")
        print("   - data/raw/products/products_with_detail.json")
        print("\n   L∆∞u √Ω: products.json ch·ªâ ch·ª©a danh s√°ch c∆° b·∫£n, kh√¥ng c√≥ detail")
        return 1

    print("‚úÖ T·∫•t c·∫£ files c·∫ßn thi·∫øt ƒë·ªÅu t·ªìn t·∫°i")

    # ƒê∆∞·ªùng d·∫´n files
    tree_file = project_root / "data" / "raw" / "categories_tree.json"
    products_dir = project_root / "data" / "raw" / "products"
    products_file = products_dir / "products.json"  # ƒê·ªÉ l·∫•y category_url mapping
    products_with_detail_file = products_dir / "products_with_detail.json"
    cache_dir = products_dir / "detail" / "cache"

    # Kh·ªüi t·∫°o DataLoader
    print("\nüîå ƒêang k·∫øt n·ªëi database...")

    # L·∫•y credentials t·ª´ environment ho·∫∑c .env file
    postgres_host = os.getenv("POSTGRES_HOST", "localhost")
    postgres_port = int(os.getenv("POSTGRES_PORT", "5432"))
    postgres_user = os.getenv("POSTGRES_USER", "airflow_user")
    postgres_password = os.getenv("POSTGRES_PASSWORD", "")
    postgres_db = os.getenv("POSTGRES_DB", "crawl_data")

    # Th·ª≠ ƒë·ªçc t·ª´ .env file n·∫øu c√≥
    env_file = project_root / ".env"
    if env_file.exists():
        print(f"üìÑ ƒêang ƒë·ªçc .env t·ª´: {env_file}")
        try:
            # Th·ª≠ d√πng python-dotenv n·∫øu c√≥
            try:
                from dotenv import load_dotenv

                load_dotenv(env_file, override=True)
                postgres_host = os.getenv("POSTGRES_HOST", postgres_host)
                postgres_port = int(os.getenv("POSTGRES_PORT", postgres_port))
                postgres_user = os.getenv("POSTGRES_USER", postgres_user)
                postgres_password = os.getenv("POSTGRES_PASSWORD", postgres_password)
                postgres_db = os.getenv("POSTGRES_DB", postgres_db)
                print("‚úÖ ƒê√£ load .env b·∫±ng python-dotenv")
            except ImportError:
                # Fallback: ƒë·ªçc th·ªß c√¥ng
                with open(env_file, encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#"):
                            if "=" in line:
                                key, value = line.split("=", 1)
                                key = key.strip()
                                value = value.strip().strip('"').strip("'")
                                if key == "POSTGRES_PASSWORD":
                                    postgres_password = value
                                elif key == "POSTGRES_HOST":
                                    postgres_host = value
                                elif key == "POSTGRES_USER":
                                    postgres_user = value
                                elif key == "POSTGRES_DB":
                                    postgres_db = value
                                elif key == "POSTGRES_PORT":
                                    try:
                                        postgres_port = int(value)
                                    except ValueError:
                                        pass
                print("‚úÖ ƒê√£ load .env th·ªß c√¥ng")
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë·ªçc .env: {e}")

    # Hi·ªÉn th·ªã th√¥ng tin k·∫øt n·ªëi (·∫©n password)
    print("\nüìã Th√¥ng tin k·∫øt n·ªëi:")
    print(f"   - Host: {postgres_host}")
    print(f"   - Port: {postgres_port}")
    print(f"   - User: {postgres_user}")
    print(f"   - Database: {postgres_db}")
    print(f"   - Password: {'***' if postgres_password else '(ch∆∞a set)'}")

    loader = DataLoader(
        database=postgres_db,
        host=postgres_host,
        port=postgres_port,
        user=postgres_user,
        password=postgres_password,
        batch_size=100,
        enable_db=True,
    )

    if not loader.enable_db:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database!")
        print("\nüí° H∆∞·ªõng d·∫´n:")
        print("   1. ƒê·∫£m b·∫£o PostgreSQL ƒëang ch·∫°y")
        print("   2. Set environment variables ho·∫∑c t·∫°o file .env:")
        print("      POSTGRES_HOST=localhost")
        print("      POSTGRES_PORT=5432")
        print("      POSTGRES_USER=airflow_user")
        print("      POSTGRES_PASSWORD=your_password")
        print("      POSTGRES_DB=crawl_data")
        print("\n   3. Ho·∫∑c ch·∫°y trong Docker v·ªõi:")
        print("      docker-compose up -d postgres")
        print("\n‚ö†Ô∏è  Script s·∫Ω ch·ªâ validate d·ªØ li·ªáu, kh√¥ng load v√†o database")

        # Ch·∫°y ·ªü ch·∫ø ƒë·ªô validate only
        print("\n" + "=" * 70)
        print("üìã VALIDATE MODE (Kh√¥ng c√≥ database)")
        print("=" * 70)

        # Validate categories
        try:
            categories = extract_categories_from_tree_file(tree_file)
            print(f"‚úÖ Categories: {len(categories)} categories h·ª£p l·ªá")
        except Exception as e:
            print(f"‚ùå L·ªói validate categories: {e}")

        # Validate products (ch·ªâ t·ª´ cache ho·∫∑c products_with_detail)
        products_count = 0
        cache_products = {}
        try:
            # Th·ª≠ load t·ª´ cache
            if cache_dir.exists():
                cache_products = load_products_from_cache(cache_dir)
                products_count += len(cache_products)
                if cache_products:
                    print(f"‚úÖ Products t·ª´ cache: {len(cache_products)} products h·ª£p l·ªá")

            # Th·ª≠ load t·ª´ products_with_detail
            if products_with_detail_file.exists():
                with open(products_with_detail_file, encoding="utf-8") as f:
                    data = json.load(f)
                products_detail = data.get("products", []) if isinstance(data, dict) else data
                # ƒê·∫øm products ch∆∞a c√≥ trong cache
                cache_ids = set(cache_products.keys())
                new_products = [p for p in products_detail if p.get("product_id") not in cache_ids]
                products_count += len(new_products)
                if new_products:
                    print(
                        f"‚úÖ Products t·ª´ products_with_detail: {len(new_products)} products h·ª£p l·ªá (ch∆∞a c√≥ trong cache)"
                    )

            if products_count == 0:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y products c√≥ detail ƒë·ªÉ validate")
                print("   üí° C·∫ßn c√≥ d·ªØ li·ªáu t·ª´ cache folder ho·∫∑c products_with_detail.json")
            else:
                print(f"‚úÖ T·ªïng c·ªông: {products_count} products c√≥ detail h·ª£p l·ªá")
        except Exception as e:
            print(f"‚ùå L·ªói validate products: {e}")

        return 1

    print("‚úÖ ƒê√£ k·∫øt n·ªëi database")

    try:
        # B∆∞·ªõc 1: Load categories
        categories_stats = load_categories_e2e(loader, tree_file)

        # B∆∞·ªõc 2: Load products c√≥ detail (t·ª´ cache ho·∫∑c products_with_detail.json)
        # Truy·ªÅn products_file ƒë·ªÉ l·∫•y category_url mapping
        products_stats = load_products_e2e(
            loader,
            cache_dir=cache_dir,
            products_with_detail_file=products_with_detail_file,
            products_file=products_file,
        )

        # B∆∞·ªõc 3: Verify links
        verify_stats = verify_data_links(loader)

        # T·ªïng k·∫øt
        print("\n" + "=" * 70)
        print("üìä T·ªîNG K·∫æT")
        print("=" * 70)
        print("\nCategories:")
        print(f"  - ƒê√£ load: {categories_stats.get('db_loaded', 0)}")
        print(f"  - Th√†nh c√¥ng: {categories_stats.get('success_count', 0)}")
        print(f"  - Th·∫•t b·∫°i: {categories_stats.get('failed_count', 0)}")

        print("\nProducts:")
        print(f"  - ƒê√£ load: {products_stats.get('db_loaded', 0)}")
        print(f"  - Th√†nh c√¥ng: {products_stats.get('success_count', 0)}")
        print(f"  - Th·∫•t b·∫°i: {products_stats.get('failed_count', 0)}")

        if verify_stats and not verify_stats.get("skipped"):
            print("\nLinks:")
            print(f"  - Categories: {verify_stats.get('category_count', 0)}")
            print(f"  - Products: {verify_stats.get('product_count', 0)}")
            print(f"  - Products linked: {verify_stats.get('products_linked', 0)}")
            print(f"  - Products unlinked: {verify_stats.get('products_unlinked', 0)}")

        print("\n‚úÖ HO√ÄN TH√ÄNH!")
        return 0

    except Exception as e:
        print(f"\n‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
        import traceback

        traceback.print_exc()
        return 1

    finally:
        loader.close()
        print("\nüîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi database")


if __name__ == "__main__":
    sys.exit(main())
