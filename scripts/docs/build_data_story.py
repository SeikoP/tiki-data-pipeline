"""
Script ƒë·ªÉ ph√¢n t√≠ch database v√† x√¢y d·ª±ng c√¢u chuy·ªán d·ªØ li·ªáu (Data Story)
T·∫°o file docx ch·ª©a c√¢u chuy·ªán d·ªØ li·ªáu c·ªßa d·ª± √°n Tiki Data Pipeline
"""

import os
import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("‚ùå C·∫ßn c√†i ƒë·∫∑t psycopg2-binary: pip install psycopg2-binary")
    sys.exit(1)

try:
    from docx import Document
    from docx.shared import Inches, Pt, RGBColor
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml.ns import qn
except ImportError:
    print("‚ùå C·∫ßn c√†i ƒë·∫∑t python-docx: pip install python-docx")
    sys.exit(1)

# Google Drive API (optional)
try:
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    from google.auth.transport.requests import Request
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaFileUpload
    import pickle
    GOOGLE_DRIVE_AVAILABLE = True
except ImportError:
    GOOGLE_DRIVE_AVAILABLE = False

# Th√™m src v√†o path ƒë·ªÉ import modules
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# ƒê·ªçc th√¥ng tin t·ª´ .env
def load_env_config():
    """ƒê·ªçc c·∫•u h√¨nh t·ª´ file .env"""
    env_file = PROJECT_ROOT / ".env"
    config = {
        "host": "localhost",
        "port": 5432,
        "database": "crawl_data",
        "user": "postgres",
        "password": "postgres",
    }
    
    if env_file.exists():
        with open(env_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    
                    if key == "POSTGRES_HOST":
                        config["host"] = value
                    elif key == "POSTGRES_PORT":
                        config["port"] = int(value)
                    elif key == "POSTGRES_USER":
                        config["user"] = value
                    elif key == "POSTGRES_PASSWORD":
                        config["password"] = value
                    elif key == "POSTGRES_DB" and value == "crawl_data":
                        config["database"] = value
    
    # Override v·ªõi environment variables n·∫øu c√≥
    config["host"] = os.getenv("POSTGRES_HOST", config["host"])
    config["port"] = int(os.getenv("POSTGRES_PORT", config["port"]))
    config["user"] = os.getenv("POSTGRES_USER", config["user"])
    config["password"] = os.getenv("POSTGRES_PASSWORD", config["password"])
    config["database"] = os.getenv("POSTGRES_DB", config["database"])
    
    return config


def connect_database(config: dict[str, Any]):
    """K·∫øt n·ªëi ƒë·∫øn PostgreSQL database"""
    try:
        conn = psycopg2.connect(
            host=config["host"],
            port=config["port"],
            database=config["database"],
            user=config["user"],
            password=config["password"],
            connect_timeout=10,
        )
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi ƒë·∫øn database: {config['database']}")
        return conn
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        sys.exit(1)


def analyze_database(conn) -> dict[str, Any]:
    """Ph√¢n t√≠ch d·ªØ li·ªáu trong database"""
    stats = {}
    
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        # 1. T·ªïng quan
        print("üìä ƒêang ph√¢n t√≠ch t·ªïng quan...")
        cur.execute("""
            SELECT 
                (SELECT COUNT(*) FROM categories) as total_categories,
                (SELECT COUNT(*) FROM products) as total_products,
                (SELECT COUNT(DISTINCT category_url) FROM products WHERE category_url IS NOT NULL) as categories_with_products,
                (SELECT COUNT(DISTINCT brand) FROM products WHERE brand IS NOT NULL) as total_brands,
                (SELECT COUNT(CASE WHEN sales_count > 0 THEN 1 END) FROM products WHERE sales_count IS NOT NULL) as products_sold,
                (SELECT COUNT(CASE WHEN sales_count > 1000 THEN 1 END) FROM products WHERE sales_count IS NOT NULL) as bestsellers
        """)
        stats["overview"] = dict(cur.fetchone())
        
        # 2. Th·ªëng k√™ categories
        print("üìÅ ƒêang ph√¢n t√≠ch categories...")
        cur.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT level) as distinct_levels,
                MIN(level) as min_level,
                MAX(level) as max_level,
                AVG(product_count) as avg_products_per_category,
                SUM(product_count) as total_product_count_in_categories
            FROM categories
        """)
        stats["categories"] = dict(cur.fetchone())
        
        # 3. Th·ªëng k√™ products
        print("üõçÔ∏è ƒêang ph√¢n t√≠ch products...")
        cur.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT category_url) as categories_covered,
                COUNT(DISTINCT brand) as distinct_brands,
                COUNT(DISTINCT seller_id) as distinct_sellers,
                COUNT(CASE WHEN seller_is_official = TRUE THEN 1 END) as official_seller_products,
                AVG(sales_count) as avg_sales_count,
                MAX(sales_count) as max_sales_count,
                MIN(sales_count) as min_sales_count,
                AVG(price) as avg_price,
                MAX(price) as max_price,
                MIN(price) as min_price,
                AVG(rating_average) as avg_rating,
                AVG(review_count) as avg_reviews,
                COUNT(CASE WHEN stock_available = TRUE THEN 1 END) as in_stock_count,
                COUNT(CASE WHEN discount_percent > 0 THEN 1 END) as discounted_products,
                AVG(discount_percent) as avg_discount_percent
            FROM products
        """)
        stats["products"] = dict(cur.fetchone())
        
        # 4. Top categories theo s·ªë l∆∞·ª£ng s·∫£n ph·∫©m
        print("üèÜ ƒêang l·∫•y top categories...")
        cur.execute("""
            SELECT 
                c.name,
                c.level,
                COUNT(p.id) as product_count,
                AVG(p.price) as avg_price,
                AVG(p.sales_count) as avg_sales
            FROM categories c
            LEFT JOIN products p ON c.url = p.category_url
            GROUP BY c.id, c.name, c.level
            HAVING COUNT(p.id) > 0
            ORDER BY product_count DESC
            LIMIT 10
        """)
        stats["top_categories"] = [dict(row) for row in cur.fetchall()]
        
        # 5. Top products theo sales_count
        print("‚≠ê ƒêang l·∫•y top products...")
        cur.execute("""
            SELECT 
                product_id,
                name,
                price,
                sales_count,
                rating_average,
                review_count,
                brand,
                seller_name
            FROM products
            WHERE sales_count IS NOT NULL
            ORDER BY sales_count DESC
            LIMIT 10
        """)
        stats["top_products"] = [dict(row) for row in cur.fetchall()]
        
        # 6. Ph√¢n b·ªë gi√°
        print("üí∞ ƒêang ph√¢n t√≠ch ph√¢n b·ªë gi√°...")
        cur.execute("""
            SELECT 
                CASE 
                    WHEN price < 100000 THEN 'D∆∞·ªõi 100k'
                    WHEN price < 500000 THEN '100k - 500k'
                    WHEN price < 1000000 THEN '500k - 1M'
                    WHEN price < 5000000 THEN '1M - 5M'
                    ELSE 'Tr√™n 5M'
                END as price_range,
                COUNT(*) as count,
                AVG(price) as avg_price
            FROM products
            WHERE price IS NOT NULL
            GROUP BY price_range
            ORDER BY MIN(price)
        """)
        stats["price_distribution"] = [dict(row) for row in cur.fetchall()]
        
        # 7. Ph√¢n b·ªë rating
        print("‚≠ê ƒêang ph√¢n t√≠ch rating...")
        cur.execute("""
            SELECT 
                CASE 
                    WHEN rating_average IS NULL THEN 'Ch∆∞a c√≥ rating'
                    WHEN rating_average < 3.0 THEN 'D∆∞·ªõi 3.0'
                    WHEN rating_average < 4.0 THEN '3.0 - 4.0'
                    WHEN rating_average < 4.5 THEN '4.0 - 4.5'
                    ELSE 'Tr√™n 4.5'
                END as rating_range,
                COUNT(*) as count
            FROM products
            GROUP BY rating_range
            ORDER BY MIN(COALESCE(rating_average, 0))
        """)
        stats["rating_distribution"] = [dict(row) for row in cur.fetchall()]
        
        # 8. Th·ªëng k√™ theo brand
        print("üè∑Ô∏è ƒêang ph√¢n t√≠ch brands...")
        cur.execute("""
            SELECT 
                brand,
                COUNT(*) as product_count,
                AVG(price) as avg_price,
                AVG(sales_count) as avg_sales,
                AVG(rating_average) as avg_rating
            FROM products
            WHERE brand IS NOT NULL
            GROUP BY brand
            ORDER BY product_count DESC
            LIMIT 10
        """)
        stats["top_brands"] = [dict(row) for row in cur.fetchall()]
        
        # 9. Ph√¢n t√≠ch computed fields
        print("üìà ƒêang ph√¢n t√≠ch computed fields...")
        cur.execute("""
            SELECT 
                AVG(estimated_revenue) as avg_revenue,
                SUM(estimated_revenue) as total_revenue,
                AVG(popularity_score) as avg_popularity,
                MAX(popularity_score) as max_popularity,
                AVG(value_score) as avg_value_score,
                MAX(value_score) as max_value_score,
                COUNT(CASE WHEN price_category = 'budget' THEN 1 END) as budget_count,
                COUNT(CASE WHEN price_category = 'mid-range' THEN 1 END) as midrange_count,
                COUNT(CASE WHEN price_category = 'premium' THEN 1 END) as premium_count,
                COUNT(CASE WHEN price_category = 'luxury' THEN 1 END) as luxury_count
            FROM products
            WHERE estimated_revenue IS NOT NULL
        """)
        stats["computed_fields"] = dict(cur.fetchone())
        
        # 10. M·ªëi quan h·ªá gi·ªØa gi√° v√† doanh s·ªë
        print("üîó ƒêang ph√¢n t√≠ch m·ªëi quan h·ªá gi√°-doanh s·ªë...")
        cur.execute("""
            SELECT 
                CASE 
                    WHEN price < 100000 THEN 'D∆∞·ªõi 100k'
                    WHEN price < 500000 THEN '100k - 500k'
                    WHEN price < 1000000 THEN '500k - 1M'
                    WHEN price < 5000000 THEN '1M - 5M'
                    ELSE 'Tr√™n 5M'
                END as price_range,
                AVG(sales_count) as avg_sales,
                AVG(rating_average) as avg_rating,
                COUNT(*) as product_count
            FROM products
            WHERE price IS NOT NULL AND sales_count IS NOT NULL
            GROUP BY price_range
            ORDER BY MIN(price)
        """)
        stats["price_sales_relationship"] = [dict(row) for row in cur.fetchall()]
        
        # 11. M·ªëi quan h·ªá gi·ªØa discount v√† sales (lo·∫°i b·ªè truy v·∫•n l·∫∑p)
        print("üí∞ ƒêang ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa discount...")
        cur.execute("""
            SELECT 
                CASE 
                    WHEN discount_percent = 0 OR discount_percent IS NULL THEN 'Kh√¥ng gi·∫£m gi√°'
                    WHEN discount_percent < 10 THEN 'Gi·∫£m d∆∞·ªõi 10%'
                    WHEN discount_percent < 20 THEN 'Gi·∫£m 10-20%'
                    WHEN discount_percent < 30 THEN 'Gi·∫£m 20-30%'
                    ELSE 'Gi·∫£m tr√™n 30%'
                END as discount_range,
                AVG(sales_count) as avg_sales,
                COUNT(*) as product_count,
                AVG(rating_average) as avg_rating
            FROM products
            WHERE sales_count IS NOT NULL
            GROUP BY discount_range
            ORDER BY MIN(COALESCE(discount_percent, 0))
        """)
        stats["discount_impact"] = [dict(row) for row in cur.fetchall()]
        
        # 12. Top products theo computed fields
        print("‚≠ê ƒêang l·∫•y top products theo computed fields...")
        cur.execute("""
            SELECT 
                product_id,
                name,
                price,
                sales_count,
                rating_average,
                popularity_score,
                value_score,
                estimated_revenue,
                discount_percent
            FROM products
            WHERE popularity_score IS NOT NULL
            ORDER BY popularity_score DESC
            LIMIT 5
        """)
        stats["top_by_popularity"] = [dict(row) for row in cur.fetchall()]
        
        cur.execute("""
            SELECT 
                product_id,
                name,
                price,
                sales_count,
                rating_average,
                popularity_score,
                value_score,
                estimated_revenue
            FROM products
            WHERE value_score IS NOT NULL
            ORDER BY value_score DESC
            LIMIT 5
        """)
        stats["top_by_value"] = [dict(row) for row in cur.fetchall()]
        
        # 13. Ph√¢n t√≠ch s·ª± ph√¢n b·ªë sales
        print("üìà ƒêang ph√¢n t√≠ch ph√¢n b·ªë doanh s·ªë...")
        cur.execute("""
            SELECT 
                COUNT(CASE WHEN sales_count = 0 THEN 1 END) as no_sales,
                COUNT(CASE WHEN sales_count > 0 AND sales_count <= 100 THEN 1 END) as low_sales,
                COUNT(CASE WHEN sales_count > 100 AND sales_count <= 500 THEN 1 END) as medium_sales,
                COUNT(CASE WHEN sales_count > 500 AND sales_count <= 1000 THEN 1 END) as high_sales,
                COUNT(CASE WHEN sales_count > 1000 THEN 1 END) as bestsellers,
                MAX(sales_count) as max_sales,
                PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY sales_count) as median_sales,
                AVG(sales_count) as avg_sales
            FROM products
            WHERE sales_count IS NOT NULL
        """)
        stats["sales_distribution"] = dict(cur.fetchone())
        
        # 14. Top 5 categories t·ª´ products
        print("üèÜ ƒêang l·∫•y top categories theo doanh s·ªë...")
        cur.execute("""
            SELECT 
                category_url,
                COUNT(*) as product_count,
                SUM(sales_count) as total_sales,
                AVG(price) as avg_price,
                AVG(rating_average) as avg_rating,
                AVG(discount_percent) as avg_discount
            FROM products
            WHERE category_url IS NOT NULL
            GROUP BY category_url
            ORDER BY total_sales DESC
            LIMIT 10
        """)
        stats["top_categories_by_sales"] = [dict(row) for row in cur.fetchall()]
        
        # 15. Ph√¢n t√≠ch official sellers
        print("üëë ƒêang ph√¢n t√≠ch official sellers...")
        cur.execute("""
            SELECT 
                COUNT(CASE WHEN seller_is_official = TRUE THEN 1 END) as official_products,
                COUNT(CASE WHEN seller_is_official = FALSE THEN 1 END) as third_party_products,
                AVG(CASE WHEN seller_is_official = TRUE THEN sales_count END) as avg_sales_official,
                AVG(CASE WHEN seller_is_official = FALSE THEN sales_count END) as avg_sales_third_party,
                AVG(CASE WHEN seller_is_official = TRUE THEN rating_average END) as avg_rating_official,
                AVG(CASE WHEN seller_is_official = FALSE THEN rating_average END) as avg_rating_third_party
            FROM products
        """)
        stats["official_analysis"] = dict(cur.fetchone())
        
        # 16. Ph√¢n t√≠ch stock
        print("üì¶ ƒêang ph√¢n t√≠ch stock...")
        cur.execute("""
            SELECT 
                COUNT(CASE WHEN stock_available = TRUE THEN 1 END) as in_stock,
                COUNT(CASE WHEN stock_available = FALSE THEN 1 END) as out_of_stock,
                AVG(CASE WHEN stock_available = TRUE THEN sales_count END) as avg_sales_in_stock,
                AVG(CASE WHEN stock_available = FALSE THEN sales_count END) as avg_sales_out_of_stock
            FROM products
        """)
        stats["stock_analysis"] = dict(cur.fetchone())
    
    return stats


def safe_format(value: Any, format_str: str = ",.0f") -> str:
    """Format s·ªë an to√†n, x·ª≠ l√Ω None"""
    if value is None:
        return "N/A"
    try:
        return f"{value:{format_str}}"
    except (ValueError, TypeError):
        return str(value) if value else "N/A"


def upload_to_google_drive(file_path: Path, folder_id: Optional[str] = None) -> Optional[str]:
    """Upload file l√™n Google Drive
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file c·∫ßn upload
        folder_id: ID c·ªßa folder tr√™n Google Drive (optional)
    
    Returns:
        File ID n·∫øu upload th√†nh c√¥ng, None n·∫øu th·∫•t b·∫°i
    """
    if not GOOGLE_DRIVE_AVAILABLE:
        print("\n‚ùå Google Drive API kh√¥ng kh·∫£ d·ª•ng")
        print("üí° C√†i ƒë·∫∑t: pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client")
        return None
    
    # ƒê∆∞·ªùng d·∫´n file credentials
    credentials_file = PROJECT_ROOT /"docs"/ "credentials" / "google_drive_credentials.json"
    token_file = PROJECT_ROOT / "docs" / "credentials" / "token.pickle"
    
    if not credentials_file.exists():
        print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y credentials file: {credentials_file}")
        print("üí° T·∫°o credentials t·∫°i: https://console.cloud.google.com/apis/credentials")
        print("   - T·∫°o OAuth 2.0 Client ID (Desktop app)")
        print("   - Download JSON v√† l∆∞u v√†o credentials/google_drive_credentials.json")
        return None
    
    creds = None
    
    # Load token n·∫øu ƒë√£ c√≥
    if token_file.exists():
        with open(token_file, 'rb') as token:
            creds = pickle.load(token)
    
    # N·∫øu kh√¥ng c√≥ credentials h·ª£p l·ªá, y√™u c·∫ßu login
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            print("üîÑ ƒêang refresh token...")
            creds.refresh(Request())
        else:
            print("üîê ƒêang m·ªü browser ƒë·ªÉ x√°c th·ª±c Google Drive...")
            flow = InstalledAppFlow.from_client_secrets_file(
                str(credentials_file),
                scopes=['https://www.googleapis.com/auth/drive.file']
            )
            creds = flow.run_local_server(port=0)
        
        # L∆∞u token
        token_file.parent.mkdir(exist_ok=True)
        with open(token_file, 'wb') as token:
            pickle.dump(creds, token)
        print("‚úÖ ƒê√£ l∆∞u token")
    
    try:
        # T·∫°o service
        service = build('drive', 'v3', credentials=creds)
        
        # Metadata file
        file_metadata = {
            'name': file_path.name,
        }
        
        if folder_id:
            file_metadata['parents'] = [folder_id]
        
        # Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
        query = f"name='{file_path.name}'"
        if folder_id:
            query += f" and '{folder_id}' in parents"
        query += " and trashed=false"
        
        results = service.files().list(
            q=query,
            spaces='drive',
            fields='files(id, name)'
        ).execute()
        
        existing_files = results.get('files', [])
        
        media = MediaFileUpload(str(file_path), mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document')
        
        if existing_files:
            # Update file hi·ªán t·∫°i
            file_id = existing_files[0]['id']
            print(f"\nüîÑ ƒêang c·∫≠p nh·∫≠t file tr√™n Google Drive...")
            file = service.files().update(
                fileId=file_id,
                media_body=media
            ).execute()
            print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t file: {file.get('name')}")
        else:
            # T·∫°o file m·ªõi
            print(f"\nüì§ ƒêang upload file l√™n Google Drive...")
            file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id, name, webViewLink'
            ).execute()
            print(f"‚úÖ ƒê√£ upload file: {file.get('name')}")
        
        # L·∫•y link
        file_id = file.get('id')
        file_link = service.files().get(
            fileId=file_id,
            fields='webViewLink'
        ).execute()
        
        print(f"üîó Link: {file_link.get('webViewLink')}")
        return file_id
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi upload l√™n Google Drive: {e}")
        import traceback
        traceback.print_exc()
        return None


def create_document(stats: dict[str, Any], output_path: Path):
    """T·∫°o file docx v·ªõi c√¢u chuy·ªán d·ªØ li·ªáu"""
    doc = Document()
    
    # C·∫•u h√¨nh font cho ti·∫øng Vi·ªát
    def set_vietnamese_font(run):
        run.font.name = "Times New Roman"
        run._element.rPr.rFonts.set(qn("w:eastAsia"), "Times New Roman")
    
    # Helper: t·∫°o ti√™u ƒë·ªÅ m·ª•c d·∫°ng bullet (indent 0.25")
    def add_section_title(text: str):
        p = doc.add_paragraph(text, style="List Bullet")
        p.paragraph_format.left_indent = Inches(0.25)
        for run in p.runs:
            set_vietnamese_font(run)
            run.bold = True
            run.font.size = Pt(12)
        return p
    
    # Helper: ƒë·ªãnh d·∫°ng ƒëo·∫°n n·ªôi dung th√†nh sub-bullet (indent 0.5")
    def format_as_subbullet(paragraph):
        paragraph.style = doc.styles["List Bullet 2"]
        paragraph.paragraph_format.left_indent = Inches(0.5)
        for run in paragraph.runs:
            set_vietnamese_font(run)
            if run.font.size is None:
                run.font.size = Pt(12)
    
    # Helper: t·∫°o sub-bullet nhanh t·ª´ text
    def add_subbullet_text(text: str, bold: bool = False):
        p = doc.add_paragraph(style="List Bullet 2")
        p.paragraph_format.left_indent = Inches(0.5)
        run = p.add_run(text)
        set_vietnamese_font(run)
        run.font.size = Pt(12)
        if bold:
            run.bold = True
        return p
    
    # Helper: spacer d√≤ng tr·ªëng c√≥ ki·ªÉm so√°t
    def add_spacer(lines: int = 1):
        for _ in range(lines):
            doc.add_paragraph()
    
    # Title
    title = doc.add_heading("C√¢u Chuy·ªán D·ªØ Li·ªáu - Tiki Data Pipeline", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in title.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(20)
        run.font.color.rgb = RGBColor(0, 51, 102)
    
    # Subtitle
    subtitle = doc.add_paragraph(f"Ng√†y c·∫≠p nh·∫≠t: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in subtitle.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(12)
        run.font.color.rgb = RGBColor(102, 102, 102)
    
    doc.add_paragraph()  # Spacing
    
    # ============================================
    # PH·∫¶N B·ªêI C·∫¢NH V√Ä GI·ªöI THI·ªÜU
    # ============================================
    
    doc.add_heading("B·ªëi C·∫£nh: Th·ªã Tr∆∞·ªùng Th∆∞∆°ng M·∫°i ƒêi·ªán T·ª≠ Vi·ªát Nam", 1)
    
    # B·ªëi c·∫£nh th·ªã tr∆∞·ªùng
    add_subbullet_text(
        "Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Vi·ªát Nam ƒëang ph√°t tri·ªÉn m·∫°nh m·∫Ω, d·ª± ki·∫øn ƒë·∫°t 49 t·ª∑ USD v√†o nƒÉm 2025. Tiki.vn l√† m·ªôt trong nh·ªØng n·ªÅn t·∫£ng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ h√†ng ƒë·∫ßu, ƒë∆∞·ª£c th√†nh l·∫≠p t·ª´ nƒÉm 2010 v·ªõi h√†ng tri·ªáu s·∫£n ph·∫©m ƒëa d·∫°ng. D·ªØ li·ªáu t·ª´ Tiki.vn ph·∫£n √°nh xu h∆∞·ªõng mua s·∫Øm, h√†nh vi ti√™u d√πng v√† c·∫•u tr√∫c th·ªã tr∆∞·ªùng, c√≥ gi√° tr·ªã nghi√™n c·ª©u cao."
    )
    
    doc.add_paragraph()  # Spacing
    
    # √ù nghƒ©a c·ªßa dataset
    add_subbullet_text(
        "Dataset n√†y kh√¥ng ch·ªâ l√† danh s√°ch s·∫£n ph·∫©m, m√† l√† c·ª≠a s·ªï ƒë·ªÉ hi·ªÉu v·ªÅ th·ªã tr∆∞·ªùng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Vi·ªát Nam. T·ª´ dataset c√≥ th·ªÉ kh√°m ph√°: xu h∆∞·ªõng ti√™u d√πng, c·∫•u tr√∫c th·ªã tr∆∞·ªùng, h√†nh vi mua s·∫Øm, s·ª± c·∫°nh tranh gi·ªØa c√°c th∆∞∆°ng hi·ªáu, v√† gi√° tr·ªã th·ªã tr∆∞·ªùng."
    )
    
    doc.add_paragraph()  # Spacing
    
    # L√Ω do ch·ªçn ƒë·ªÅ t√†i
    doc.add_heading("L√Ω Do Ch·ªçn ƒê·ªÅ T√†i", 1)
    
    add_subbullet_text("Vi·ªác x√¢y d·ª±ng dataset v√† ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ Tiki.vn ƒë∆∞·ª£c l·ª±a ch·ªçn d·ª±a tr√™n nh·ªØng l√Ω do sau:")
    
    reasons = [
        ("T·∫ßm quan tr·ªçng c·ªßa th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", 
         "Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ ƒëang l√† xu h∆∞·ªõng t·∫•t y·∫øu c·ªßa th·ªùi ƒë·∫°i s·ªë. Vi·ªác hi·ªÉu r√µ th·ªã tr∆∞·ªùng n√†y kh√¥ng ch·ªâ quan tr·ªçng ƒë·ªëi v·ªõi c√°c doanh nghi·ªáp, m√† c√≤n c√≥ gi√° tr·ªã nghi√™n c·ª©u v√† h·ªçc thu·∫≠t cao. D·ªØ li·ªáu t·ª´ Tiki.vn cung c·∫•p c√°i nh√¨n to√†n di·ªán v·ªÅ h√†nh vi ti√™u d√πng, xu h∆∞·ªõng th·ªã tr∆∞·ªùng v√† c·∫•u tr√∫c kinh doanh."),
        
        ("Gi√° tr·ªã th·ª±c ti·ªÖn c·ªßa d·ªØ li·ªáu", 
         "Kh√°c v·ªõi c√°c dataset m·∫´u trong s√°ch gi√°o khoa, d·ªØ li·ªáu t·ª´ Tiki.vn l√† d·ªØ li·ªáu th·ª±c t·∫ø t·ª´ th·ªã tr∆∞·ªùng. Dataset n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ nghi√™n c·ª©u th·ªã tr∆∞·ªùng, ph√¢n t√≠ch c·∫°nh tranh, d·ª± ƒëo√°n xu h∆∞·ªõng. ƒê√¢y l√† c∆° h·ªôi ƒë·ªÉ l√†m vi·ªác v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø v√† √°p d·ª•ng ki·∫øn th·ª©c ƒë√£ h·ªçc."),
        
        ("Th√°ch th·ª©c k·ªπ thu·∫≠t v√† c∆° h·ªôi h·ªçc h·ªèi", 
         "X√¢y d·ª±ng h·ªá th·ªëng thu th·∫≠p d·ªØ li·ªáu t·ª´ website ƒë·ªông nh∆∞ Tiki.vn l√† m·ªôt th√°ch th·ª©c k·ªπ thu·∫≠t. D·ª± √°n n√†y ƒë√≤i h·ªèi ki·∫øn th·ª©c v·ªÅ web scraping, x·ª≠ l√Ω d·ªØ li·ªáu, thi·∫øt k·∫ø database, v√† ph√¢n t√≠ch d·ªØ li·ªáu. ƒê√¢y l√† c∆° h·ªôi ƒë·ªÉ √°p d·ª•ng v√† n√¢ng cao c√°c k·ªπ nƒÉng trong lƒ©nh v·ª±c Data Engineering v√† Data Analytics."),
        
        ("T√≠nh m·ªõi v√† ƒë√≥ng g√≥p", 
         "M·∫∑c d√π c√≥ nhi·ªÅu nghi√™n c·ª©u v·ªÅ th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠, nh∆∞ng vi·ªác x√¢y d·ª±ng m·ªôt dataset to√†n di·ªán v√† c√≥ h·ªá th·ªëng t·ª´ Tiki.vn v·∫´n c√≤n h·∫°n ch·∫ø. Dataset n√†y c√≥ th·ªÉ ƒë√≥ng g√≥p cho c·ªông ƒë·ªìng nghi√™n c·ª©u, c√°c nh√† ph√¢n t√≠ch d·ªØ li·ªáu, v√† nh·ªØng ng∆∞·ªùi quan t√¢m ƒë·∫øn th·ªã tr∆∞·ªùng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Vi·ªát Nam."),
        
        ("·ª®ng d·ª•ng trong gi√°o d·ª•c v√† nghi√™n c·ª©u", 
         "Dataset n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ m·ªôt case study trong gi√°o d·ª•c v·ªÅ Data Engineering, Data Analytics, v√† Business Intelligence. N√≥ cung c·∫•p m·ªôt v√≠ d·ª• th·ª±c t·∫ø v·ªÅ c√°ch thu th·∫≠p, x·ª≠ l√Ω, v√† ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ ngu·ªìn th·ª±c t·∫ø, gi√∫p sinh vi√™n v√† nh√† nghi√™n c·ª©u hi·ªÉu r√µ h∆°n v·ªÅ quy tr√¨nh l√†m vi·ªác v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø."),
        
        ("Ti·ªÅm nƒÉng m·ªü r·ªông", 
         "D·ª± √°n n√†y c√≥ ti·ªÅm nƒÉng m·ªü r·ªông l·ªõn. C√≥ th·ªÉ ph√°t tri·ªÉn th√†nh m·ªôt h·ªá th·ªëng monitoring th·ªã tr∆∞·ªùng, m·ªôt c√¥ng c·ª• ph√¢n t√≠ch c·∫°nh tranh, ho·∫∑c m·ªôt n·ªÅn t·∫£ng cung c·∫•p d·ªØ li·ªáu cho c√°c ·ª©ng d·ª•ng kh√°c. Dataset c√≥ th·ªÉ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªãnh k·ª≥ ƒë·ªÉ theo d√µi s·ª± thay ƒë·ªïi c·ªßa th·ªã tr∆∞·ªùng.")
    ]
    
    for idx, (title, content) in enumerate(reasons, 1):
        reason_heading = doc.add_paragraph(f"{idx}. {title}", style="List Bullet")
        reason_heading.paragraph_format.left_indent = Inches(0.25)
        for run in reason_heading.runs:
            set_vietnamese_font(run)
            run.bold = True
            run.font.size = Pt(12)
        
        reason_para = doc.add_paragraph(content, style="List Bullet 2")
        reason_para.paragraph_format.left_indent = Inches(0.5)
        for run in reason_para.runs:
            set_vietnamese_font(run)
            run.font.size = Pt(11)
    
    doc.add_paragraph()  # Spacing
    
    # L·ªùi m·ªü ƒë·∫ßu
    doc.add_heading("L·ªùi M·ªü ƒê·∫ßu: C√¢u Chuy·ªán T·ª´ D·ªØ Li·ªáu", 1)
    add_subbullet_text(
        "ƒê·∫±ng sau m·ªói con s·ªë l√† m·ªôt c√¢u chuy·ªán. ƒê·∫±ng sau m·ªói s·∫£n ph·∫©m l√† m·ªôt l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi ti√™u d√πng. ƒê·∫±ng sau m·ªói danh m·ª•c l√† m·ªôt xu h∆∞·ªõng th·ªã tr∆∞·ªùng. T√†i li·ªáu n√†y tr√¨nh b√†y nh·ªØng c√¢u chuy·ªán ƒë∆∞·ª£c kh√°m ph√° t·ª´ d·ªØ li·ªáu thu th·∫≠p t·ª´ Tiki.vn, m·ªôt trong nh·ªØng n·ªÅn t·∫£ng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ h√†ng ƒë·∫ßu Vi·ªát Nam."
    )
    
    doc.add_paragraph()  # Spacing
    
    # Gi·ªõi thi·ªáu v·ªÅ dataset
    doc.add_heading("V·ªÅ Dataset", 2)
    add_subbullet_text(
        "Dataset n√†y ch·ª©a th√¥ng tin v·ªÅ h√†ng ngh√¨n s·∫£n ph·∫©m t·ª´ Tiki.vn, ƒë∆∞·ª£c thu th·∫≠p v√† x·ª≠ l√Ω m·ªôt c√°ch c√≥ h·ªá th·ªëng. M·ªói s·∫£n ph·∫©m trong dataset bao g·ªìm th√¥ng tin chi ti·∫øt v·ªÅ: t√™n s·∫£n ph·∫©m, gi√° c·∫£, m√¥ t·∫£, ƒë√°nh gi√° c·ªßa ng∆∞·ªùi d√πng, th√¥ng tin ng∆∞·ªùi b√°n, th∆∞∆°ng hi·ªáu, s·ªë l∆∞·ª£ng ƒë√£ b√°n, v√† nhi·ªÅu ch·ªâ s·ªë ph√¢n t√≠ch kh√°c. M·ªói d√≤ng d·ªØ li·ªáu kh√¥ng ch·ªâ ph·∫£n √°nh th√¥ng tin v·ªÅ s·∫£n ph·∫©m, m√† c√≤n cho th·∫•y v·ªÅ th·ªã tr∆∞·ªùng, v·ªÅ h√†nh vi mua s·∫Øm c·ªßa ng∆∞·ªùi ti√™u d√πng, v√† v·ªÅ nh·ªØng xu h∆∞·ªõng ƒëang di·ªÖn ra."
    )
    
    doc.add_paragraph()  # Spacing
    
    # C√¢u chuy·ªán t·ª´ d·ªØ li·ªáu
    doc.add_heading("Nh·ªØng C√¢u H·ªèi Nghi√™n C·ª©u", 2)
    add_subbullet_text(
        "Khi b·∫Øt ƒë·∫ßu v·ªõi dataset n√†y, c√≥ nhi·ªÅu c√¢u h·ªèi nghi√™n c·ª©u ƒë∆∞·ª£c ƒë·∫∑t ra. D·ªØ li·ªáu s·∫Ω gi√∫p tr·∫£ l·ªùi nh·ªØng c√¢u h·ªèi ƒë√≥. D∆∞·ªõi ƒë√¢y l√† nh·ªØng v·∫•n ƒë·ªÅ c√≥ th·ªÉ kh√°m ph√° t·ª´ dataset:"
    )
    
    story_points = [
        "Th·ªã tr∆∞·ªùng Tiki c√≥ quy m√¥ nh∆∞ th·∫ø n√†o? C√≥ bao nhi√™u s·∫£n ph·∫©m v√† danh m·ª•c?",
        "Ng∆∞·ªùi ti√™u d√πng ƒëang mua g√¨? S·∫£n ph·∫©m n√†o ƒë∆∞·ª£c mua nhi·ªÅu nh·∫•t?",
        "Gi√° c·∫£ tr√™n th·ªã tr∆∞·ªùng ph√¢n b·ªë nh∆∞ th·∫ø n√†o? Ng∆∞·ªùi ti√™u d√πng th∆∞·ªùng mua ·ªü m·ª©c gi√° n√†o?",
        "Th∆∞∆°ng hi·ªáu n√†o ƒëang d·∫´n ƒë·∫ßu? Ai l√† ng∆∞·ªùi b√°n t·ªët nh·∫•t?",
        "Ng∆∞·ªùi ti√™u d√πng ƒë√°nh gi√° s·∫£n ph·∫©m nh∆∞ th·∫ø n√†o? ƒêi·ªÉm s·ªë v√† review ph·∫£n √°nh ƒëi·ªÅu g√¨?",
        "Gi·∫£m gi√° c√≥ th·ª±c s·ª± ·∫£nh h∆∞·ªüng ƒë·∫øn doanh s·ªë kh√¥ng?"
    ]
    
    for point in story_points:
        p = doc.add_paragraph(point, style="List Bullet 2")
        for run in p.runs:
            set_vietnamese_font(run)
            run.font.size = Pt(12)
    
    doc.add_paragraph()  # Spacing
    
    # 1. T·ªïng quan (bullet format)
    add_section_title("1. T·ªïng Quan D·ªØ Li·ªáu: M·∫´u Nghi√™n C·ª©u")
    overview = stats["overview"]
    overview_text = doc.add_paragraph()
    overview_text.add_run("Sau qu√° tr√¨nh thu th·∫≠p v√† x·ª≠ l√Ω, dataset ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi quy m√¥ ƒë√°ng k·ªÉ. ")
    overview_text.add_run(f"Dataset hi·ªán t·∫°i bao g·ªìm ")
    overview_text.add_run(f"{overview['total_products']:,}").bold = True
    overview_text.add_run(" s·∫£n ph·∫©m t·ª´ ")
    overview_text.add_run(f"{overview['categories_with_products']:,}").bold = True
    overview_text.add_run(" danh m·ª•c kh√°c nhau, ƒë∆∞·ª£c cung c·∫•p b·ªüi ")
    overview_text.add_run(f"{overview['total_brands']:,}").bold = True
    overview_text.add_run(" th∆∞∆°ng hi·ªáu. ")
    
    # T√≠nh to√°n t·ª∑ l·ªá bestsellers
    if overview['total_products'] > 0:
        bestseller_ratio = (overview['bestsellers'] / overview['total_products']) * 100
        overview_text.add_run(f"Th√∫ v·ªã l√†, c√≥ ")
        overview_text.add_run(f"{bestseller_ratio:.1f}%").bold = True
        overview_text.add_run(f" s·∫£n ph·∫©m ƒë∆∞·ª£c xem l√† 'bestseller' (b√°n > 1000 c√°i).")
    
    format_as_subbullet(overview_text)
    add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 2. Categories
    add_section_title("2. C√¢u Chuy·ªán V·ªÅ Danh M·ª•c: C·∫•u Tr√∫c Th·ªã Tr∆∞·ªùng")
    cat_text = doc.add_paragraph()
    cat_text.add_run("Danh m·ª•c s·∫£n ph·∫©m ph·∫£n √°nh c√°ch t·ªï ch·ª©c v√† ph√¢n lo·∫°i th·ªã tr∆∞·ªùng. ")
    
    if overview.get("categories_with_products"):
        cat_text.add_run(f"D·ªØ li·ªáu cho th·∫•y s·∫£n ph·∫©m ƒë∆∞·ª£c ph√¢n b·ªë v√†o ")
        cat_text.add_run(f"{overview['categories_with_products']:,}").bold = True
        cat_text.add_run(" danh m·ª•c kh√°c nhau. ")
    
    cat_text.add_run("M·ªói danh m·ª•c ƒë·∫°i di·ªán cho m·ªôt ph√¢n kh√∫c th·ªã tr∆∞·ªùng v·ªõi nh·ªØng ƒë·∫∑c ƒëi·ªÉm ri√™ng v·ªÅ gi√° c·∫£, th∆∞∆°ng hi·ªáu, v√† h√†nh vi mua s·∫Øm c·ªßa ng∆∞·ªùi ti√™u d√πng.")
    
    format_as_subbullet(cat_text)
    
    # Insights v·ªÅ danh m·ª•c t·ª´ top_categories_by_sales
    if stats.get("top_categories_by_sales"):
        top_cat_insight = doc.add_paragraph()
        top_cat = stats["top_categories_by_sales"][0] if stats["top_categories_by_sales"] else None
        if top_cat:
            top_cat_insight.add_run("Danh m·ª•c c√≥ doanh s·ªë cao nh·∫•t l√†: ")
            top_cat_insight.add_run(f"{top_cat['product_count']:,} s·∫£n ph·∫©m").bold = True
            if top_cat.get("avg_price"):
                top_cat_insight.add_run(f" v·ªõi gi√° trung b√¨nh ")
                top_cat_insight.add_run(f"{safe_format(top_cat.get('avg_price'), ',.0f')} VND").bold = True
            
            if top_cat.get("total_sales") and top_cat.get("product_count"):
                avg_sales_per_product = top_cat["total_sales"] / top_cat["product_count"]
                top_cat_insight.add_run(f". Trung b√¨nh m·ªói s·∫£n ph·∫©m b√°n ƒë∆∞·ª£c ")
                top_cat_insight.add_run(f"{avg_sales_per_product:.0f} c√°i").bold = True
            top_cat_insight.add_run(".")
            format_as_subbullet(top_cat_insight)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 3. Products
    add_section_title("3. C√¢u Chuy·ªán V·ªÅ S·∫£n Ph·∫©m: Th·ªã Tr∆∞·ªùng Trong L√≤ng B√†n Tay")
    prod_stats = stats["products"]
    prod_text = doc.add_paragraph()
    prod_text.add_run("M·ªói s·∫£n ph·∫©m trong dataset ph·∫£n √°nh m·ªôt l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi ti√™u d√πng d·ª±a tr√™n nhu c·∫ßu, gi√° c·∫£, v√† ƒë√°nh gi√°. ")
    prod_text.add_run(f"Dataset bao g·ªìm s·∫£n ph·∫©m t·ª´ ")
    prod_text.add_run(f"{prod_stats['distinct_brands']:,}").bold = True
    prod_text.add_run(" th∆∞∆°ng hi·ªáu kh√°c nhau v√† ")
    prod_text.add_run(f"{prod_stats['distinct_sellers']:,}").bold = True
    prod_text.add_run(" ng∆∞·ªùi b√°n. ")
    prod_text.add_run("ƒêi·ªÅu n√†y cho th·∫•y th·ªã tr∆∞·ªùng r·∫•t ƒëa d·∫°ng v√† c·∫°nh tranh, t·∫°o ra nhi·ªÅu l·ª±a ch·ªçn cho ng∆∞·ªùi ti√™u d√πng.")
    
    format_as_subbullet(prod_text)
    
    # C√¢u chuy·ªán v·ªÅ gi√° c·∫£
    if prod_stats.get("avg_price") is not None:
        price_story = doc.add_paragraph()
        price_story.add_run("M·ªôt c√¢u h·ªèi nghi√™n c·ª©u quan tr·ªçng: Ng∆∞·ªùi ti√™u d√πng th∆∞·ªùng mua ·ªü m·ª©c gi√° n√†o? ")
        if stats["price_distribution"]:
            max_count_range = max(stats["price_distribution"], key=lambda x: x["count"])
            total_products = sum(r["count"] for r in stats["price_distribution"])
            max_percentage = (max_count_range["count"] / total_products) * 100 if total_products > 0 else 0
            price_story.add_run(f"D·ªØ li·ªáu cho th·∫•y ")
            price_story.add_run(f"{max_percentage:.1f}%").bold = True
            price_story.add_run(f" s·∫£n ph·∫©m n·∫±m trong kho·∫£ng ")
            price_story.add_run(f'"{max_count_range["price_range"]}"').bold = True
            price_story.add_run(". ")
            price_story.add_run("ƒêi·ªÅu n√†y ph·∫£n √°nh ph√¢n kh√∫c gi√° m√† ng∆∞·ªùi ti√™u d√πng Vi·ªát Nam th∆∞·ªùng l·ª±a ch·ªçn khi mua s·∫Øm online.")
        else:
            price_story.add_run(f"Gi√° trung b√¨nh l√† ")
            price_story.add_run(f"{safe_format(prod_stats.get('avg_price'), ',.0f')} VND").bold = True
            price_story.add_run(", cho th·∫•y m·ª©c gi√° ph·ªï bi·∫øn tr√™n th·ªã tr∆∞·ªùng.")
        
        format_as_subbullet(price_story)
    
    # C√¢u chuy·ªán v·ªÅ s·∫£n ph·∫©m b√°n ch·∫°y
    if stats["top_products"]:
        top_prod_story = doc.add_paragraph()
        top_prod = stats["top_products"][0] if stats["top_products"] else None
        if top_prod:
            top_prod_story.add_run("S·∫£n ph·∫©m n√†o ƒë∆∞·ª£c mua nhi·ªÅu nh·∫•t? ")
            top_prod_story.add_run(f'"{top_prod["name"][:60]}"').bold = True
            if top_prod.get("sales_count"):
                top_prod_story.add_run(f" ƒë√£ b√°n ƒë∆∞·ª£c ")
                top_prod_story.add_run(f"{safe_format(top_prod.get('sales_count'), ',')}").bold = True
                top_prod_story.add_run(" s·∫£n ph·∫©m. ")
            top_prod_story.add_run("ƒêi·ªÅu n√†y cho th·∫•y s·∫£n ph·∫©m n√†y ƒë√°p ·ª©ng ƒë∆∞·ª£c nhu c·∫ßu v√† s·ªü th√≠ch c·ªßa ƒë√¥ng ƒë·∫£o ng∆∞·ªùi ti√™u d√πng.")
            format_as_subbullet(top_prod_story)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 4. Brands
    if stats["top_brands"]:
        add_section_title("4. C√¢u Chuy·ªán V·ªÅ Th∆∞∆°ng Hi·ªáu: Ai ƒêang D·∫´n ƒê·∫ßu?")
        brand_text = doc.add_paragraph()
        brand_text.add_run("Th∆∞∆°ng hi·ªáu ƒë√≥ng vai tr√≤ quan tr·ªçng trong quy·∫øt ƒë·ªãnh mua s·∫Øm c·ªßa ng∆∞·ªùi ti√™u d√πng. ")
        brand_text.add_run("Th∆∞∆°ng hi·ªáu kh√¥ng ch·ªâ l√† t√™n g·ªçi, m√† c√≤n l√† l·ªùi h·ª©a v·ªÅ ch·∫•t l∆∞·ª£ng v√† gi√° tr·ªã. ")
        brand_text.add_run("Tr√™n Tiki, c√≥ r·∫•t nhi·ªÅu th∆∞∆°ng hi·ªáu c·∫°nh tranh v·ªõi nhau ƒë·ªÉ thu h√∫t ng∆∞·ªùi mua, t·∫°o n√™n m·ªôt th·ªã tr∆∞·ªùng ƒëa d·∫°ng v√† s√¥i ƒë·ªông.")
        format_as_subbullet(brand_text)
        
        # C√¢u chuy·ªán v·ªÅ th∆∞∆°ng hi·ªáu h√†ng ƒë·∫ßu
        top_brand = stats["top_brands"][0] if stats["top_brands"] else None
        if top_brand:
            brand_story = doc.add_paragraph()
            brand_story.add_run("Th∆∞∆°ng hi·ªáu c√≥ nhi·ªÅu s·∫£n ph·∫©m nh·∫•t l√† ")
            brand_story.add_run(f'"{top_brand["brand"]}"').bold = True
            brand_story.add_run(f" v·ªõi ")
            brand_story.add_run(f"{top_brand['product_count']:,}").bold = True
            brand_story.add_run(" s·∫£n ph·∫©m. ")
            brand_story.add_run("ƒêi·ªÅu n√†y cho th·∫•y th∆∞∆°ng hi·ªáu n√†y ƒë√£ x√¢y d·ª±ng ƒë∆∞·ª£c m·ªôt danh m·ª•c s·∫£n ph·∫©m ƒëa d·∫°ng v√† c√≥ v·ªã th·∫ø m·∫°nh tr√™n th·ªã tr∆∞·ªùng.")
            format_as_subbullet(brand_story)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 5. Market value
    if stats.get("computed_fields"):
        add_section_title("5. C√¢u Chuy·ªán V·ªÅ Gi√° Tr·ªã Th·ªã Tr∆∞·ªùng")
        computed = stats["computed_fields"]
        
        # Estimated Revenue - c√¢u chuy·ªán v·ªÅ quy m√¥
        if computed.get("total_revenue"):
            revenue_story = doc.add_paragraph()
            revenue_story.add_run("M·ªôt c√¢u h·ªèi nghi√™n c·ª©u quan tr·ªçng: Th·ªã tr∆∞·ªùng n√†y c√≥ gi√° tr·ªã bao nhi√™u? ")
            revenue_story.add_run("T·ª´ dataset, t·ªïng doanh thu ∆∞·ªõc t√≠nh l√† ")
            revenue_story.add_run(f"{safe_format(computed.get('total_revenue') / 1000000000, '.2f')} t·ª∑ VND").bold = True
            revenue_story.add_run(". ")
            revenue_story.add_run("Con s·ªë n√†y ph·∫£n √°nh quy m√¥ v√† ti·ªÅm nƒÉng c·ªßa th·ªã tr∆∞·ªùng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Vi·ªát Nam.")
            
            format_as_subbullet(revenue_story)
        
        doc.add_paragraph()  # Spacing
    
    # 6. Price-sales relationship
    if stats.get("price_sales_relationship"):
        add_section_title("6. C√¢u Chuy·ªán: Gi√° N√†o B√°n Ch·∫°y Nh·∫•t?")
        relationship_story = doc.add_paragraph()
        relationship_story.add_run("M·ªôt c√¢u h·ªèi nghi√™n c·ª©u quan tr·ªçng: ·ªû m·ª©c gi√° n√†o th√¨ s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t? ")
        relationship_story.add_run("ƒê√¢y l√† c√¢u h·ªèi m√† nhi·ªÅu ng∆∞·ªùi b√°n v√† doanh nghi·ªáp quan t√¢m. ")
        relationship_story.add_run("D·ªØ li·ªáu c√≥ th·ªÉ gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi n√†y:")
        format_as_subbullet(relationship_story)
        
        # T√¨m kho·∫£ng gi√° c√≥ doanh s·ªë cao nh·∫•t
        max_sales_range = max(stats["price_sales_relationship"], key=lambda x: x.get("avg_sales", 0) or 0)
        if max_sales_range.get("avg_sales"):
            insight_story = doc.add_paragraph()
            insight_story.add_run("Kho·∫£ng gi√° ")
            insight_story.add_run(f'"{max_sales_range["price_range"]}"').bold = True
            insight_story.add_run(" c√≥ doanh s·ªë trung b√¨nh cao nh·∫•t. ")
            insight_story.add_run("ƒêi·ªÅu n√†y cho th·∫•y ƒë√¢y l√† 'v√πng gi√° v√†ng' - m·ª©c gi√° m√† ng∆∞·ªùi ti√™u d√πng c·∫£m th·∫•y h·ª£p l√Ω v√† s·∫µn s√†ng mua nh·∫•t. ")
            insight_story.add_run("ƒê√¢y l√† insight qu√Ω gi√° cho c√°c doanh nghi·ªáp khi ƒë·ªãnh gi√° s·∫£n ph·∫©m.")
            
            format_as_subbullet(insight_story)
        
        doc.add_paragraph()  # Spacing
    
    # 7. Sales distribution
    if stats.get("sales_distribution"):
        add_section_title("7. C√¢u Chuy·ªán: Ai L√† Nh·ªØng S·∫£n Ph·∫©m B√°n Ch·∫°y?")
        sales_dist = stats["sales_distribution"]
        
        sales_story = doc.add_paragraph()
        sales_story.add_run("Kh√¥ng ph·∫£i t·∫•t c·∫£ s·∫£n ph·∫©m ƒë·ªÅu b√°n ƒë∆∞·ª£c nh∆∞ nhau. ")
        sales_story.add_run("Tr√™n th·ªã tr∆∞·ªùng, ch·ªâ c√≥ m·ªôt t·ªâ l·ªá nh·ªè s·∫£n ph·∫©m ƒë∆∞·ª£c ng∆∞·ªùi ti√™u d√πng ∆∞a chu·ªông. ")
        
        # T√≠nh to√°n ph√¢n b·ªë
        total_products = (sales_dist.get("no_sales") or 0) + (sales_dist.get("low_sales") or 0) + \
                        (sales_dist.get("medium_sales") or 0) + (sales_dist.get("high_sales") or 0) + \
                        (sales_dist.get("bestsellers") or 0)
        
        if total_products > 0:
            no_sales_pct = ((sales_dist.get("no_sales") or 0) / total_products) * 100
            bestseller_pct = ((sales_dist.get("bestsellers") or 0) / total_products) * 100
            
            sales_story.add_run(f"D·ªØ li·ªáu cho th·∫•y: ")
            sales_story.add_run(f"{no_sales_pct:.1f}%").bold = True
            sales_story.add_run(f" s·∫£n ph·∫©m ch∆∞a b√°n ƒë∆∞·ª£c (c√≥ th·ªÉ l√† s·∫£n ph·∫©m m·ªõi ho·∫∑c ch·∫•t l∆∞·ª£ng ch∆∞a t·ªët), ")
            sales_story.add_run(f"trong khi ")
            sales_story.add_run(f"{bestseller_pct:.1f}%").bold = True
            sales_story.add_run(f" l√† 'bestsellers' b√°n h∆°n 1000 c√°i. ")
            sales_story.add_run("S·ª± ch√™nh l·ªách n√†y cho th·∫•y h√†nh vi mua s·∫Øm r·∫•t t·∫≠p trung v√†o m·ªôt s·ªë s·∫£n ph·∫©m 'sao' nh·∫•t ƒë·ªãnh.")
        
        format_as_subbullet(sales_story)
        
        # Median vs Average insight
        if sales_dist.get("avg_sales") and sales_dist.get("median_sales"):
            median_insight = doc.add_paragraph()
            median_insight.add_run("M·ªôt ph√°t hi·ªán th√∫ v·ªã kh√°c: ")
            median_insight.add_run(f"doanh s·ªë trung v·ªã (median) l√† {safe_format(sales_dist.get('median_sales'), '.0f')} c√°i, ").bold = True
            median_insight.add_run(f"nh∆∞ng doanh s·ªë trung b√¨nh (average) l√† {safe_format(sales_dist.get('avg_sales'), '.0f')} c√°i. ")
            median_insight.add_run("ƒêi·ªÅu n√†y cho th·∫•y c√≥ m·ªôt s·ªë s·∫£n ph·∫©m bestseller 'k√©o' doanh s·ªë trung b√¨nh l√™n r·∫•t cao. ")
            median_insight.add_run("N√≥i c√°ch kh√°c, th·ªã tr∆∞·ªùng c√≥ ph√¢n h√≥a l·ªõn - c√≥ nh·ªØng s·∫£n ph·∫©m b√°n c·ª±c ch·∫°y, nh∆∞ng nhi·ªÅu s·∫£n ph·∫©m kh√°c b√°n kh√¥ng t·ªët.")
            format_as_subbullet(median_insight)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 8. Official vs third-party
    if stats.get("official_analysis"):
        add_section_title("8. C√¢u Chuy·ªán: Official Store vs Third-party Sellers")
        official = stats["official_analysis"]
        
        official_story = doc.add_paragraph()
        official_story.add_run("Tr√™n Tiki, c√≥ hai lo·∫°i ng∆∞·ªùi b√°n: Official Store (c·ª≠a h√†ng ch√≠nh th·ª©c) v√† Third-party Sellers (nh√† b√°n l·∫ª ƒë·ªôc l·∫≠p). ")
        official_story.add_run("C√¢u h·ªèi ƒë·∫∑t ra l√†: C·ª≠a h√†ng ch√≠nh th·ª©c c√≥ th·ª±c s·ª± b√°n t·ªët h∆°n kh√¥ng? ")
        
        if official.get("official_products") and official.get("third_party_products"):
            total_products = official["official_products"] + official["third_party_products"]
            official_pct = (official["official_products"] / total_products) * 100
            third_party_pct = (official["third_party_products"] / total_products) * 100
            
            official_story.add_run(f"Dataset cho th·∫•y: ")
            official_story.add_run(f"{official_pct:.1f}%").bold = True
            official_story.add_run(f" s·∫£n ph·∫©m t·ª´ Official Store, ")
            official_story.add_run(f"{third_party_pct:.1f}%").bold = True
            official_story.add_run(f" t·ª´ Third-party.")
        
        format_as_subbullet(official_story)
        
        # So s√°nh hi·ªáu su·∫•t
        if official.get("avg_sales_official") and official.get("avg_sales_third_party"):
            comparison = doc.add_paragraph()
            official_sales = official.get("avg_sales_official") or 0
            third_party_sales = official.get("avg_sales_third_party") or 0
            official_rating = official.get("avg_rating_official") or 0
            third_party_rating = official.get("avg_rating_third_party") or 0
            
            if official_sales > third_party_sales:
                diff = ((official_sales - third_party_sales) / third_party_sales) * 100 if third_party_sales > 0 else 0
                comparison.add_run(f"Th√∫ v·ªã l√†: Official Store b√°n t·ªët h∆°n trung b√¨nh ")
                comparison.add_run(f"{diff:.1f}%").bold = True
                comparison.add_run(f" so v·ªõi Third-party. ")
            else:
                diff = ((third_party_sales - official_sales) / official_sales) * 100 if official_sales > 0 else 0
                comparison.add_run(f"ƒêi·ªÅu b·∫•t ng·ªù l√†: Third-party Sellers b√°n t·ªët h∆°n Official Store trung b√¨nh ")
                comparison.add_run(f"{diff:.1f}%").bold = True
                comparison.add_run(f". ")
            
            comparison.add_run("ƒêi·ªÅu n√†y c√≥ th·ªÉ do Third-party th∆∞·ªùng c√≥ gi√° c·∫°nh tranh h∆°n ho·∫∑c ng∆∞·ªùi ti√™u d√πng tin t∆∞·ªüng v√†o c√°c review t·ª´ ng∆∞·ªùi d√πng th·ª±c.")
            
            format_as_subbullet(comparison)
        
        # So s√°nh rating
        if official.get("avg_rating_official") and official.get("avg_rating_third_party"):
            rating_para = doc.add_paragraph()
            official_rating = official.get("avg_rating_official") or 0
            third_party_rating = official.get("avg_rating_third_party") or 0
            
            rating_para.add_run("V·ªÅ ch·∫•t l∆∞·ª£ng (d·ª±a tr√™n rating): ")
            rating_para.add_run(f"Official Store c√≥ rating {safe_format(official_rating, '.2f')}/5, ").bold = True
            rating_para.add_run(f"Third-party c√≥ {safe_format(third_party_rating, '.2f')}/5. ")
            
            if official_rating > third_party_rating:
                rating_para.add_run("Official Store c√≥ ∆∞u th·∫ø v·ªÅ ch·∫•t l∆∞·ª£ng nh·∫≠n th·ª©c t·ª´ ng∆∞·ªùi ti√™u d√πng.")
            else:
                rating_para.add_run("Ng∆∞·ªùi ti√™u d√πng ƒë√°nh gi√° Third-party cao h∆°n, cho th·∫•y s·ª± c·∫°nh tranh l√†nh m·∫°nh.")
            format_as_subbullet(rating_para)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 9. Stock
    if stats.get("stock_analysis"):
        add_section_title("9. C√¢u Chuy·ªán: T√≠nh S·∫µn L√≤ng B√°n H√†ng")
        stock = stats["stock_analysis"]
        
        stock_story = doc.add_paragraph()
        stock_story.add_run("M·ªôt y·∫øu t·ªë quan tr·ªçng ƒë·ªÉ ng∆∞·ªùi ti√™u d√πng mua ƒë∆∞·ª£c s·∫£n ph·∫©m: h√†ng ph·∫£i c√≤n trong kho. ")
        
        if stock.get("in_stock") and stock.get("out_of_stock"):
            total_stock = stock["in_stock"] + stock["out_of_stock"]
            in_stock_pct = (stock["in_stock"] / total_stock) * 100
            out_stock_pct = (stock["out_of_stock"] / total_stock) * 100
            
            stock_story.add_run(f"Dataset cho th·∫•y: ")
            stock_story.add_run(f"{in_stock_pct:.1f}%").bold = True
            stock_story.add_run(f" s·∫£n ph·∫©m c√≤n trong kho, ")
            stock_story.add_run(f"{out_stock_pct:.1f}%").bold = True
            stock_story.add_run(f" s·∫£n ph·∫©m h·∫øt h√†ng. ")
        
        format_as_subbullet(stock_story)
        
        # So s√°nh doanh s·ªë
        if stock.get("avg_sales_in_stock") and stock.get("avg_sales_out_of_stock"):
            stock_impact = doc.add_paragraph()
            in_stock_sales = stock.get("avg_sales_in_stock") or 0
            out_stock_sales = stock.get("avg_sales_out_of_stock") or 0
            
            stock_impact.add_run("S·∫£n ph·∫©m c√≥ trong kho b√°n ƒë∆∞·ª£c trung b√¨nh ")
            stock_impact.add_run(f"{safe_format(in_stock_sales, '.0f')} c√°i, ").bold = True
            stock_impact.add_run("trong khi s·∫£n ph·∫©m h·∫øt h√†ng ch·ªâ b√°n ƒë∆∞·ª£c ")
            stock_impact.add_run(f"{safe_format(out_stock_sales, '.0f')} c√°i. ").bold = True
            
            if in_stock_sales > 0 and out_stock_sales > 0:
                diff_ratio = in_stock_sales / out_stock_sales
                stock_impact.add_run(f"S·∫£n ph·∫©m c√≥ trong kho b√°n g·∫•p ")
                stock_impact.add_run(f"{diff_ratio:.1f}x").bold = True
                stock_impact.add_run(" so v·ªõi h·∫øt h√†ng! ƒêi·ªÅu n√†y cho th·∫•y s·ª± s·∫µn s√†ng c·ªßa ng∆∞·ªùi b√°n r·∫•t quan tr·ªçng ƒë·∫øn doanh s·ªë.")
            format_as_subbullet(stock_impact)
        add_spacer()
    
    doc.add_paragraph()  # Spacing
    
    # 10. K·∫øt lu·∫≠n (retain numbering style from user request but bullet formatting)
    add_section_title("10. K·∫øt Lu·∫≠n: Nh·ªØng C√¢u Chuy·ªán ƒê√£ K·ªÉ")
    
    # T·ªïng h·ª£p c√¢u chuy·ªán
    conclusion = doc.add_paragraph()
    conclusion.add_run("Qua qu√° tr√¨nh ph√¢n t√≠ch d·ªØ li·ªáu, ƒë√£ kh√°m ph√° ƒë∆∞·ª£c nhi·ªÅu insights th√∫ v·ªã v·ªÅ th·ªã tr∆∞·ªùng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Vi·ªát Nam. ")
    conclusion.add_run(f"V·ªõi ")
    conclusion.add_run(f"{overview['total_products']:,}").bold = True
    conclusion.add_run(" s·∫£n ph·∫©m t·ª´ ")
    conclusion.add_run(f"{overview['total_categories']:,}").bold = True
    conclusion.add_run(" danh m·ª•c, dataset n√†y ph·∫£n √°nh m·ªôt th·ªã tr∆∞·ªùng s√¥i ƒë·ªông, ƒëa d·∫°ng v√† ƒë·∫ßy ti·ªÅm nƒÉng.")
    
    for run in conclusion.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(12)
    
    doc.add_paragraph()  # Spacing
    
    # Nh·ªØng ph√°t hi·ªán ch√≠nh
    doc.add_heading("Nh·ªØng Ph√°t Hi·ªán Ch√≠nh", 2)
    learnings_intro = doc.add_paragraph()
    learnings_intro.add_run("T·ª´ ph√¢n t√≠ch d·ªØ li·ªáu, c√≥ th·ªÉ r√∫t ra nh·ªØng ph√°t hi·ªán sau:")
    
    for run in learnings_intro.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(12)
    
    # T√≠nh to√°n learnings t·ª´ d·ªØ li·ªáu
    learnings = []
    
    if prod_stats.get("distinct_brands"):
        learnings.append(f"Th·ªã tr∆∞·ªùng c√≥ {prod_stats['distinct_brands']:,} th∆∞∆°ng hi·ªáu kh√°c nhau, cho th·∫•y s·ª± ƒëa d·∫°ng v√† c·∫°nh tranh tr√™n th·ªã tr∆∞·ªùng.")
    
    if prod_stats.get("discounted_products") and overview.get("total_products"):
        discount_ratio = (prod_stats["discounted_products"] / overview["total_products"]) * 100
        learnings.append(f"Khuy·∫øn m√£i l√† m·ªôt chi·∫øn l∆∞·ª£c ph·ªï bi·∫øn - c√≥ {discount_ratio:.1f}% s·∫£n ph·∫©m ƒëang gi·∫£m gi√°.")
    
    if prod_stats.get("avg_rating"):
        learnings.append(f"Ng∆∞·ªùi ti√™u d√πng kh√° h√†i l√≤ng v·ªõi s·∫£n ph·∫©m tr√™n th·ªã tr∆∞·ªùng, v·ªõi ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh {safe_format(prod_stats.get('avg_rating'), '.2f')}/5.0.")
    
    if stats.get("sales_distribution") and stats["sales_distribution"].get("bestsellers"):
        bestseller_count = stats["sales_distribution"]["bestsellers"]
        learnings.append(f"Hi·ªáu ·ª©ng '80/20' xu·∫•t hi·ªán r√µ: ch·ªâ {bestseller_count} s·∫£n ph·∫©m ({(bestseller_count/overview.get('total_products', 1)*100):.1f}%) l√† bestsellers, nh∆∞ng h·ªç chi·∫øm l∆∞·ª£ng b√°n ƒë√°ng k·ªÉ.")
    
    if stats.get("official_analysis"):
        official = stats["official_analysis"]
        if official.get("official_products"):
            official_pct = (official["official_products"] / (official["official_products"] + official.get("third_party_products", 1))) * 100
            learnings.append(f"S·ª± c√¢n b·∫±ng gi·ªØa Official Store ({official_pct:.1f}%) v√† Third-party Sellers t·∫°o ra m·ªôt th·ªã tr∆∞·ªùng ƒëa d·∫°ng v√† c·∫°nh tranh.")
    
    if stats.get("stock_analysis"):
        stock = stats["stock_analysis"]
        if stock.get("in_stock"):
            in_stock_pct = (stock["in_stock"] / (stock["in_stock"] + stock.get("out_of_stock", 1))) * 100
            learnings.append(f"T√≠nh s·∫µn l√≤ng: {in_stock_pct:.1f}% s·∫£n ph·∫©m c√≤n trong kho. S·∫£n ph·∫©m c√≥ trong kho b√°n r·∫•t t·ªët h∆°n h·∫øt h√†ng.")
    
    learnings.append(f"Ph√¢n h√≥a gi·ªØa bestsellers v√† s·∫£n ph·∫©m th∆∞·ªùng - kh√¥ng ph·∫£i t·∫•t c·∫£ s·∫£n ph·∫©m ƒë·ªÅu b√°n ch·∫°y, th·ªã tr∆∞·ªùng c√≥ ph√¢n t·∫ßng r√µ r√†ng.")
    learnings.append("Ng∆∞·ªùi ti√™u d√πng online kh√¥ng ch·ªâ quan t√¢m gi√°, m√† c√≤n ch·∫•t l∆∞·ª£ng (rating), th∆∞∆°ng hi·ªáu, v√† t√≠nh s·∫µn s√†ng c·ªßa ng∆∞·ªùi b√°n.")
    
    for learning in learnings:
        p = doc.add_paragraph(learning, style="List Bullet 2")
        for run in p.runs:
            set_vietnamese_font(run)
            run.font.size = Pt(12)
    
    doc.add_paragraph()  # Spacing
    
    # ·ª®ng d·ª•ng
    doc.add_heading("·ª®ng D·ª•ng C·ªßa Dataset", 2)
    application = doc.add_paragraph()
    application.add_run("Dataset n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho nhi·ªÅu m·ª•c ƒë√≠ch nghi√™n c·ª©u v√† ph√¢n t√≠ch:")
    
    for run in application.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(12)
    
    applications = [
        "Nghi√™n c·ª©u v·ªÅ h√†nh vi mua s·∫Øm c·ªßa ng∆∞·ªùi Vi·ªát Nam",
        "Ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng v√† d·ª± ƒëo√°n t∆∞∆°ng lai",
        "So s√°nh c√°c th∆∞∆°ng hi·ªáu v√† s·∫£n ph·∫©m",
        "H·ªçc t·∫≠p v·ªÅ Data Engineering v√† Data Analytics v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø",
        "Ph√°t tri·ªÉn c√°c d·ª± √°n ph√¢n t√≠ch d·ªØ li·ªáu kh√°c"
    ]
    
    for app in applications:
        p = doc.add_paragraph(app, style="List Bullet 2")
        for run in p.runs:
            set_vietnamese_font(run)
            run.font.size = Pt(12)
    
    # Footer
    doc.add_paragraph()
    footer = doc.add_paragraph("---")
    footer.alignment = WD_ALIGN_PARAGRAPH.CENTER
    footer_text = doc.add_paragraph("Tiki Data Pipeline - Data Story Document")
    footer_text.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in footer_text.runs:
        set_vietnamese_font(run)
        run.font.size = Pt(10)
        run.font.color.rgb = RGBColor(128, 128, 128)
    
    # L∆∞u file (x√≥a file c≈© n·∫øu t·ªìn t·∫°i v√† ƒëang b·ªã lock)
    try:
        if output_path.exists():
            try:
                output_path.unlink()
            except PermissionError:
                print(f"‚ö†Ô∏è  File ƒëang ƒë∆∞·ª£c m·ªü, vui l√≤ng ƒë√≥ng file {output_path} v√† ch·∫°y l·∫°i script")
                raise
        doc.save(str(output_path))
        print(f"‚úÖ ƒê√£ t·∫°o file docx: {output_path}")
    except PermissionError as e:
        print(f"‚ùå Kh√¥ng th·ªÉ ghi file: {e}")
        print(f"üí° Vui l√≤ng ƒë√≥ng file {output_path} n·∫øu ƒëang m·ªü v√† ch·∫°y l·∫°i script")
        raise


def main():
    """H√†m ch√≠nh"""
    # Set UTF-8 encoding for Windows
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")
    
    # Parse arguments
    parser = argparse.ArgumentParser(
        description='X√¢y d·ª±ng c√¢u chuy·ªán d·ªØ li·ªáu t·ª´ database v√† t·∫°o file DOCX'
    )
    parser.add_argument(
        '--upload',
        action='store_true',
        help='T·ª± ƒë·ªông upload file l√™n Google Drive sau khi build'
    )
    parser.add_argument(
        '--folder-id',
        type=str,
        help='ID c·ªßa folder tr√™n Google Drive ƒë·ªÉ upload file v√†o',
        default=None
    )
    args = parser.parse_args()
    
    print("=" * 70)
    print("üìñ X√ÇY D·ª∞NG C√ÇU CHUY·ªÜN D·ªÆ LI·ªÜU - TIKI DATA PIPELINE")
    print("=" * 70)
    
    # Load config
    config = load_env_config()
    print(f"\nüìã Th√¥ng tin k·∫øt n·ªëi:")
    print(f"   Host: {config['host']}")
    print(f"   Port: {config['port']}")
    print(f"   Database: {config['database']}")
    print(f"   User: {config['user']}")
    
    # K·∫øt n·ªëi database
    conn = connect_database(config)
    
    try:
        # Ph√¢n t√≠ch d·ªØ li·ªáu
        print("\nüîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch d·ªØ li·ªáu...")
        stats = analyze_database(conn)
        
        # T·∫°o file docx (lu√¥n ghi ƒë√® v√†o c√πng m·ªôt file)
        output_dir = PROJECT_ROOT / "docs"
        output_dir.mkdir(exist_ok=True)
        output_path = output_dir / "data_story.docx"
        
        print("\nüìù ƒêang t·∫°o file docx...")
        create_document(stats, output_path)
        
        print("\n" + "=" * 70)
        print("‚úÖ HO√ÄN T·∫§T!")
        print("=" * 70)
        print(f"\nüìÑ File ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {output_path}")
        if output_path.exists():
            print(f"   K√≠ch th∆∞·ªõc: {output_path.stat().st_size / 1024:.2f} KB")
        
        # Upload l√™n Google Drive n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if args.upload:
            print("\n" + "=" * 70)
            print("üì§ UPLOAD L√äN GOOGLE DRIVE")
            print("=" * 70)
            upload_to_google_drive(output_path, args.folder_id)
        
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        conn.close()
        print("\nüîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi database")


if __name__ == "__main__":
    main()

