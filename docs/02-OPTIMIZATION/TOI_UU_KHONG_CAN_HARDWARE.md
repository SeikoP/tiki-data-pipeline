# ‚ö° T·ªêI ∆ØU NGAY - KH√îNG C·∫¶N THAY ƒê·ªîI HARDWARE

**Ng√†y t·∫°o**: 2025-12-01  
**M·ª•c ti√™u**: C√°c t·ªëi ∆∞u h√≥a c√≥ th·ªÉ tri·ªÉn khai NGAY L·∫¨P T·ª®C m√† kh√¥ng c·∫ßn upgrade hardware, th√™m RAM, CPU hay infrastructure

---

## üéØ T√ìM T·∫ÆT ƒêI·ªÄU H√ÄNH

### C√°c T·ªëi ∆Øu C√≥ Th·ªÉ B·∫Øt ƒê·∫ßu Ngay

| Lo·∫°i T·ªëi ∆Øu | Expected Impact | Effort | C√≥ Th·ªÉ B·∫Øt ƒê·∫ßu |
|-------------|----------------|--------|----------------|
| **Configuration Tuning** | +20-40% | 1-2 gi·ªù | ‚úÖ NGAY |
| **Caching Strategy** | +30-50% | 2-4 gi·ªù | ‚úÖ NGAY |
| **Rate Limiting Adaptive** | +20-30% | 3-6 gi·ªù | ‚úÖ NGAY |
| **Browser Flags Optimization** | +20-30% | 1-2 gi·ªù | ‚úÖ NGAY |
| **Smart Waiting** | +15-25% | 2-4 gi·ªù | ‚úÖ NGAY |
| **Code Algorithm** | +10-20% | 4-8 gi·ªù | ‚úÖ NGAY |
| **Data Structure** | +10-15% | 2-3 gi·ªù | ‚úÖ NGAY |
| **Connection Reuse** | +15-20% | 1-2 gi·ªù | ‚úÖ NGAY |

**T·ªïng Expected Improvement**: **+50-100% t·ªëc ƒë·ªô** (kh√¥ng c·∫ßn hardware)

---

## üöÄ 1. CONFIGURATION TUNING (1-2 gi·ªù)

### 1.1 T·ªëi ∆Øu C√°c Tham S·ªë Hi·ªán C√≥

**Kh√¥ng c·∫ßn code changes, ch·ªâ c·∫ßn thay ƒë·ªïi config!**

#### ‚úÖ Task 1.1.1: TƒÉng DNS Cache TTL
**Impact**: +10-15%  
**Effort**: 5 ph√∫t  
**Risk**: Th·∫•p

**Hi·ªán t·∫°i**:
```python
HTTP_DNS_CACHE_TTL = 300  # 5 ph√∫t
```

**ƒê·ªÅ xu·∫•t**:
```python
HTTP_DNS_CACHE_TTL = 1800  # 30 ph√∫t (tƒÉng 6x)
```

**L√Ω do**: DNS lookup cho Tiki.vn kh√¥ng thay ƒë·ªïi th∆∞·ªùng xuy√™n. Cache l√¢u h∆°n gi·∫£m s·ªë l·∫ßn lookup.

**C√°ch l√†m**:
1. S·ª≠a `src/pipelines/crawl/config.py`
2. Thay ƒë·ªïi `HTTP_DNS_CACHE_TTL = 1800`
3. Restart service

**Expected**: DNS lookup time gi·∫£m t·ª´ 50-200ms ‚Üí 5-10ms (cache hit)

---

#### ‚úÖ Task 1.1.2: T·ªëi ∆Øu Connection Pool Limits
**Impact**: +15-20%  
**Effort**: 15 ph√∫t  
**Risk**: Th·∫•p

**Hi·ªán t·∫°i**:
```python
HTTP_CONNECTOR_LIMIT = 100
HTTP_CONNECTOR_LIMIT_PER_HOST = 10
```

**ƒê·ªÅ xu·∫•t**:
```python
HTTP_CONNECTOR_LIMIT = 150  # TƒÉng 50%
HTTP_CONNECTOR_LIMIT_PER_HOST = 15  # TƒÉng 50%
```

**L√Ω do**: Connection pooling hi·ªán t·∫°i c√≥ th·ªÉ ch∆∞a t·ªëi ∆∞u. TƒÉng limit gi√∫p reuse connections t·ªët h∆°n.

**C√°ch l√†m**:
1. S·ª≠a `src/pipelines/crawl/config.py`
2. Monitor connection usage
3. TƒÉng t·ª´ t·ª´ n·∫øu stable

**Expected**: Connection reuse rate tƒÉng t·ª´ 85% ‚Üí 92-95%

---

#### ‚úÖ Task 1.1.3: Adaptive Rate Limiting
**Impact**: +20-30%  
**Effort**: 2-4 gi·ªù  
**Risk**: Trung b√¨nh (c·∫ßn test c·∫©n th·∫≠n)

**Hi·ªán t·∫°i**:
```python
# Fixed rate limit
RATE_LIMIT_DELAY = 0.7  # Fixed 0.7s
```

**ƒê·ªÅ xu·∫•t**: Dynamic rate limiting d·ª±a tr√™n response

```python
# Adaptive rate limiting
def get_rate_limit_delay(consecutive_successes, error_rate):
    """T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh delay d·ª±a tr√™n performance"""
    if consecutive_successes > 100 and error_rate < 0.5%:
        return 0.3  # Aggressive - √≠t delay nh·∫•t
    elif consecutive_successes > 50 and error_rate < 1%:
        return 0.5  # Moderate
    elif error_rate < 2%:
        return 0.7  # Current default
    else:
        return 1.0  # Conservative khi c√≥ nhi·ªÅu errors
```

**C√°ch l√†m**:
1. Implement tracking cho consecutive_successes v√† error_rate
2. Update rate limit delay ƒë·ªông
3. Monitor v√† adjust thresholds

**Expected**: Average delay gi·∫£m t·ª´ 0.7s ‚Üí 0.4-0.5s (khi stable)

---

## üíæ 2. CACHING STRATEGY (2-4 gi·ªù)

### 2.1 Smart Cache Key Strategy

**Impact**: +10-15% cache hit rate  
**Effort**: 2-3 gi·ªù

#### ‚úÖ Task 2.1.1: URL Normalization C·∫£i Ti·∫øn

**Hi·ªán t·∫°i**: C√≥ th·ªÉ c√≥ nhi·ªÅu URLs cho c√πng 1 product
```
https://tiki.vn/product?spid=12345
https://tiki.vn/product/12345?utm_source=google
https://tiki.vn/product/12345?ref=category
```

**ƒê·ªÅ xu·∫•t**: Normalize URL t·ªët h∆°n

```python
def normalize_product_url(url: str) -> str:
    """Chu·∫©n h√≥a URL ƒë·ªÉ cache key consistent"""
    from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
    
    parsed = urlparse(url)
    
    # Ch·ªâ gi·ªØ c√°c query params quan tr·ªçng
    important_params = ['spid', 'id', 'sku']
    query_params = parse_qs(parsed.query)
    filtered_params = {
        k: v[0] for k, v in query_params.items() 
        if k.lower() in important_params
    }
    
    # Rebuild URL
    new_query = urlencode(filtered_params)
    normalized = urlunparse((
        parsed.scheme,
        parsed.netloc,
        parsed.path,
        parsed.params,
        new_query,
        ''  # Remove fragment
    ))
    
    return normalized
```

**Expected**: Cache hit rate tƒÉng 10-15% (gi·∫£m duplicate cache entries)

---

#### ‚úÖ Task 2.1.2: Cache Pre-Warming

**Impact**: +5-10% initial speed  
**Effort**: 1-2 gi·ªù

**√ù t∆∞·ªüng**: Pre-cache c√°c products ph·ªï bi·∫øn tr∆∞·ªõc khi crawl

```python
def warm_cache(popular_product_ids: list[str]):
    """Pre-cache popular products"""
    for product_id in popular_product_ids[:100]:  # Top 100
        url = f"https://tiki.vn/product/{product_id}"
        # Crawl v√† cache (kh√¥ng block main crawl)
        asyncio.create_task(crawl_and_cache(url))
```

**C√°ch l√†m**:
1. Identify top popular products (t·ª´ DB ho·∫∑c previous crawl)
2. Background task ƒë·ªÉ pre-cache
3. Ch·∫°y tr∆∞·ªõc khi main crawl start

**Expected**: Khi crawl start, 20-30% products ƒë√£ c√≥ cache

---

#### ‚úÖ Task 2.1.3: Partial Cache Strategy

**Impact**: +15-20% effective cache  
**Effort**: 2-3 gi·ªù

**Hi·ªán t·∫°i**: Cache to√†n b·ªô product data ho·∫∑c kh√¥ng cache g√¨ c·∫£

**ƒê·ªÅ xu·∫•t**: Cache t·ª´ng ph·∫ßn (fields)

```python
# Cache structure
{
    "product:12345:basic": {...},  # name, price, rating
    "product:12345:details": {...},  # description, specs
    "product:12345:images": [...],  # images
}

# Khi crawl, ch·ªâ fetch ph·∫ßn ch∆∞a c√≥
if cached_basic:
    use_cached_basic()
    fetch_only_details_and_images()
```

**Expected**: Cache hit rate tƒÉng 15-20% (c√≥ th·ªÉ d√πng partial data)

---

## üåê 3. NETWORK OPTIMIZATION (1-2 gi·ªù)

### 3.1 Connection Reuse T·ªëi ∆Øu

**Impact**: +15-20%  
**Effort**: 1 gi·ªù

#### ‚úÖ Task 3.1.1: HTTP/2 Support (N·∫øu Tiki h·ªó tr·ª£)

```python
import aiohttp

connector = aiohttp.TCPConnector(
    limit=150,
    limit_per_host=15,
    ttl_dns_cache=1800,
    force_close=False,  # Keep connections alive
    enable_cleanup_closed=True,
    use_dns_cache=True,
)

# Enable HTTP/2 n·∫øu server h·ªó tr·ª£
async with aiohttp.ClientSession(
    connector=connector,
    version=aiohttp.ClientHttpVersion.HTTP_2  # Th·ª≠ HTTP/2
) as session:
    ...
```

**Expected**: Multiplexing requests, gi·∫£m latency 10-15%

---

#### ‚úÖ Task 3.1.2: Request Header Optimization

**Impact**: +5-10%  
**Effort**: 30 ph√∫t

```python
# Minimize headers
headers = {
    'User-Agent': 'Mozilla/5.0...',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'vi-VN,vi;q=0.9',
    # B·ªè c√°c headers kh√¥ng c·∫ßn thi·∫øt
    # 'Accept-Encoding': 'gzip, deflate' - let aiohttp handle
    # 'Connection': 'keep-alive' - default
}
```

**Expected**: Request size nh·ªè h∆°n, faster transmission

---

## üñ•Ô∏è 4. BROWSER OPTIMIZATION (1-2 gi·ªù)

### 4.1 Chrome Flags T·ªëi ∆Øu

**Impact**: +20-30% browser speed  
**Effort**: 1 gi·ªù

#### ‚úÖ Task 4.1.1: Add Performance Flags

**Hi·ªán t·∫°i**: C√≥ th·ªÉ ch∆∞a c√≥ ƒë·ªß flags

**ƒê·ªÅ xu·∫•t**:
```python
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument('--headless=new')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

# TH√äM C√ÅC FLAGS M·ªöI:
options.add_argument('--disable-software-rasterizer')
options.add_argument('--disable-extensions')
options.add_argument('--disable-plugins')
options.add_argument('--disable-images')  # Block images
options.add_argument('--blink-settings=imagesEnabled=false')
options.add_argument('--disable-javascript')  # N·∫øu kh√¥ng c·∫ßn JS
options.add_argument('--disable-css')  # N·∫øu kh√¥ng c·∫ßn CSS
options.add_argument('--disable-background-networking')
options.add_argument('--disable-background-timer-throttling')
options.add_argument('--disable-renderer-backgrounding')
options.add_argument('--disable-backgrounding-occluded-windows')
options.add_argument('--disable-ipc-flooding-protection')

# Memory optimization
options.add_argument('--memory-pressure-off')
options.add_argument('--max_old_space_size=4096')

# Performance
options.add_argument('--disable-features=TranslateUI')
options.add_argument('--disable-ipc-flooding-protection')
```

**Expected**: Browser load time gi·∫£m 20-30%

---

#### ‚úÖ Task 4.1.2: Block Unnecessary Resources

**Impact**: +15-20% page load  
**Effort**: 1 gi·ªù

```python
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

caps = DesiredCapabilities.CHROME
caps['goog:chromeOptions'] = {
    'prefs': {
        'profile.managed_default_content_settings.images': 2,  # Block images
        'profile.default_content_setting_values.stylesheets': 2,  # Block CSS
        'profile.default_content_setting_values.javascript': 2,  # Block JS (n·∫øu kh√¥ng c·∫ßn)
        'profile.default_content_setting_values.plugins': 2,  # Block plugins
        'profile.default_content_setting_values.media_stream': 2,  # Block media
    }
}

# Ho·∫∑c d√πng Chrome DevTools Protocol
driver.execute_cdp_cmd('Network.setBlockedURLs', {
    'urls': [
        '*://*.google-analytics.com/*',
        '*://*.googletagmanager.com/*',
        '*://*.facebook.com/*',
        '*://*.doubleclick.net/*',
        # Block ads & tracking
    ]
})
```

**Expected**: Page load size gi·∫£m 50-70%, load time gi·∫£m 15-20%

---

## ‚è±Ô∏è 5. SMART WAITING (2-4 gi·ªù)

### 5.1 Replace time.sleep() v·ªõi Explicit Waits

**Impact**: +15-25%  
**Effort**: 2-3 gi·ªù

#### ‚úÖ Task 5.1.1: Conditional Waits

**Hi·ªán t·∫°i**: C√≥ th·ªÉ c√≥ nhi·ªÅu `time.sleep()` fixed

**ƒê·ªÅ xu·∫•t**: Wait cho specific elements

```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# Thay v√¨
time.sleep(3)  # Fixed wait

# D√πng
wait = WebDriverWait(driver, timeout=5)
try:
    # Ch·ªù element xu·∫•t hi·ªán (t·ªëi ƒëa 5s)
    element = wait.until(
        EC.presence_of_element_located((By.CSS_SELECTOR, '.product-price'))
    )
    # Element ƒë√£ load, ti·∫øp t·ª•c ngay
except TimeoutException:
    # Element kh√¥ng load, timeout sau 5s (thay v√¨ wait ƒë·ªß 3s)
    pass
```

**Expected**: Average wait time gi·∫£m t·ª´ 3s ‚Üí 1-1.5s (n·∫øu page load nhanh)

---

#### ‚úÖ Task 5.1.2: Skip Unnecessary Waits

**Impact**: +10-15%  
**Effort**: 1-2 gi·ªù

**√ù t∆∞·ªüng**: Ch·ªâ wait khi th·ª±c s·ª± c·∫ßn

```python
def extract_product_detail_smart(driver, url):
    # 1. Load page
    driver.get(url)
    
    # 2. Ch·ªù price element (critical)
    try:
        price = wait_for_element(driver, '.product-price', timeout=3)
    except:
        return None  # Fail fast n·∫øu kh√¥ng c√≥ price
    
    # 3. Ch·ªâ wait cho description n·∫øu c·∫ßn
    if need_description:
        description = wait_for_element(driver, '.product-description', timeout=2)
    else:
        description = None  # Skip wait
    
    # 4. Kh√¥ng wait cho reviews (non-critical)
    reviews = driver.find_elements(By.CSS_SELECTOR, '.reviews')
    # Process n·∫øu c√≥, kh√¥ng th√¨ None
    
    return {...}
```

**Expected**: 30-40% products kh√¥ng c·∫ßn wait ƒë·ªß time

---

## üîÑ 6. CODE ALGORITHM (4-8 gi·ªù)

### 6.1 Batch Processing Optimization

**Impact**: +10-20%  
**Effort**: 2-3 gi·ªù

#### ‚úÖ Task 6.1.1: Dynamic Batch Sizing

**Hi·ªán t·∫°i**: Fixed batch size = 12

**ƒê·ªÅ xu·∫•t**: Adaptive batch size

```python
def calculate_optimal_batch_size(
    product_count: int,
    available_workers: int,
    avg_processing_time: float
) -> int:
    """T√≠nh optimal batch size d·ª±a tr√™n context"""
    
    # M·ª•c ti√™u: M·ªói batch x·ª≠ l√Ω trong 30-60 gi√¢y
    target_batch_time = 45  # seconds
    
    # ∆Ø·ªõc t√≠nh products per batch
    products_per_batch = target_batch_time / avg_processing_time
    
    # ƒê·∫£m b·∫£o kh√¥ng qu√° nh·ªè ho·∫∑c qu√° l·ªõn
    min_batch = 5
    max_batch = 20
    
    optimal = max(min_batch, min(max_batch, int(products_per_batch)))
    
    return optimal
```

**Expected**: Batch efficiency tƒÉng 10-15%

---

#### ‚úÖ Task 6.2.2: Parallel Processing Optimization

**Impact**: +10-15%  
**Effort**: 2-3 gi·ªù

**Hi·ªán t·∫°i**: C√≥ th·ªÉ c√≥ sequential processing

**ƒê·ªÅ xu·∫•t**: Parallelize t·∫•t c·∫£ independent tasks

```python
# Thay v√¨ sequential
for product in products:
    detail = crawl_detail(product.url)
    transform = transform_product(detail)
    save(transform)

# D√πng parallel
async def process_product(product):
    detail = await crawl_detail(product.url)
    transform = transform_product(detail)
    await save(transform)

# Run parallel
tasks = [process_product(p) for p in products]
await asyncio.gather(*tasks, return_exceptions=True)
```

**Expected**: Throughput tƒÉng 10-15%

---

### 6.2 Data Structure Optimization

**Impact**: +10-15%  
**Effort**: 2-3 gi·ªù

#### ‚úÖ Task 6.2.1: Use Generators Thay V√¨ Lists

```python
# Thay v√¨
def get_products():
    products = []
    for category in categories:
        products.extend(get_products_from_category(category))
    return products  # Load t·∫•t c·∫£ v√†o memory

# D√πng generator
def get_products():
    for category in categories:
        yield from get_products_from_category(category)
        # Stream processing, kh√¥ng load h·∫øt v√†o memory
```

**Expected**: Memory usage gi·∫£m, c√≥ th·ªÉ process nhi·ªÅu products h∆°n

---

#### ‚úÖ Task 6.2.2: Cache Data Structures

**Impact**: +5-10%  
**Effort**: 1-2 gi·ªù

```python
# Cache parsed HTML trees
@lru_cache(maxsize=1000)
def parse_html(html_content: str):
    return BeautifulSoup(html_content, 'html.parser')

# Cache regex patterns
PRICE_PATTERN = re.compile(r'(\d{1,3}(?:\.\d{3})*)')
# Reuse thay v√¨ compile m·ªói l·∫ßn
```

**Expected**: Parsing speed tƒÉng 5-10%

---

## üìã IMPLEMENTATION PRIORITY

### üî¥ HIGH PRIORITY (B·∫Øt ƒê·∫ßu Ngay)

1. **Configuration Tuning** (1-2 gi·ªù)
   - TƒÉng DNS cache TTL ‚Üí 1800s
   - Optimize connection pool limits
   - ‚úÖ **Impact**: +20-30%, **Risk**: Th·∫•p

2. **Adaptive Rate Limiting** (2-4 gi·ªù)
   - Dynamic delay based on success/error rate
   - ‚úÖ **Impact**: +20-30%, **Risk**: Trung b√¨nh

3. **Browser Flags Optimization** (1 gi·ªù)
   - Add performance flags
   - Block unnecessary resources
   - ‚úÖ **Impact**: +20-30%, **Risk**: Th·∫•p

### üü° MEDIUM PRIORITY (Tu·∫ßn n√†y)

4. **Smart Waiting** (2-4 gi·ªù)
   - Replace time.sleep() v·ªõi explicit waits
   - Skip unnecessary waits
   - ‚úÖ **Impact**: +15-25%, **Risk**: Th·∫•p

5. **Caching Strategy** (2-4 gi·ªù)
   - URL normalization
   - Partial cache
   - ‚úÖ **Impact**: +15-20%, **Risk**: Th·∫•p

6. **Code Algorithm** (4-8 gi·ªù)
   - Dynamic batch sizing
   - Parallel processing
   - ‚úÖ **Impact**: +10-20%, **Risk**: Th·∫•p

### üü¢ LOW PRIORITY (Tu·∫ßn sau)

7. **Data Structure** (2-3 gi·ªù)
   - Use generators
   - Cache data structures
   - ‚úÖ **Impact**: +10-15%, **Risk**: Th·∫•p

8. **Network Optimization** (1-2 gi·ªù)
   - HTTP/2 support
   - Request header optimization
   - ‚úÖ **Impact**: +10-15%, **Risk**: Th·∫•p

---

## üöÄ QUICK START PLAN

### Ng√†y 1 (2-3 gi·ªù)
- [x] Task 1.1.1: TƒÉng DNS cache TTL
- [x] Task 1.1.2: T·ªëi ∆Øu connection pool
- [x] Task 4.1.1: Add browser flags

**Expected**: +30-40% improvement

### Ng√†y 2 (4-6 gi·ªù)
- [x] Task 3.2.1: Adaptive rate limiting
- [x] Task 5.1.1: Smart waiting
- [x] Task 4.1.2: Block resources

**Expected**: +40-50% cumulative improvement

### Ng√†y 3 (4-6 gi·ªù)
- [x] Task 2.1.1: URL normalization
- [x] Task 2.1.3: Partial cache
- [x] Task 6.1.1: Dynamic batch sizing

**Expected**: +60-70% cumulative improvement

---

## üìä EXPECTED RESULTS

### After All Optimizations

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **T·ªëc ƒë·ªô crawl** | 2.8 products/min | **5-8 products/min** | **+79-186%** |
| **Cache hit rate** | 35-40% | **50-60%** | **+15-20%** |
| **Average delay** | 0.7s | **0.4-0.5s** | **-30-40%** |
| **Browser load time** | 5-9s | **3-6s** | **-30-40%** |
| **Memory usage** | Current | **Same/Reduced** | - |

### Timeline

- **Tu·∫ßn 1**: Implement High Priority tasks ‚Üí +50-70% improvement
- **Tu·∫ßn 2**: Implement Medium Priority tasks ‚Üí +70-90% cumulative
- **Tu·∫ßn 3**: Implement Low Priority tasks ‚Üí +90-100% cumulative

**Total Expected**: **2-3x faster** m√† kh√¥ng c·∫ßn hardware changes!

---

## ‚ö†Ô∏è L∆ØU √ù

### Risks & Mitigation

1. **Adaptive Rate Limiting**
   - Risk: C√≥ th·ªÉ b·ªã block n·∫øu qu√° aggressive
   - Mitigation: Start conservative, monitor errors, adjust d·∫ßn

2. **Browser Flags**
   - Risk: M·ªôt s·ªë flags c√≥ th·ªÉ break functionality
   - Mitigation: Test t·ª´ng flag, rollback n·∫øu c√≥ v·∫•n ƒë·ªÅ

3. **Configuration Changes**
   - Risk: C√≥ th·ªÉ g√¢y unexpected behavior
   - Mitigation: Test trong staging tr∆∞·ªõc, monitor metrics

---

## ‚úÖ CHECKLIST

### Immediate (H√¥m nay)
- [ ] TƒÉng DNS cache TTL ‚Üí 1800s
- [ ] Optimize connection pool limits
- [ ] Add browser performance flags

### This Week
- [ ] Implement adaptive rate limiting
- [ ] Replace time.sleep() v·ªõi explicit waits
- [ ] Block unnecessary browser resources
- [ ] URL normalization cho cache

### Next Week
- [ ] Partial cache strategy
- [ ] Dynamic batch sizing
- [ ] Parallel processing optimization
- [ ] Data structure improvements

---

**T·ªïng k·∫øt**: V·ªõi c√°c t·ªëi ∆∞u tr√™n, c√≥ th·ªÉ ƒë·∫°t **2-3x improvement** m√† kh√¥ng c·∫ßn b·∫•t k·ª≥ thay ƒë·ªïi hardware n√†o. T·∫•t c·∫£ ƒë·ªÅu l√† code/configuration changes v√† c√≥ th·ªÉ implement ngay!

