# üìä B·∫¢N CHI TI·∫æT T·∫§T C·∫¢ THAM S·ªê D·ª∞ √ÅN TIKI DATA PIPELINE

**C·∫≠p nh·∫≠t:** 18/11/2025 | **Phi√™n b·∫£n:** T·ªëi ∆∞u h√≥a v2  
**Tr·∫°ng th√°i:** ‚úÖ T·∫•t c·∫£ tham s·ªë ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u v√† tri·ªÉn khai

---

## üéØ I. PH√ÇN LO·∫†I THAM S·ªê

D·ª± √°n s·ª≠ d·ª•ng 5 l·ªõp tham s·ªë:

1. **Airflow Variables** - C·∫•u h√¨nh th·ª±c thi DAG (Admin ‚Üí Variables)
2. **Environment Variables** (`.env`) - C·∫•u h√¨nh h·ªá th·ªëng & database
3. **Code Config** (`config.py`) - C·∫•u h√¨nh pipeline c·ª©ng
4. **Pool Configuration** - C·∫•u h√¨nh connection/thread pooling
5. **HTTP Client Config** - C·∫•u h√¨nh aiohttp & Selenium

---

## üìã II. CHI TI·∫æT THAM S·ªê AIRFLOW VARIABLES

### 2.1 Category Crawling Configuration

| Tham S·ªë | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã M·∫∑c ƒê·ªãnh | Gi·ªõi H·∫°n | √ù Nghƒ©a |
|---------|-----------------|-----------------|---------|--------|
| `TIKI_MAX_CATEGORIES` | `0` | `0` | 0-1000 | S·ªë danh m·ª•c t·ªëi ƒëa ƒë·ªÉ crawl. `0` = crawl t·∫•t c·∫£ |
| `TIKI_MIN_CATEGORY_LEVEL` | `2` | `2` | 1-5 | M·ª©c ƒë·ªô danh m·ª•c t·ªëi thi·ªÉu (s√¢u nh·∫•t trong c√¢y) |
| `TIKI_MAX_CATEGORY_LEVEL` | `4` | `4` | 1-10 | M·ª©c ƒë·ªô danh m·ª•c t·ªëi ƒëa (n√¥ng nh·∫•t) |
| `TIKI_MAX_PAGES_PER_CATEGORY` | `20` | `20` | 1-100 | S·ªë trang s·∫£n ph·∫©m t·ªëi ƒëa per danh m·ª•c |
| `TIKI_CRAWL_TIMEOUT` | `300` | `300` | 60-3600 | Timeout cho category crawl (gi√¢y) - **5 ph√∫t** |
| `TIKI_RATE_LIMIT_DELAY` | `1.0` | `1.0` | 0.1-10.0 | Delay gi·ªØa c√°c request (gi√¢y) ƒë·ªÉ tr√°nh rate limit |
| `TIKI_USE_SELENIUM` | `false` | `false` | true/false | S·ª≠ d·ª•ng Selenium thay v√¨ API tr·ª±c ti·∫øp |

**√ù nghƒ©a ho·∫°t ƒë·ªông:**
- Crawl t·∫•t c·∫£ danh m·ª•c c·∫•p 2-4 c·ªßa Tiki
- M·ªói danh m·ª•c crawl t·ªëi ƒëa 20 trang (m·ªói trang ~40 s·∫£n ph·∫©m)
- Gi·ªØa m·ªói request delay 1 gi√¢y ƒë·ªÉ tr√°nh b·ªã block
- Timeout 5 ph√∫t n·∫øu danh m·ª•c ch·∫≠m

---

### 2.2 Product Crawling Configuration

| Tham S·ªë | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã M·∫∑c ƒê·ªãnh | Gi·ªõi H·∫°n | √ù Nghƒ©a |
|---------|-----------------|-----------------|---------|--------|
| `TIKI_PRODUCTS_PER_DAY` | `280` | `280` | 1-10000 | S·ªë s·∫£n ph·∫©m ƒë·ªÉ crawl chi ti·∫øt m·ªói DAG run |
| `TIKI_MAX_PRODUCTS_FOR_DETAIL` | `0` | `0` | 0-1000000 | T·ªëi ƒëa s·∫£n ph·∫©m crawl detail. `0` = kh√¥ng limit |
| `TIKI_SAVE_BATCH_SIZE` | `10000` | `10000` | 100-100000 | S·ªë s·∫£n ph·∫©m m·ªói batch khi l∆∞u DB |

**√ù nghƒ©a:**
- M·ªói DAG run crawl 280 s·∫£n ph·∫©m chi ti·∫øt
- L∆∞u v√†o DB theo batch 10000 s·∫£n ph·∫©m (tr√°nh qu√° t·∫£i b·ªô nh·ªõ)

---

### 2.3 Selenium Pool Configuration (T·ªëi ∆∞u h√≥a v2)

| Tham S·ªë | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã C≈© | Gi·ªõi H·∫°n | √ù Nghƒ©a |
|---------|-----------------|-----------|---------|--------|
| `TIKI_DETAIL_POOL_SIZE` | `15` | `5` | 1-50 | S·ªë Selenium driver ch·∫°y song song |
| `TIKI_DETAIL_MAX_CONCURRENT_TASKS` | `15` | `5` | 1-50 | S·ªë task song song t·ªëi ƒëa |
| `TIKI_DETAIL_RATE_LIMIT_DELAY` | `1.5` | `1.5` | 0.1-10.0 | Delay gi·ªØa c√°c detail request (gi√¢y) |

**T√°c d·ª•ng t·ªëi ∆∞u h√≥a:**
- **Pool size 5‚Üí15**: +200% x·ª≠ l√Ω ƒë·ªìng th·ªùi
  - 5 drivers = 5 s·∫£n ph·∫©m c√πng l√∫c ‚Üí 1 product/driver
  - 15 drivers = 15 s·∫£n ph·∫©m c√πng l√∫c ‚Üí 3x nhanh h∆°n
  - Crawl 280 s·∫£n ph·∫©m: 56 batches ‚Üí 19 batches

---

### 2.4 Circuit Breaker Configuration (Resilience)

| Tham S·ªë | Gi√° Tr·ªã | Gi·ªõi H·∫°n | √ù Nghƒ©a |
|---------|--------|---------|--------|
| `TIKI_CIRCUIT_BREAKER_FAILURE_THRESHOLD` | `5` | 1-20 | S·ªë l·ªói tr∆∞·ªõc khi v√†o tr·∫°ng th√°i OPEN |
| `TIKI_CIRCUIT_BREAKER_RECOVERY_TIMEOUT` | `60` | 10-600 | Th·ªùi gian ƒë·ª£i tr∆∞·ªõc khi th·ª≠ l·∫°i (gi√¢y) |

**√ù nghƒ©a:** 
- N·∫øu 5 request li√™n ti·∫øp fail ‚Üí d·ª´ng crawl, ch·ªù 60s, r·ªìi th·ª≠ l·∫°i
- Tr√°nh "l√£ng ph√≠" time crawl tr√™n l·ªói li√™n t·ª•c

---

### 2.5 Degradation Mode Configuration (Fallback)

| Tham S·ªë | Gi√° Tr·ªã | Gi·ªõi H·∫°n | √ù Nghƒ©a |
|---------|--------|---------|--------|
| `TIKI_DEGRADATION_FAILURE_THRESHOLD` | `3` | 1-20 | S·ªë l·ªói tr∆∞·ªõc khi chuy·ªÉn mode degradation |
| `TIKI_DEGRADATION_RECOVERY_THRESHOLD` | `5` | 1-20 | S·ªë success tr∆∞·ªõc khi tho√°t degradation |

**√ù nghƒ©a:**
- N·∫øu 3 l·ªói li√™n ti·∫øp ‚Üí gi·∫£m t·ªëc ƒë·ªô (skip some tasks)
- N·∫øu 5 success li√™n ti·∫øp ‚Üí quay l·∫°i b√¨nh th∆∞·ªùng

---

### 2.6 Database Configuration

| Tham S·ªë | Gi√° Tr·ªã | M·∫∑c ƒê·ªãnh | √ù Nghƒ©a |
|---------|--------|---------|--------|
| `POSTGRES_HOST` | `postgres` | `localhost` | Host PostgreSQL (Docker container name) |
| `POSTGRES_PORT` | `5432` | `5432` | Port PostgreSQL |
| `POSTGRES_USER` | `postgres` | `postgres` | T√™n user PostgreSQL |
| `POSTGRES_PASSWORD` | `***` | (t·ª´ .env) | M·∫≠t kh·∫©u PostgreSQL |

---

### 2.7 Redis Configuration

| Tham S·ªë | Gi√° Tr·ªã | M·∫∑c ƒê·ªãnh | √ù Nghƒ©a |
|---------|--------|---------|--------|
| `REDIS_URL` | `redis://redis:6379/3` | (ch√≠nh s√°ch) | URL Redis cho caching |

**L∆∞u √Ω:**
- DB 0: Airflow Celery broker (t·ª± ƒë·ªông)
- DB 1: Pipeline caching
- DB 3: DAG monitoring

---

### 2.8 DAG Scheduling Configuration

| Tham S·ªë | Gi√° Tr·ªã | M·∫∑c ƒê·ªãnh | √ù Nghƒ©a |
|---------|--------|---------|--------|
| `TIKI_DAG_SCHEDULE_MODE` | `manual` | `manual` | `manual` ho·∫∑c cron schedule |

---

## üîß III. CHI TI·∫æT ENVIRONMENT VARIABLES (`.env`)

### 3.1 Core System Configuration

```env
# Airflow Container Setup
AIRFLOW_UID=50000                              # User ID cho Airflow process
AIRFLOW_PROJ_DIR=.                             # Project root directory
AIRFLOW_IMAGE_NAME=tiki-airflow:3.1.2         # Custom Docker image
ENV_FILE_PATH=.env                             # Path t·ªõi .env file

# Python Dependencies
_PIP_ADDITIONAL_REQUIREMENTS=selenium>=4.0.0 beautifulsoup4>=4.12.0 requests>=2.31.0 lxml>=4.9.0 tqdm>=4.65.0 webdriver-manager>=4.0.0
```

### 3.2 PostgreSQL Configuration

```env
POSTGRES_USER=postgres                         # T√™n user database
POSTGRES_PASSWORD=your_secure_password_here   # M·∫≠t kh·∫©u (NEVER commit!)
```

### 3.3 Redis Configuration

```env
REDIS_PASSWORD=                               # Redis password (n·∫øu c·∫ßn)
```

### 3.4 Airflow Web UI Credentials

```env
_AIRFLOW_WWW_USER_USERNAME=airflow            # Username Airflow UI
_AIRFLOW_WWW_USER_PASSWORD=airflow            # Password Airflow UI
```

---

## üìù IV. CHI TI·∫æT CODE CONFIGURATION (`config.py`)

### 4.1 Category Crawling Configuration

```python
# T·ª´: src/pipelines/crawl/config.py

CATEGORY_BATCH_SIZE = 5              # Categories per batch
CATEGORY_TIMEOUT = 120               # Seconds (t·ª´ 180 ‚Üí 120) ‚ö°
CATEGORY_CONCURRENT_REQUESTS = 5    # HTTP requests ƒë·ªìng th·ªùi (t·ª´ 3 ‚Üí 5) ‚ö°
CATEGORY_POOL_SIZE = 8               # Selenium drivers cho category
```

**Gi·∫£i th√≠ch:**
- **BATCH_SIZE=5**: Crawl 5 danh m·ª•c c√πng l√∫c
- **TIMEOUT=120**: N·∫øu 1 batch v∆∞·ª£t 120s th√¨ fail (t·ª´ 180s)
- **CONCURRENT=5**: M·ªói batch g·ª≠i 5 HTTP request song song (t·ª´ 3)
- **POOL_SIZE=8**: C√≥ 8 Selenium driver s·∫µn cho category

### 4.2 Product Crawling Configuration

```python
PRODUCT_BATCH_SIZE = 12              # Products per batch (t·ª´ 15 ‚Üí 12) ‚ö°
PRODUCT_TIMEOUT = 60                 # Seconds per batch (t·ª´ 90 ‚Üí 60) ‚ö°
PRODUCT_POOL_SIZE = 15               # Selenium drivers (t·ª´ 5 ‚Üí 15) ‚ö°
```

**T√°c d·ª•ng t·ªëi ∆∞u h√≥a:**
- **BATCH_SIZE 15‚Üí12**: 
  - 280 products √∑ 15 = 19 batches
  - 280 products √∑ 12 = 23 batches
  - **+4 batches = 21% song song h∆°n** üöÄ
- **TIMEOUT 90‚Üí60**: Fail nhanh, retry s·ªõm h∆°n
- **POOL_SIZE 5‚Üí15**: 3x x·ª≠ l√Ω ƒë·ªìng th·ªùi

### 4.3 HTTP Client Configuration

```python
HTTP_CONNECTOR_LIMIT = 100           # T·ªïng concurrent HTTP connections
HTTP_CONNECTOR_LIMIT_PER_HOST = 10  # Per-host limit (Tiki.vn)
HTTP_TIMEOUT_TOTAL = 20              # Gi√¢y (t·ª´ 30 ‚Üí 20) ‚ö°
HTTP_TIMEOUT_CONNECT = 10            # Gi√¢y connect timeout
HTTP_DNS_CACHE_TTL = 300             # Gi√¢y (5 ph√∫t DNS cache)
```

**Gi·∫£i th√≠ch:**
- **LIMIT=100**: T·ªëi ƒëa 100 TCP connection c√πng l√∫c
- **LIMIT_PER_HOST=10**: T·ªëi ƒëa 10 ƒë·∫øn tiki.vn (tu√¢n th·ªß rate limit)
- **TIMEOUT_TOTAL=20**: N·∫øu request >20s fail (t·ª´ 30s)
- **TIMEOUT_CONNECT=10**: N·∫øu connect >10s fail ngay
- **DNS_CACHE_TTL=300**: L∆∞u cache DNS 5 ph√∫t (tr√°nh DNS query m·ªói l·∫ßn)

---

## üèä V. CHI TI·∫æT DATABASE POOL CONFIGURATION

### 5.1 PostgreSQL Connection Pool

**T·ª´:** `src/pipelines/load/db_pool.py`

```python
class PostgresConnectionPool:
    def initialize(
        minconn: int = 2,              # T·ªëi thi·ªÉu connection
        maxconn: int = 10,             # T·ªëi ƒëa connection
        connect_timeout: int = 10,     # Gi√¢y timeout connect
        statement_timeout: int = 30000 # 30s statement timeout
    )
```

| Tham S·ªë | Gi√° Tr·ªã | √ù Nghƒ©a |
|---------|--------|--------|
| `minconn` | 2 | Lu√¥n m·ªü 2 connection |
| `maxconn` | 10 | T·ªëi ƒëa 10 connection c√πng l√∫c |
| `connect_timeout` | 10s | Timeout k·∫øt n·ªëi |
| `statement_timeout` | 30s | Timeout per SQL statement |

**T√°c d·ª•ng:**
- **T√°i s·ª≠ d·ª•ng connection**: Thay v√¨ m·ªü/ƒë√≥ng m·ªói l·∫ßn = 40-50% nhanh h∆°n
- **Singleton pattern**: M·ªôt pool duy nh·∫•t cho to√†n ·ª©ng d·ª•ng
- **Thread-safe**: `ThreadedConnectionPool` t·ª´ psycopg2

---

## üì¶ VI. CHI TI·∫æT BATCH PROCESSOR CONFIGURATION

**T·ª´:** `src/common/batch_processor.py`

```python
class BatchProcessor:
    def __init__(
        batch_size: int = 100,         # S·ªë items per batch
        show_progress: bool = True,    # Hi·ªÉn th·ªã progress
        continue_on_error: bool = True # Ti·∫øp t·ª•c n·∫øu batch fail
    )
```

| Tham S·ªë | Gi√° Tr·ªã | √ù Nghƒ©a |
|---------|--------|--------|
| `batch_size` | 100 | X·ª≠ l√Ω 100 items c√πng l√∫c |
| `show_progress` | True | Log chi ti·∫øt progress |
| `continue_on_error` | True | Skip batch fail, ti·∫øp t·ª•c batch ti·∫øp |

**S·ª≠ d·ª•ng:**
```python
# L∆∞u 1000 products v√†o DB
processor = BatchProcessor(batch_size=100)  # 10 batches
processor.process(products, save_to_db)     # Process song song
```

---

## üîó VII. CHI TI·∫æT REDIS CONNECTION POOL CONFIGURATION

**T·ª´:** `src/pipelines/crawl/storage/redis_cache.py`

```python
def get_redis_pool(
    redis_url: str,
    max_connections: int = 20  # T·ªëi ƒëa connection
) -> ConnectionPool
```

| Tham S·ªë | Gi√° Tr·ªã | √ù Nghƒ©a |
|---------|--------|--------|
| `max_connections` | 20 | T·ªëi ƒëa 20 Redis connection |
| `socket_connect_timeout` | 5s | Timeout connect |
| `socket_timeout` | 5s | Timeout socket |
| `retry_on_timeout` | True | Retry n·∫øu timeout |

**√ù nghƒ©a:**
- **Connection pooling**: T√°i s·ª≠ d·ª•ng Redis connection = 20-30% nhanh h∆°n
- **Multiple databases**: 
  - DB 0: Airflow Celery (t·ª± ƒë·ªông)
  - DB 1: Cache (crawl HTML, responses)
  - DB 3: DAG monitoring

---

## üéõÔ∏è VIII. CHI TI·∫æT TASK-LEVEL TIMEOUT CONFIGURATION

**T·ª´:** `airflow/dags/tiki_crawl_products_dag.py`

### 8.1 Category Crawl Task

| Th√†nh Ph·∫ßn | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã C≈© | √ù Nghƒ©a |
|-----------|-----------------|-----------|--------|
| `execution_timeout` | 12 min | 15 min | Timeout per category batch task |
| `retries` | 1 | 2 | Retry l·∫ßn n·∫øu fail |
| `retry_delay` | 15s | 2min | Ch·ªù tr∆∞·ªõc khi retry |

### 8.2 Product Detail Task

| Th√†nh Ph·∫ßn | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã C≈© | √ù Nghƒ©a |
|-----------|-----------------|-----------|--------|
| `SeleniumDriverPool` | 15 drivers | 5 drivers | Pool size |
| `timeout` | 60s | 90s | Selenium timeout |
| `execution_timeout` | Kh√¥ng limit | 60min | DAG task timeout |

### 8.3 Merge Task

| Th√†nh Ph·∫ßn | Gi√° Tr·ªã Hi·ªán T·∫°i | Gi√° Tr·ªã C≈© | √ù Nghƒ©a |
|-----------|-----------------|-----------|--------|
| `execution_timeout` | 30 min | 60 min | G·ªôp d·ªØ li·ªáu 280 s·∫£n ph·∫©m |

---

## üåç IX. SELENIUM WEBDRIVER CONFIGURATION

**T·ª´:** `src/pipelines/crawl/utils.py`

```python
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--headless")                    # Ch·∫ø ƒë·ªô kh√¥ng giao di·ªán
chrome_options.add_argument("--disable-gpu")                # T·∫Øt GPU (ch·ªâ headless)
chrome_options.add_argument("--no-sandbox")                 # T·∫Øt sandbox (Docker)
chrome_options.add_argument("--disable-dev-shm-usage")      # T·∫Øt /dev/shm (Docker)
chrome_options.add_argument("--disable-software-rasterizer") # T·∫Øt rasterizer
chrome_options.add_argument("--start-maximized")            # Maximize window
chrome_options.add_argument("--disable-extensions")         # T·∫Øt extension
```

---

## üìä X. T√ìML√ù T·∫§T C·∫¢ T·ªêI ∆ØU H√ìA V2

| Tham S·ªë | T·ª´ | Th√†nh | +/- % | T√°c D·ª•ng |
|---------|-----|--------|-------|---------|
| **SELENIUM_POOL** | 5 | 15 | +200% | 3x x·ª≠ l√Ω ƒë·ªìng th·ªùi |
| **BATCH_SIZE** | 15 | 12 | +92% para | 23 vs 19 batches |
| **PRODUCT_TIMEOUT** | 90s | 60s | -33% | Fail nhanh h∆°n |
| **HTTP_TIMEOUT** | 30s | 20s | -33% | Request nhanh h∆°n |
| **HTTP_CONNECTOR** | N/A | 100 | NEW | Connection pooling |
| **RETRY** | 2x,2min | 1x,30s | -75% wait | Ph·ª•c h·ªìi nhanh |
| **CATEGORY_TIMEOUT** | 180s | 120s | -33% | Batch nhanh h∆°n |
| **CATEGORY_CONCURRENT** | 3 | 5 | +67% | G·ª≠i request song song |

**D·ª± ki·∫øn hi·ªáu su·∫•t:**
- **Crawl 280 products**: 45 ph√∫t ‚Üí **12-15 ph√∫t** ‚ú®
- **T·ªïng c·∫£i ti·∫øn: 3-4x nhanh h∆°n** üöÄ

---

## üéØ XI. H∆Ø·ªöNG D·∫™N TUNING THAM S·ªê

### N·∫øu mu·ªën **T·ªêC H∆†N·ªÆA**:

```bash
# Airflow Variables
TIKI_DETAIL_POOL_SIZE = 30              # TƒÉng t·ª´ 15 ‚Üí 30 drivers
TIKI_DETAIL_MAX_CONCURRENT_TASKS = 30   # TƒÉng t·ª´ 15 ‚Üí 30 tasks
TIKI_MAX_PAGES_PER_CATEGORY = 30        # TƒÉng t·ª´ 20 ‚Üí 30 pages
```

‚ö†Ô∏è **Risk**: C√≥ th·ªÉ b·ªã rate limit t·ª´ Tiki.vn ho·∫∑c l·ªói OOM (out of memory)

### N·∫øu mu·ªën **AN TO√ÄN H∆†N**:

```bash
# Airflow Variables
TIKI_DETAIL_POOL_SIZE = 8               # Gi·∫£m t·ª´ 15 ‚Üí 8 drivers
TIKI_DETAIL_RATE_LIMIT_DELAY = 3.0     # TƒÉng t·ª´ 1.5 ‚Üí 3.0 gi√¢y
TIKI_RATE_LIMIT_DELAY = 2.0            # TƒÉng t·ª´ 1.0 ‚Üí 2.0 gi√¢y
```

‚úÖ **Benefit**: √çt l·ªói, √≠t b·ªã block, nh∆∞ng ch·∫≠m h∆°n

### N·∫øu g·∫∑p **L·ªñI MEMORY**:

```python
# Trong config.py
PRODUCT_BATCH_SIZE = 8                  # Gi·∫£m t·ª´ 12 ‚Üí 8 s·∫£n ph·∫©m/batch
CATEGORY_BATCH_SIZE = 3                 # Gi·∫£m t·ª´ 5 ‚Üí 3 danh m·ª•c/batch
```

---

## üìö XII. THAM KH·∫¢O CHI TI·∫æT

| Ngu·ªìn File | D√≤ng | Tham S·ªë |
|-----------|------|--------|
| `airflow/dags/tiki_crawl_products_dag.py` | 1980 | POOL_SIZE=15, timeout=60s |
| `airflow/dags/tiki_crawl_products_dag.py` | 5383 | Category task timeout=12min |
| `airflow/dags/tiki_crawl_products_dag.py` | 5481 | BATCH_SIZE=12 |
| `src/pipelines/crawl/config.py` | 15-31 | T·∫•t c·∫£ config t·ªëi ∆∞u h√≥a |
| `src/pipelines/crawl/crawl_products_detail.py` | 915-928 | aiohttp TCPConnector |
| `src/pipelines/load/db_pool.py` | 38-90 | Database pool config |
| `src/pipelines/crawl/storage/redis_cache.py` | 35-60 | Redis pool config |

---

## ‚úÖ CHECKLSIT TUNING THAM S·ªê

Tr∆∞·ªõc khi DAG ch·∫°y:

- [ ] Ki·ªÉm tra Airflow Variables t·∫°i Admin ‚Üí Variables
- [ ] Ki·ªÉm tra .env file ƒë√£ ƒë∆∞·ª£c set (POSTGRES_USER, PASSWORD)
- [ ] Ki·ªÉm tra Redis ƒëang ch·∫°y: `docker-compose ps | grep redis`
- [ ] Ki·ªÉm tra PostgreSQL ƒëang ch·∫°y: `docker-compose ps | grep postgres`
- [ ] Xem log: `docker-compose logs -f airflow-scheduler`

N·∫øu DAG ch·∫°y ch·∫≠m:

1. TƒÉng `TIKI_DETAIL_POOL_SIZE` ‚Üí 20-25
2. TƒÉng `HTTP_CONNECTOR_LIMIT` ‚Üí 150-200 (trong config.py)
3. Gi·∫£m retry_delay ‚Üí 10s

N·∫øu DAG b·ªã error (rate limit):

1. TƒÉng `TIKI_DETAIL_RATE_LIMIT_DELAY` ‚Üí 2.0-3.0
2. Gi·∫£m `TIKI_DETAIL_POOL_SIZE` ‚Üí 8-10
3. Gi·∫£m `TIKI_MAX_PAGES_PER_CATEGORY` ‚Üí 10

---

## üìû LI√äN H·ªÜ & H·ªñ TR·ª¢

ƒê·ªÉ xem gi√° tr·ªã hi·ªán t·∫°i c·ªßa tham s·ªë:

```bash
# Xem Airflow Variables
docker exec tiki-data-pipeline-airflow-scheduler-1 airflow variables list

# Xem Environment Variables
docker exec tiki-data-pipeline-postgres-1 env | grep POSTGRES

# Xem Database Connection Pool
docker exec tiki-data-pipeline-airflow-scheduler-1 python -c "from src.pipelines.load.db_pool import PostgresConnectionPool; p = PostgresConnectionPool(); print(p._pool)"
```

---

**T·∫°o b·ªüi:** GitHub Copilot  
**Ng√†y:** 18/11/2025  
**B·∫£n quy·ªÅn:** Tiki Data Pipeline Project
