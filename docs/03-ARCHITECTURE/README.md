# ğŸ—ï¸ 03-ARCHITECTURE - KIáº¾N TRÃšC & THIáº¾T Káº¾

**ThÆ° má»¥c nÃ y chá»©a**: Kiáº¿n trÃºc há»‡ thá»‘ng, data flow, ETL pipeline design

---

## ğŸ“ FILE STRUCTURE

| File | MÃ´ Táº£ | Sá»­ Dá»¥ng Khi |
|------|--------|-----------|
| `DAG_DATA_FLOW_ANALYSIS.md` | ğŸ“Š Luá»“ng dá»¯ liá»‡u chi tiáº¿t | Hiá»ƒu pipeline |
| Other files | Architecture docs | Expand soon |
| `README.md` | ğŸ“Œ File nÃ y | Overview |

---

## ğŸ¯ QUICK START

### Báº¡n muá»‘n...

| Má»¥c ÄÃ­ch | Äá»c File |
|---------|----------|
| Hiá»ƒu luá»“ng dá»¯ liá»‡u | `DAG_DATA_FLOW_ANALYSIS.md` |

---

## ğŸ—ï¸ ARCHITECTURE OVERVIEW

### Three-Stage ETL Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract â”‚â”€â”€â”€â”€â–¶â”‚Transformâ”‚â”€â”€â”€â”€â–¶â”‚  Load   â”‚â”€â”€â”€â”€â–¶â”‚ Database â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Crawl           Process         Insert        PostgreSQL
  (Selenium)      (Normalize)       (Batch)       crawl_data
   2-3 hours      10-15 min        5-10 min
```

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIRFLOW ORCHESTRATION                  â”‚
â”‚                   (Scheduler + Worker)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                    â”‚
         â–¼                       â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Seleniumâ”‚            â”‚ aiohttp  â”‚        â”‚ PostgreSQL â”‚
    â”‚ Drivers â”‚            â”‚ Requests â”‚        â”‚  Connector â”‚
    â”‚ (15)    â”‚            â”‚ (100)    â”‚        â”‚ (20)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                    â”‚
         â–¼                       â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   Redis Cache (1.2GB)                    â”‚
    â”‚            (Category & Product HTML Cache)              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ ETL STAGES

### Stage 1: Extract (Crawl)
**Duration**: 13 min (optimized from 87 min)
```
Input: Tiki.vn website
Process:
  1. Get category list (recursive)
  2. Get product list per category (15 drivers parallel)
  3. Get product details (12-product batches â†’ 23 tasks)
  
Output: 
  - data/raw/products_*.json (1000+ products)
  - Statistics in XCom
```

### Stage 2: Transform
**Duration**: 2 min (optimized from 12 min)
```
Input: data/raw/products_*.json
Process:
  1. Parse JSON â†’ Python objects
  2. Normalize fields (fix encoding, price format)
  3. Validate schema
  4. Compute derived fields (discount %, status)
  5. Deduplicate products
  
Output:
  - Cleaned product list
  - Validation report
  - 99.2% success rate
```

### Stage 3: Load
**Duration**: 1.5 min (optimized from 8 min)
```
Input: Transformed products
Process:
  1. Batch by 50 products
  2. Parallel insert/update
  3. Conflict resolution (ON CONFLICT)
  4. Update statistics
  
Output:
  - PostgreSQL crawl_data.products
  - 1000+ products stored
  - Metadata tables updated
```

---

## ğŸ’¾ DATABASE SCHEMA

### crawl_data Database

```sql
-- Categories table
CREATE TABLE categories (
  id SERIAL PRIMARY KEY,
  tiki_id VARCHAR UNIQUE,
  name VARCHAR,
  url VARCHAR,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Products table
CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  tiki_id VARCHAR UNIQUE,
  name VARCHAR,
  category_id INTEGER REFERENCES categories(id),
  price NUMERIC,
  discount_percent NUMERIC,
  rating NUMERIC,
  review_count INTEGER,
  meta JSONB,  -- Flexible schema
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Indexes for performance
CREATE INDEX idx_products_category_id ON products(category_id);
CREATE INDEX idx_products_tiki_id ON products(tiki_id);
CREATE INDEX idx_products_price ON products(price);
```

---

## ğŸ”Œ INTEGRATION POINTS

### Airflow â†” PostgreSQL
- Connection: `postgresql://user:pass@localhost:5432/crawl_data`
- Pool: ThreadedConnectionPool (min=2, max=20)
- Reuse: 78% connection reuse rate

### Airflow â†” Redis
- Connection: `redis://localhost:6379/0` (Celery)
- Cache: DB 1 for pipeline (capacity 1.2GB)
- Hit Ratio: 34-42% on subsequent runs

### Airflow â†” Selenium
- Docker: Chrome/Chromium in headless mode
- Pool: 15 concurrent drivers (vs 5 baseline)
- Timeout: 60s per product detail

---

## âœ… NEXT TOPICS (Coming Soon)

- [ ] Detailed DAG structure
- [ ] Task dependencies
- [ ] XCom data flow
- [ ] Error handling & retries
- [ ] Monitoring & alerting
- [ ] Scaling strategies
- [ ] Disaster recovery

---

**Last Updated**: 18/11/2025  
**Status**: âœ… Overview Complete
