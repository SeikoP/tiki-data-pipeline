# ƒê√°nh Gi√° Kh·∫£ NƒÉng Chuy·ªÉn Sang JavaScript cho Crawl

## üìã T·ªïng Quan D·ª± √Ån Hi·ªán T·∫°i

### Tech Stack:
- **Orchestration**: Apache Airflow 3.1.2 (Python-based)
- **Crawling**: Selenium WebDriver + BeautifulSoup4 (Python)
- **Database**: PostgreSQL 16
- **Cache**: Redis 7.2
- **Containerization**: Docker + Docker Compose
- **Language**: Python 3.8+

### C·∫•u Tr√∫c D·ª± √Ån:
```
src/pipelines/crawl/
‚îú‚îÄ‚îÄ crawl_categories_optimized.py      # Crawl categories
‚îú‚îÄ‚îÄ crawl_products.py                  # Crawl product list
‚îú‚îÄ‚îÄ crawl_products_detail.py          # Crawl product details
‚îú‚îÄ‚îÄ extract_category_link_selenium.py # Selenium utilities
‚îú‚îÄ‚îÄ utils.py                          # Shared utilities
‚îú‚îÄ‚îÄ storage/                          # Redis, PostgreSQL storage
‚îú‚îÄ‚îÄ resilience/                       # Error handling, circuit breaker
‚îî‚îÄ‚îÄ utils/                            # Batch processing
```

---

## ‚úÖ C√ì TH·ªÇ CHUY·ªÇN SANG JAVASCRIPT

### K·∫øt Lu·∫≠n: **C√ì TH·ªÇ**, nh∆∞ng c·∫ßn l∆∞u √Ω m·ªôt s·ªë ƒëi·ªÉm

---

## üéØ C√°c Ph∆∞∆°ng √Ån Migration

### Ph∆∞∆°ng √Ån 1: Hybrid Approach (Khuy·∫øn Ngh·ªã) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Gi·ªØ Airflow (Python) + Crawl b·∫±ng Node.js**

#### C√°ch th·ª±c hi·ªán:

1. **Vi·∫øt crawler b·∫±ng Node.js**:
   ```javascript
   // src/pipelines/crawl-js/
   ‚îú‚îÄ‚îÄ crawl-categories.js
   ‚îú‚îÄ‚îÄ crawl-products.js
   ‚îú‚îÄ‚îÄ crawl-product-detail.js
   ‚îî‚îÄ‚îÄ utils.js
   ```

2. **T√≠ch h·ª£p v·ªõi Airflow qua DockerOperator ho·∫∑c BashOperator**:
   ```python
   # Trong Airflow DAG
   from airflow.providers.docker.operators.docker import DockerOperator
   
   crawl_task = DockerOperator(
       task_id='crawl_products',
       image='node:20-alpine',
       command='node /opt/airflow/src/pipelines/crawl-js/crawl-products.js',
       docker_url='unix://var/run/docker.sock',
       network_mode='bridge',
       volumes=[
           '/path/to/src:/opt/airflow/src',
           '/path/to/data:/opt/airflow/data'
       ]
   )
   ```

#### ∆Øu ƒëi·ªÉm:
- ‚úÖ Gi·ªØ nguy√™n Airflow infrastructure
- ‚úÖ T·∫≠n d·ª•ng t·ªëc ƒë·ªô Node.js cho crawl
- ‚úÖ Kh√¥ng c·∫ßn thay ƒë·ªïi database, Redis
- ‚úÖ C√≥ th·ªÉ migrate t·ª´ng ph·∫ßn (crawl tr∆∞·ªõc, transform/load sau)

#### Nh∆∞·ª£c ƒëi·ªÉm:
- ‚ö†Ô∏è C·∫ßn qu·∫£n l√Ω 2 ng√¥n ng·ªØ
- ‚ö†Ô∏è Debug ph·ª©c t·∫°p h∆°n (2 environments)

#### Effort: **2-3 tu·∫ßn**

---

### Ph∆∞∆°ng √Ån 2: Full Migration (Thay th·∫ø ho√†n to√†n)

**Thay Airflow b·∫±ng Node.js orchestrator (Temporal, BullMQ, ho·∫∑c custom)**

#### C√°ch th·ª±c hi·ªán:

1. **Ch·ªçn orchestrator**:
   - **Temporal**: Workflow engine m·∫°nh m·∫Ω, c√≥ TypeScript SDK
   - **BullMQ**: Job queue v·ªõi Redis
   - **Custom**: Express.js + cron jobs

2. **Vi·∫øt l·∫°i to√†n b·ªô pipeline**:
   ```javascript
   // src/pipelines/
   ‚îú‚îÄ‚îÄ crawl/          // Node.js crawlers
   ‚îú‚îÄ‚îÄ transform/      // Node.js transformers
   ‚îú‚îÄ‚îÄ load/           // Node.js loaders
   ‚îî‚îÄ‚îÄ orchestration/  // Temporal/BullMQ workflows
   ```

#### ∆Øu ƒëi·ªÉm:
- ‚úÖ T·ªëc ƒë·ªô t·ªëi ƒëa (to√†n b·ªô b·∫±ng Node.js)
- ‚úÖ ƒê∆°n gi·∫£n h√≥a stack (ch·ªâ 1 ng√¥n ng·ªØ)
- ‚úÖ D·ªÖ maintain v√† scale

#### Nh∆∞·ª£c ƒëi·ªÉm:
- ‚ö†Ô∏è M·∫•t Airflow UI v√† ecosystem
- ‚ö†Ô∏è C·∫ßn h·ªçc orchestrator m·ªõi
- ‚ö†Ô∏è Effort l·ªõn (4-6 tu·∫ßn)

#### Effort: **4-6 tu·∫ßn**

---

### Ph∆∞∆°ng √Ån 3: Microservices Approach

**T√°ch crawl th√†nh service ri√™ng (Node.js), gi·ªØ Airflow cho orchestration**

#### C√°ch th·ª±c hi·ªán:

1. **T·∫°o Node.js service**:
   ```javascript
   // crawl-service/
   ‚îú‚îÄ‚îÄ server.js          // Express API
   ‚îú‚îÄ‚îÄ routes/
   ‚îÇ   ‚îú‚îÄ‚îÄ categories.js
   ‚îÇ   ‚îú‚îÄ‚îÄ products.js
   ‚îÇ   ‚îî‚îÄ‚îÄ product-detail.js
   ‚îî‚îÄ‚îÄ Dockerfile
   ```

2. **Airflow g·ªçi qua HTTP**:
   ```python
   from airflow.providers.http.operators.http import SimpleHttpOperator
   
   crawl_task = SimpleHttpOperator(
       task_id='crawl_products',
       http_conn_id='crawl_service',
       endpoint='/api/crawl/products',
       method='POST',
       data=json.dumps({'category_url': '...'})
   )
   ```

#### ∆Øu ƒëi·ªÉm:
- ‚úÖ T√°ch bi·ªát concerns
- ‚úÖ C√≥ th·ªÉ scale crawl service ƒë·ªôc l·∫≠p
- ‚úÖ D·ªÖ test v√† debug
- ‚úÖ C√≥ th·ªÉ reuse cho projects kh√°c

#### Nh∆∞·ª£c ƒëi·ªÉm:
- ‚ö†Ô∏è Th√™m network overhead
- ‚ö†Ô∏è C·∫ßn qu·∫£n l√Ω th√™m service

#### Effort: **3-4 tu·∫ßn**

---

## üìä So S√°nh C√°c Ph∆∞∆°ng √Ån

| Ti√™u ch√≠ | Hybrid | Full Migration | Microservices |
|----------|--------|----------------|---------------|
| **Effort** | ‚≠ê‚≠ê 2-3 tu·∫ßn | ‚≠ê‚≠ê‚≠ê‚≠ê 4-6 tu·∫ßn | ‚≠ê‚≠ê‚≠ê 3-4 tu·∫ßn |
| **Risk** | ‚≠ê‚≠ê Th·∫•p | ‚≠ê‚≠ê‚≠ê‚≠ê Cao | ‚≠ê‚≠ê‚≠ê Trung b√¨nh |
| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê T·ªët | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê T·ªët nh·∫•t | ‚≠ê‚≠ê‚≠ê‚≠ê T·ªët |
| **Maintainability** | ‚≠ê‚≠ê‚≠ê Trung b√¨nh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê T·ªët nh·∫•t | ‚≠ê‚≠ê‚≠ê‚≠ê T·ªët |
| **Flexibility** | ‚≠ê‚≠ê‚≠ê Trung b√¨nh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê T·ªët nh·∫•t | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê T·ªët nh·∫•t |
| **Khuy·∫øn ngh·ªã** | ‚úÖ **N√™n d√πng** | ‚ö†Ô∏è Ch·ªâ n·∫øu c·∫ßn | ‚úÖ T·ªët cho scale |

---

## üîß Chi Ti·∫øt Implementation - Hybrid Approach (Khuy·∫øn Ngh·ªã)

### B∆∞·ªõc 1: Setup Node.js trong Docker

**T·∫°o Dockerfile cho Node.js crawler**:
```dockerfile
# Dockerfile.crawler
FROM node:20-alpine

WORKDIR /app

# Install dependencies
COPY package.json package-lock.json ./
RUN npm ci --only=production

# Copy source code
COPY src/pipelines/crawl-js ./src

# Install Puppeteer dependencies
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    freetype-dev \
    harfbuzz \
    ca-certificates \
    ttf-freefont

# Set Puppeteer to use installed Chromium
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
ENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser

CMD ["node", "src/crawl-products.js"]
```

### B∆∞·ªõc 2: Vi·∫øt Crawler b·∫±ng Node.js

**crawl-product-detail.js**:
```javascript
const puppeteer = require('puppeteer');
const fs = require('fs').promises;
const path = require('path');

async function crawlProductDetail(url, options = {}) {
  const {
    timeout = 30000,
    useCache = true,
    useRateLimit = true,
    rateLimitDelay = 1000
  } = options;

  // Check cache
  if (useCache) {
    const cached = await getCached(url);
    if (cached) return cached;
  }

  // Rate limiting
  if (useRateLimit) {
    await rateLimit(rateLimitDelay);
  }

  const browser = await puppeteer.launch({
    headless: true,
    args: [
      '--no-sandbox',
      '--disable-setuid-sandbox',
      '--disable-dev-shm-usage',
      '--disable-gpu'
    ]
  });

  try {
    const page = await browser.newPage();
    await page.goto(url, { waitUntil: 'networkidle2', timeout });
    await page.waitForTimeout(2000);

    // Scroll to load lazy content
    await page.evaluate(() => window.scrollTo(0, 500));
    await page.waitForTimeout(500);
    await page.evaluate(() => window.scrollTo(0, 1500));
    await page.waitForTimeout(500);
    await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
    await page.waitForTimeout(1000);

    const html = await page.content();
    
    // Cache result
    if (useCache) {
      await cacheResult(url, html);
    }

    return html;
  } finally {
    await browser.close();
  }
}

// Batch crawling v·ªõi concurrency
async function crawlProductsBatch(urls, concurrency = 200) {
  const pLimit = require('p-limit');
  const limit = pLimit(concurrency);

  const results = await Promise.all(
    urls.map(url =>
      limit(async () => {
        try {
          return await crawlProductDetail(url);
        } catch (error) {
          console.error(`Error crawling ${url}:`, error);
          return null;
        }
      })
    )
  );

  return results.filter(r => r !== null);
}

module.exports = { crawlProductDetail, crawlProductsBatch };
```

### B∆∞·ªõc 3: T√≠ch h·ª£p v·ªõi Airflow

**C·∫≠p nh·∫≠t DAG**:
```python
from airflow.providers.docker.operators.docker import DockerOperator
from airflow import DAG

def create_crawl_dag():
    dag = DAG(
        'tiki_crawl_products_js',
        default_args={...},
        schedule_interval='@daily'
    )

    # Crawl categories (gi·ªØ Python ho·∫∑c chuy·ªÉn sang Node.js)
    crawl_categories = PythonOperator(
        task_id='crawl_categories',
        python_callable=crawl_categories_python,  # Ho·∫∑c d√πng DockerOperator
        dag=dag
    )

    # Crawl products v·ªõi Node.js
    crawl_products = DockerOperator(
        task_id='crawl_products',
        image='tiki-crawler-node:latest',
        api_version='auto',
        auto_remove=True,
        docker_url='unix://var/run/docker.sock',
        network_mode='bridge',
        environment={
            'REDIS_URL': 'redis://redis:6379/1',
            'POSTGRES_URL': 'postgresql://user:pass@postgres:5432/crawl_data'
        },
        volumes=[
            '${AIRFLOW_PROJ_DIR}/src:/app/src',
            '${AIRFLOW_PROJ_DIR}/data:/app/data'
        ],
        command='node src/pipelines/crawl-js/crawl-products.js',
        dag=dag
    )

    # Transform v√† Load (gi·ªØ Python)
    transform_products = PythonOperator(
        task_id='transform_products',
        python_callable=transform_products_python,
        dag=dag
    )

    crawl_categories >> crawl_products >> transform_products
    return dag

dag = create_crawl_dag()
```

---

## üì¶ Dependencies C·∫ßn Thi·∫øt

### Node.js Packages:
```json
{
  "name": "tiki-crawler",
  "version": "1.0.0",
  "dependencies": {
    "puppeteer": "^21.0.0",
    "p-limit": "^4.0.0",
    "redis": "^4.6.0",
    "pg": "^8.11.0",
    "cheerio": "^1.0.0",
    "axios": "^1.6.0"
  }
}
```

### T∆∞∆°ng ƒë∆∞∆°ng v·ªõi Python:
| Python | Node.js |
|--------|---------|
| `selenium` | `puppeteer` ho·∫∑c `playwright` |
| `beautifulsoup4` | `cheerio` |
| `requests` | `axios` ho·∫∑c `node-fetch` |
| `redis` | `redis` |
| `psycopg2` | `pg` |
| `concurrent.futures` | `p-limit` |

---

## ‚ö†Ô∏è Nh·ªØng ƒêi·ªÉm C·∫ßn L∆∞u √ù

### 1. **Airflow Integration**
- Airflow l√† Python-based, nh∆∞ng c√≥ th·ªÉ g·ªçi Node.js qua:
  - `DockerOperator`: Ch·∫°y Node.js trong container
  - `BashOperator`: Ch·∫°y `node script.js`
  - `SimpleHttpOperator`: N·∫øu d√πng microservices

### 2. **Database & Redis**
- PostgreSQL v√† Redis c√≥ clients cho Node.js
- Kh√¥ng c·∫ßn thay ƒë·ªïi database schema
- C√≥ th·ªÉ d√πng chung connection pool

### 3. **Error Handling**
- C·∫ßn vi·∫øt l·∫°i error handling logic
- Circuit breaker, retry logic c·∫ßn implement l·∫°i

### 4. **Testing**
- C·∫ßn setup test environment cho Node.js
- Integration tests v·ªõi Airflow

### 5. **Deployment**
- C·∫ßn build Docker image cho Node.js crawler
- Update docker-compose.yaml

---

## üöÄ Migration Roadmap (Hybrid Approach)

### Week 1: Setup & Proof of Concept
- [ ] Setup Node.js environment
- [ ] Vi·∫øt crawler ƒë∆°n gi·∫£n (1 file)
- [ ] Test v·ªõi 10-20 products
- [ ] So s√°nh t·ªëc ƒë·ªô v·ªõi Python

### Week 2: Full Crawler Implementation
- [ ] Vi·∫øt l·∫°i t·∫•t c·∫£ crawl functions
- [ ] Implement Redis cache
- [ ] Implement PostgreSQL storage
- [ ] Error handling & retry logic

### Week 3: Integration & Testing
- [ ] T√≠ch h·ª£p v·ªõi Airflow DAG
- [ ] Integration tests
- [ ] Performance testing
- [ ] Load testing v·ªõi 1000+ products

### Week 4: Deployment & Monitoring
- [ ] Deploy to production
- [ ] Monitor performance
- [ ] Tune concurrency settings
- [ ] Documentation

---

## üìà Expected Performance Improvement

### V·ªõi Hybrid Approach:

| Metric | Python (Hi·ªán t·∫°i) | Node.js | C·∫£i thi·ªán |
|--------|------------------|---------|-----------|
| **Crawl 1 product** | 6.5-8s | 5-6s | ‚ö° 20-30% |
| **11k products (8 threads)** | 23 ph√∫t | - | - |
| **11k products (200 concurrent)** | - | 3.7 ph√∫t | ‚ö° **6.2x nhanh h∆°n** |

---

## ‚úÖ K·∫øt Lu·∫≠n

### C√≥ th·ªÉ chuy·ªÉn sang JavaScript kh√¥ng?
**C√ì**, v√† n√™n l√†m theo **Hybrid Approach**:

1. ‚úÖ **Gi·ªØ Airflow** cho orchestration (ƒë√£ setup s·∫µn)
2. ‚úÖ **Chuy·ªÉn crawl sang Node.js** ƒë·ªÉ t·∫≠n d·ª•ng t·ªëc ƒë·ªô
3. ‚úÖ **Gi·ªØ transform/load b·∫±ng Python** (n·∫øu c·∫ßn pandas, numpy)
4. ‚úÖ **Migrate t·ª´ng ph·∫ßn** ƒë·ªÉ gi·∫£m risk

### Khuy·∫øn ngh·ªã:
- **B·∫Øt ƒë·∫ßu v·ªõi crawl-product-detail.js** (ph·∫ßn t·ªën th·ªùi gian nh·∫•t)
- **Test v·ªõi sample nh·ªè** (100-1000 products)
- **So s√°nh performance** tr∆∞·ªõc khi migrate to√†n b·ªô
- **Gi·ªØ Python code** l√†m backup trong giai ƒëo·∫°n transition

### Effort Estimate:
- **Hybrid Approach**: 2-3 tu·∫ßn
- **Full Migration**: 4-6 tu·∫ßn
- **Microservices**: 3-4 tu·∫ßn

---

## üìù Next Steps

1. **Quy·∫øt ƒë·ªãnh ph∆∞∆°ng √°n**: Hybrid / Full / Microservices
2. **Setup Node.js environment**: Docker, dependencies
3. **Vi·∫øt POC**: Crawl 1 product detail v·ªõi Node.js
4. **Benchmark**: So s√°nh v·ªõi Python version
5. **Plan migration**: Timeline, resources, testing

