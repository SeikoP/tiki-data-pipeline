import sys
import os
import json
import re

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o sys.path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src', 'pipelines', 'crawl'))

from crawl_products import (
    crawl_category_products,
    crawl_products_from_categories,
    get_page_with_requests,
    parse_products_from_html,
    get_total_pages
)

def test_parse_products_from_html():
    """Test parse s·∫£n ph·∫©m t·ª´ HTML"""
    print("="*70)
    print("üß™ TEST: Parse s·∫£n ph·∫©m t·ª´ HTML")
    print("="*70)
    
    # HTML m·∫´u (gi·∫£ l·∫≠p)
    html_sample = """
    <html>
    <body>
        <div class="product-item">
            <a href="/p/12345">
                <img src="https://salt.tikicdn.com/cache/200x200/ts/product/12/34/56.jpg" alt="S·∫£n ph·∫©m test">
                <h3 class="product-title">S·∫£n ph·∫©m Test 1</h3>
                <div class="product-price__current-price">100.000 ‚Ç´</div>
            </a>
        </div>
        <div class="product-item">
            <a href="/p/67890">
                <img src="https://salt.tikicdn.com/cache/200x200/ts/product/67/89/90.jpg" alt="S·∫£n ph·∫©m test 2">
                <h3 class="product-title">S·∫£n ph·∫©m Test 2</h3>
                <div class="product-price__current-price">200.000 ‚Ç´</div>
            </a>
        </div>
    </body>
    </html>
    """
    
    category_url = "https://tiki.vn/test/c123"
    products = parse_products_from_html(html_sample, category_url)
    
    print(f"‚úì T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
    for product in products:
        print(f"  - {product.get('name')} (ID: {product.get('product_id')})")
    
    assert len(products) >= 2, "Ph·∫£i t√¨m th·∫•y √≠t nh·∫•t 2 s·∫£n ph·∫©m"
    assert products[0].get('product_id') == '12345', "Product ID kh√¥ng ƒë√∫ng"
    assert products[0].get('name') == 'S·∫£n ph·∫©m Test 1', "T√™n s·∫£n ph·∫©m kh√¥ng ƒë√∫ng"
    
    print("‚úÖ Test parse HTML th√†nh c√¥ng!\n")


def test_crawl_single_category():
    """Test crawl m·ªôt danh m·ª•c c·ª• th·ªÉ"""
    print("="*70)
    print("üß™ TEST: Crawl m·ªôt danh m·ª•c")
    print("="*70)
    
        # Ch·ªçn m·ªôt danh m·ª•c b·∫•t k·ª≥ t·ª´ file (th·ª≠ v·ªõi danh m·ª•c kh√°c)
    categories_file = 'data/raw/categories_recursive_optimized.json'
    
    try:
        with open(categories_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        
        # Ch·ªçn danh m·ª•c c√≥ nhi·ªÅu s·∫£n ph·∫©m - th·ª≠ v·ªõi danh m·ª•c level 2-3 v√† c√≥ nhi·ªÅu s·∫£n ph·∫©m
        # ∆Øu ti√™n c√°c danh m·ª•c ph·ªï bi·∫øn nh∆∞ ƒëi·ªán t·ª≠, th·ªùi trang, etc.
        test_categories = []
        for cat in categories:
            level = cat.get('level', 0)
            name = cat.get('name', '').lower()
            # Ch·ªçn c√°c danh m·ª•c ph·ªï bi·∫øn
            if 2 <= level <= 3:
                if any(keyword in name for keyword in ['ƒëi·ªán tho·∫°i', 'laptop', 'tai nghe', '√°o', 'qu·∫ßn', 'gi√†y', 't√∫i', 'ƒë·ªìng h·ªì']):
                    test_categories.append(cat)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y danh m·ª•c ph·ªï bi·∫øn, l·∫•y b·∫•t k·ª≥ danh m·ª•c level 2-3
        if not test_categories:
            for cat in categories:
                if 2 <= cat.get('level', 0) <= 3:
                    test_categories.append(cat)
        
        # N·∫øu v·∫´n kh√¥ng c√≥, l·∫•y b·∫•t k·ª≥
        if not test_categories:
            test_categories = categories[:10] if len(categories) > 10 else categories
        
        # Ch·ªçn ng·∫´u nhi√™n ho·∫∑c l·∫•y danh m·ª•c th·ª© 3 ƒë·ªÉ test v·ªõi danh m·ª•c kh√°c
        import random
        test_category = test_categories[2] if len(test_categories) > 2 else test_categories[0] if test_categories else None
        
        if not test_category:
            print("‚ùå Kh√¥ng t√¨m th·∫•y danh m·ª•c ƒë·ªÉ test")
            return
        
        category_url = test_category.get('url', '')
        category_name = test_category.get('name', 'Unknown')
        
        print(f"üìÅ Danh m·ª•c: {category_name}")
        print(f"üîó URL: {category_url}")
        print(f"üìä Level: {test_category.get('level', 0)}")
        print(f"üìù L∆∞u √Ω: Ch·ªâ crawl th√¥ng tin c∆° b·∫£n (ID, t√™n, URL, h√¨nh) - s·∫Ω crawl detail sau")
        print(f"\n‚è≥ ƒêang crawl...")
        
        # Crawl v·ªõi gi·ªõi h·∫°n 2 trang ƒë·ªÉ test nhanh
        # Th·ª≠ v·ªõi Selenium n·∫øu requests kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m
        products = crawl_category_products(
            category_url,
            max_pages=2,
            use_selenium=True  # D√πng Selenium ƒë·ªÉ render JavaScript
        )
        
        print(f"\n‚úÖ T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
        
        if products:
            print(f"\nüì¶ M·∫´u s·∫£n ph·∫©m (5 s·∫£n ph·∫©m ƒë·∫ßu):")
            for i, product in enumerate(products[:5], 1):
                print(f"  {i}. {product.get('name', 'N/A')}")
                print(f"     ID: {product.get('product_id')}")
                print(f"     URL: {product.get('url')}")
                if product.get('image_url'):
                    print(f"     H√¨nh: {product.get('image_url')[:50]}...")
                print(f"     (Gi√°, ƒë√°nh gi√°, s·ªë l∆∞·ª£ng b√°n s·∫Ω crawl detail sau)")
                print()
        
        if len(products) == 0:
            print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m. C√≥ th·ªÉ do:")
            print("   - Danh m·ª•c kh√¥ng c√≥ s·∫£n ph·∫©m")
            print("   - C·∫ßn Selenium ƒë·ªÉ render JavaScript")
            print("   - C·∫•u tr√∫c HTML ƒë√£ thay ƒë·ªïi")
            print("   - Website ch·∫∑n crawler")
            print("\nüí° Th·ª≠:")
            print("   - Ki·ªÉm tra URL tr·ª±c ti·∫øp tr√™n browser")
            print("   - D√πng Selenium n·∫øu ch∆∞a d√πng")
            print("   - Ki·ªÉm tra __NEXT_DATA__ trong HTML")
        else:
            assert len(products) > 0, "Ph·∫£i t√¨m th·∫•y √≠t nh·∫•t 1 s·∫£n ph·∫©m"
            print("‚úÖ Test crawl danh m·ª•c th√†nh c√¥ng!\n")
        
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {categories_file}")
        print("   Ch·∫°y crawl categories tr∆∞·ªõc!")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


def test_crawl_multiple_categories():
    """Test crawl nhi·ªÅu danh m·ª•c - CRAWL H·∫æT S·∫¢N PH·∫®M trong m·ªôt v√†i danh m·ª•c deep level"""
    print("="*70)
    print("üß™ TEST: Crawl H·∫æT s·∫£n ph·∫©m trong m·ªôt v√†i danh m·ª•c DEEP LEVEL")
    print("="*70)
    
    categories_file = 'data/raw/categories_recursive_optimized.json'
    output_file = 'data/demo/products/products.json'
    
    try:
        with open(categories_file, 'r', encoding='utf-8') as f:
            all_categories = json.load(f)
        
        # Filter l·∫•y c√°c danh m·ª•c ·ªü DEEP LEVEL (level 3-4) - c√≥ nhi·ªÅu s·∫£n ph·∫©m c·ª• th·ªÉ
        deep_level_categories = []
        for cat in all_categories:
            level = cat.get('level', 0)
            # L·∫•y level 3-4 (deep level - danh m·ª•c con s√¢u)
            if 3 <= level <= 4:
                deep_level_categories.append(cat)
        
        # Ch·ªçn m·ªôt v√†i danh m·ª•c t·ª´ deep level
        # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng danh m·ª•c ·ªü ƒë√¢y
        NUM_CATEGORIES_TO_CRAWL = 4  # S·ªë danh m·ª•c ƒë·ªÉ crawl (c√≥ th·ªÉ thay ƒë·ªïi)
        
        selected_categories = deep_level_categories[:NUM_CATEGORIES_TO_CRAWL] if len(deep_level_categories) > NUM_CATEGORIES_TO_CRAWL else deep_level_categories
        
        if not selected_categories:
            print("‚ùå Kh√¥ng t√¨m th·∫•y danh m·ª•c ·ªü deep level (level 3-4)")
            print("   Th·ª≠ v·ªõi level 2-3...")
            # Fallback: l·∫•y level 2-3 n·∫øu kh√¥ng c√≥ level 3-4
            for cat in all_categories:
                if 2 <= cat.get('level', 0) <= 3:
                    selected_categories.append(cat)
                    if len(selected_categories) >= 10:
                        break
        
        if not selected_categories:
            print("‚ùå Kh√¥ng t√¨m th·∫•y danh m·ª•c ph√π h·ª£p")
            return
        
        print(f"üìñ ƒê√£ ch·ªçn {len(selected_categories)} danh m·ª•c ·ªü deep level (level 3-4)")
        print(f"üìÅ File output: {output_file}")
        print(f"üìù L∆∞u √Ω: Crawl H·∫æT s·∫£n ph·∫©m (T·∫§T C·∫¢ trang) trong c√°c danh m·ª•c n√†y")
        print(f"          Ch·ªâ l·∫•y th√¥ng tin c∆° b·∫£n (ID, t√™n, URL, h√¨nh)")
        print(f"          Gi√°, ƒë√°nh gi√°, s·ªë l∆∞·ª£ng b√°n s·∫Ω crawl detail sau")
        print(f"\nüìã Danh s√°ch {len(selected_categories)} danh m·ª•c s·∫Ω crawl:")
        for i, cat in enumerate(selected_categories, 1):
            print(f"   {i}. {cat.get('name')} (Level {cat.get('level')})")
            print(f"      {cat.get('url')}")
        print("="*70)
        
        # Filter function ƒë·ªÉ ch·ªâ crawl c√°c danh m·ª•c ƒë√£ ch·ªçn
        selected_urls = {cat.get('url') for cat in selected_categories}
        def filter_selected_categories(cat):
            return cat.get('url') in selected_urls
        
        products = crawl_products_from_categories(
            categories_file=categories_file,
            output_file=output_file,
            max_categories=None,  # Crawl t·∫•t c·∫£ c√°c danh m·ª•c ƒë√£ ch·ªçn
            max_pages_per_category=None,  # Crawl T·∫§T C·∫¢ trang (kh√¥ng gi·ªõi h·∫°n)
            max_workers=5,  # 5 thread song song
            use_selenium=False,  # D√πng requests (nhanh h∆°n), t·ª± ƒë·ªông fallback Selenium n·∫øu c·∫ßn
            categories_filter=filter_selected_categories  # Ch·ªâ crawl c√°c danh m·ª•c ƒë√£ ch·ªçn
        )
        
        print(f"\n‚úÖ Crawl ho√†n th√†nh!")
        print(f"üì¶ T·ªïng s·∫£n ph·∫©m: {len(products)}")
        print(f"üìÅ File output: {output_file}")
        
        # Th·ªëng k√™ theo danh m·ª•c
        if products:
            category_stats = {}
            category_names = {}
            
            # L·∫•y t√™n danh m·ª•c t·ª´ selected_categories
            for cat in selected_categories:
                category_names[cat.get('url')] = cat.get('name', 'Unknown')
            
            for product in products:
                cat_url = product.get('category_url', 'Unknown')
                category_stats[cat_url] = category_stats.get(cat_url, 0) + 1
                # L∆∞u t√™n danh m·ª•c n·∫øu ch∆∞a c√≥
                if cat_url not in category_names:
                    category_names[cat_url] = cat_url.split('/')[-2] if '/' in cat_url else 'Unknown'
            
            print(f"\nüìä Th·ªëng k√™ theo danh m·ª•c:")
            print(f"   S·ªë danh m·ª•c: {len(category_stats)}")
            print(f"   T·ªïng s·∫£n ph·∫©m: {len(products)}")
            
            # S·∫Øp x·∫øp v√† hi·ªÉn th·ªã
            sorted_stats = sorted(category_stats.items(), key=lambda x: x[1], reverse=True)
            for cat_url, count in sorted_stats:
                cat_name = category_names.get(cat_url, cat_url)
                print(f"   - {count:4d} s·∫£n ph·∫©m | {cat_name} (Level {next((c.get('level') for c in selected_categories if c.get('url') == cat_url), '?')})")
                print(f"     {cat_url}")
        
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {categories_file}")
        print("   Ch·∫°y crawl categories tr∆∞·ªõc!")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


def test_get_page():
    """Test l·∫•y trang web"""
    print("="*70)
    print("üß™ TEST: L·∫•y trang web")
    print("="*70)
    
    test_url = "https://tiki.vn/dien-thoai-smartphone/c1795"
    
    try:
        print(f"‚è≥ ƒêang l·∫•y HTML t·ª´: {test_url}")
        html = get_page_with_requests(test_url)
        
        if html:
            print(f"‚úÖ ƒê√£ l·∫•y th√†nh c√¥ng ({len(html)} k√Ω t·ª±)")
            
            # Ki·ªÉm tra __NEXT_DATA__
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            next_data = soup.find('script', id='__NEXT_DATA__')
            if next_data:
                print(f"‚úì T√¨m th·∫•y __NEXT_DATA__ ({len(next_data.string)} k√Ω t·ª±)")
            else:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y __NEXT_DATA__ - c√≥ th·ªÉ c·∫ßn Selenium")
            
            # Test parse
            products = parse_products_from_html(html, test_url)
            print(f"üì¶ T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
            
            if len(products) == 0:
                # Debug: t√¨m c√°c link /p/
                all_links = soup.find_all('a', href=re.compile(r'/p/\d+'))
                print(f"   Debug: T√¨m th·∫•y {len(all_links)} link /p/ trong HTML")
            
            # Test get total pages
            total_pages = get_total_pages(html)
            print(f"üìÑ T·ªïng s·ªë trang: {total_pages}")
            
        else:
            print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c HTML")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


def main():
    """Ch·∫°y t·∫•t c·∫£ tests - CRAWL H·∫æT S·∫¢N PH·∫®M V√Ä L∆ØU V√ÄO DATA/DEMO"""
    print("="*70)
    print("üß™ CH·∫†Y TEST CRAWL S·∫¢N PH·∫®M - CRAWL H·∫æT V√Ä L∆ØU V√ÄO DATA/DEMO")
    print("="*70)
    print()
    
    # C√≥ th·ªÉ b·ªè qua c√°c test nh·ªè v√† ch·ªâ ch·∫°y crawl h·∫øt
    # Uncomment c√°c test d∆∞·ªõi n·∫øu mu·ªën ch·∫°y ƒë·∫ßy ƒë·ªß
    
    # # Test 1: Parse HTML
    # try:
    #     test_parse_products_from_html()
    # except Exception as e:
    #     print(f"‚ùå Test parse HTML th·∫•t b·∫°i: {e}\n")
    
    # # Test 2: Get page
    # try:
    #     test_get_page()
    #     print()
    # except Exception as e:
    #     print(f"‚ùå Test get page th·∫•t b·∫°i: {e}\n")
    
    # # Test 3: Crawl single category (test nhanh)
    # try:
    #     test_crawl_single_category()
    # except Exception as e:
    #     print(f"‚ùå Test crawl single category th·∫•t b·∫°i: {e}\n")
    
    # Test 4: Crawl multiple categories - CRAWL H·∫æT S·∫¢N PH·∫®M
    print("\n" + "="*70)
    print("üöÄ B·∫ÆT ƒê·∫¶U CRAWL H·∫æT S·∫¢N PH·∫®M")
    print("="*70)
    print("üìÅ L∆∞u v√†o: data/demo/products/products.json")
    print("‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian...")
    print("="*70)
    
    try:
        test_crawl_multiple_categories()
        print("\n" + "="*70)
        print("‚úÖ HO√ÄN TH√ÄNH CRAWL H·∫æT S·∫¢N PH·∫®M!")
        print("="*70)
        print("üìÅ File k·∫øt qu·∫£: data/demo/products/products.json")
        print("üí° C√≥ th·ªÉ d√πng file n√†y ƒë·ªÉ crawl detail (gi√°, ƒë√°nh gi√°, s·ªë l∆∞·ª£ng b√°n) sau")
    except Exception as e:
        print(f"‚ùå Test crawl multiple categories th·∫•t b·∫°i: {e}\n")
        import traceback
        traceback.print_exc()
        print("\n" + "="*70)
        print("‚ùå CRAWL TH·∫§T B·∫†I!")
        print("="*70)


if __name__ == "__main__":
    main()

